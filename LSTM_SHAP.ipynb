{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import tensorflow as tf\n",
    "from joblib import dump, load\n",
    "import ltn\n",
    "import csv\n",
    "import math\n",
    "import wandb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import os\n",
    "import joblib  # For saving the scaler model\n",
    "import os\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "dataset_path = '/home/ubuntu/dds_paper/DDS_Paper/data/DDS_Data_SEU'\n",
    "\n",
    "PGB_path = '/home/ubuntu/dds_paper/DDS_Paper/data/DDS_Data_SEU/PGB/PGB'\n",
    "RGB_path = '/home/ubuntu/dds_paper/DDS_Paper/data/DDS_Data_SEU/RGB/RGB'\n",
    "\n",
    "# Specify the CSV file path\n",
    "csv_file = '/home/ubuntu/dds_paper/DDS_Paper/data/data_robust.csv'\n",
    "preprocessor_file = 'preprocessor.joblib'\n",
    "\n",
    "train_path = '/home/ubuntu/dds_paper/DDS_Paper/data/train.csv'\n",
    "val_path = '/home/ubuntu/dds_paper/DDS_Paper/data/val.csv'\n",
    "\n",
    "np.random.seed(45)\n",
    "\n",
    "# Set the chunk size for reading the CSV\n",
    "chunk_size = 100000  # Adjust the chunk size according to your memory limitations\n",
    "\n",
    "\n",
    "# Directory containing your scaled CSV files\n",
    "csv_directory = '/home/ubuntu/dds_paper/DDS_Paper/data/DDS_Data_SEU/data/csvs'\n",
    "# Define your dataset directory\n",
    "data_root_folder = '/home/ubuntu/dds_paper/DDS_Paper/data/DDS_Data_SEU/PGB/PGB'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Variable_speed Variable_speed: 0file [00:00, ?file/s]\n",
      "Processing Variable_speed Experiment8: 100%|██████████| 9/9 [01:08<00:00,  7.57s/file]\n",
      "Processing Variable_speed Experiment4: 100%|██████████| 9/9 [01:06<00:00,  7.41s/file]\n",
      "Processing Variable_speed Experiment6: 100%|██████████| 9/9 [01:09<00:00,  7.75s/file]\n",
      "Processing Variable_speed Experiment2: 100%|██████████| 9/9 [01:00<00:00,  6.77s/file]\n",
      "Processing Variable_speed Experiment5: 100%|██████████| 9/9 [01:06<00:00,  7.40s/file]\n",
      "Processing Variable_speed Experiment1: 100%|██████████| 9/9 [01:05<00:00,  7.30s/file]\n",
      "Processing Variable_speed Experiment9: 100%|██████████| 9/9 [01:12<00:00,  8.07s/file]\n",
      "Processing Variable_speed Experiment7: 100%|██████████| 9/9 [01:05<00:00,  7.29s/file]\n",
      "Processing Variable_speed Experiment3: 100%|██████████| 9/9 [01:04<00:00,  7.16s/file]\n",
      "Processing Variable_speed Experiment10: 100%|██████████| 9/9 [01:06<00:00,  7.42s/file]\n",
      "Processing 40_0 : 100%|██████████| 9/9 [01:06<00:00,  7.38s/file]\n",
      "Processing 30_4 : 100%|██████████| 9/9 [01:04<00:00,  7.14s/file]\n",
      "Processing 30_3 : 100%|██████████| 9/9 [01:12<00:00,  8.06s/file]\n",
      "Processing 20_0 : 100%|██████████| 9/9 [01:07<00:00,  7.51s/file]\n",
      "Processing 30_5 : 100%|██████████| 9/9 [01:09<00:00,  7.71s/file]\n",
      "Processing 30_2 : 100%|██████████| 9/9 [01:08<00:00,  7.56s/file]\n",
      "Processing 50_0 : 100%|██████████| 9/9 [01:06<00:00,  7.40s/file]\n",
      "Processing 30_0 : 100%|██████████| 9/9 [01:07<00:00,  7.45s/file]\n",
      "Processing 30_1 : 100%|██████████| 9/9 [01:05<00:00,  7.31s/file]\n"
     ]
    }
   ],
   "source": [
    "def extract_fault(file_name):\n",
    "    fault_mapping = {\n",
    "        '0Health': 'HEA', '1Chipped': 'CTF', '2Miss': 'MTF', \n",
    "        '3Root': 'RCF', '4Surface': 'SWF', '5Ball': 'BWF', \n",
    "        '6Combination': 'CWF', '7Inner': 'IRF', '8Outer': 'ORF'\n",
    "    }\n",
    "    for key, value in fault_mapping.items():\n",
    "        if key in file_name:\n",
    "            return value\n",
    "    return None\n",
    "\n",
    "def make_csv_writer(csv_file):\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow(['Channel1', 'Channel2', 'Channel3', 'Channel4', 'Channel5', 'Channel6', 'Channel7', 'Channel8', 'Fault'])\n",
    "    return csv_writer\n",
    "\n",
    "def generate_csv(output_directory, root_path, speed, experiment, files):\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    train_filename_suffix = f\"{speed}_{experiment}_train\" if experiment else f\"{speed}_train\"\n",
    "    test_filename_suffix = f\"{speed}_{experiment}_test\" if experiment else f\"{speed}_test\"\n",
    "    \n",
    "    train_output_file_path = os.path.join(output_directory, f\"PGB_{train_filename_suffix}.csv\")\n",
    "    test_output_file_path = os.path.join(output_directory, f\"PGB_{test_filename_suffix}.csv\")\n",
    "    \n",
    "    with open(train_output_file_path, 'w', newline='', encoding='utf-8') as train_csvfile, \\\n",
    "        open(test_output_file_path, 'w', newline='', encoding='utf-8') as test_csvfile:\n",
    "        train_csv_writer = make_csv_writer(train_csvfile)\n",
    "        test_csv_writer = make_csv_writer(test_csvfile)\n",
    "        \n",
    "        for file in tqdm(files, desc=f\"Processing {speed} {experiment}\", unit=\"file\"):\n",
    "            fault_type = extract_fault(file)\n",
    "            # Only append 'speed' directory for non-variable speed cases\n",
    "            if experiment:\n",
    "                file_path = os.path.join(root_path, file)  # Already includes 'Variable_speed/Experiment#'\n",
    "            else:\n",
    "                file_path = os.path.join(root_path, file)  # 'root_path' already includes 'speed' directory\n",
    "            \n",
    "            data = pd.read_csv(file_path, sep='\\t', header=None, encoding='ISO-8859-1', skiprows=1, nrows=100000)\n",
    "            train_samples, test_samples = data.iloc[:80000, :], data.iloc[80000:100000, :]\n",
    "            \n",
    "            for index, row in train_samples.iterrows():\n",
    "                train_csv_writer.writerow(row[:8].tolist() + [fault_type])\n",
    "            \n",
    "            for index, row in test_samples.iterrows():\n",
    "                test_csv_writer.writerow(row[:8].tolist() + [fault_type])\n",
    "\n",
    "def process_pgb_data(data_root_folder, csv_directory):\n",
    "    for root, dirs, files in os.walk(data_root_folder):\n",
    "        parts = root.split(os.sep)\n",
    "        if 'Variable_speed' in parts:\n",
    "            speed = \"Variable_speed\"\n",
    "            experiment_dir = parts[-1]  # Get the last part as the experiment name\n",
    "            exp_files = [f for f in os.listdir(root) if f.endswith('.txt')]\n",
    "            # Pass the 'root' directly without modifying it for variable speed\n",
    "            generate_csv(csv_directory, root, speed, experiment_dir, exp_files)\n",
    "        elif 'PGB' in parts and files:\n",
    "            speed = parts[-1]  # Last part of 'root' is the speed directory\n",
    "            # For non-variable speed, pass the 'root' directly\n",
    "            generate_csv(csv_directory, root, speed, '', files)\n",
    "\n",
    "\n",
    "\n",
    "process_pgb_data(data_root_folder, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted empty file: /home/ubuntu/dds_paper/DDS_Paper/data/DDS_Data_SEU/data/csvs/PGB_Variable_speed_Variable_speed_train.csv\n",
      "Deleted empty file: /home/ubuntu/dds_paper/DDS_Paper/data/DDS_Data_SEU/data/csvs/PGB_Variable_speed_Variable_speed_test.csv\n",
      "                                File Name  Number of Samples   BWF   CTF   CWF   HEA   IRF   MTF   ORF   RCF   SWF\n",
      "                        PGB_20_0_test.csv             180000 20000 20000 20000 20000 20000 20000 20000 20000 20000\n",
      "                       PGB_20_0_train.csv             720000 80000 80000 80000 80000 80000 80000 80000 80000 80000\n",
      "                        PGB_30_0_test.csv             180000 20000 20000 20000 20000 20000 20000 20000 20000 20000\n",
      "                       PGB_30_0_train.csv             720000 80000 80000 80000 80000 80000 80000 80000 80000 80000\n",
      "                        PGB_30_1_test.csv             180000 20000 20000 20000 20000 20000 20000 20000 20000 20000\n",
      "                       PGB_30_1_train.csv             720000 80000 80000 80000 80000 80000 80000 80000 80000 80000\n",
      "                        PGB_30_2_test.csv             180000 20000 20000 20000 20000 20000 20000 20000 20000 20000\n",
      "                       PGB_30_2_train.csv             720000 80000 80000 80000 80000 80000 80000 80000 80000 80000\n",
      "                        PGB_30_3_test.csv             180000 20000 20000 20000 20000 20000 20000 20000 20000 20000\n",
      "                       PGB_30_3_train.csv             720000 80000 80000 80000 80000 80000 80000 80000 80000 80000\n",
      "                        PGB_30_4_test.csv             180000 20000 20000 20000 20000 20000 20000 20000 20000 20000\n",
      "                       PGB_30_4_train.csv             720000 80000 80000 80000 80000 80000 80000 80000 80000 80000\n",
      "                        PGB_30_5_test.csv             180000 20000 20000 20000 20000 20000 20000 20000 20000 20000\n",
      "                       PGB_30_5_train.csv             720000 80000 80000 80000 80000 80000 80000 80000 80000 80000\n",
      "                        PGB_40_0_test.csv             180000 20000 20000 20000 20000 20000 20000 20000 20000 20000\n",
      "                       PGB_40_0_train.csv             720000 80000 80000 80000 80000 80000 80000 80000 80000 80000\n",
      "                        PGB_50_0_test.csv             180000 20000 20000 20000 20000 20000 20000 20000 20000 20000\n",
      "                       PGB_50_0_train.csv             720000 80000 80000 80000 80000 80000 80000 80000 80000 80000\n",
      " PGB_Variable_speed_Experiment10_test.csv             180000 20000 20000 20000 20000 20000 20000 20000 20000 20000\n",
      "PGB_Variable_speed_Experiment10_train.csv             720000 80000 80000 80000 80000 80000 80000 80000 80000 80000\n",
      "  PGB_Variable_speed_Experiment1_test.csv             180000 20000 20000 20000 20000 20000 20000 20000 20000 20000\n",
      " PGB_Variable_speed_Experiment1_train.csv             720000 80000 80000 80000 80000 80000 80000 80000 80000 80000\n",
      "  PGB_Variable_speed_Experiment2_test.csv             180000 20000 20000 20000 20000 20000 20000 20000 20000 20000\n",
      " PGB_Variable_speed_Experiment2_train.csv             720000 80000 80000 80000 80000 80000 80000 80000 80000 80000\n",
      "  PGB_Variable_speed_Experiment3_test.csv             180000 20000 20000 20000 20000 20000 20000 20000 20000 20000\n",
      " PGB_Variable_speed_Experiment3_train.csv             720000 80000 80000 80000 80000 80000 80000 80000 80000 80000\n",
      "  PGB_Variable_speed_Experiment4_test.csv             180000 20000 20000 20000 20000 20000 20000 20000 20000 20000\n",
      " PGB_Variable_speed_Experiment4_train.csv             720000 80000 80000 80000 80000 80000 80000 80000 80000 80000\n",
      "  PGB_Variable_speed_Experiment5_test.csv             180000 20000 20000 20000 20000 20000 20000 20000 20000 20000\n",
      " PGB_Variable_speed_Experiment5_train.csv             720000 80000 80000 80000 80000 80000 80000 80000 80000 80000\n",
      "  PGB_Variable_speed_Experiment6_test.csv             180000 20000 20000 20000 20000 20000 20000 20000 20000 20000\n",
      " PGB_Variable_speed_Experiment6_train.csv             720000 80000 80000 80000 80000 80000 80000 80000 80000 80000\n",
      "  PGB_Variable_speed_Experiment7_test.csv             180000 20000 20000 20000 20000 20000 20000 20000 20000 20000\n",
      " PGB_Variable_speed_Experiment7_train.csv             720000 80000 80000 80000 80000 80000 80000 80000 80000 80000\n",
      "  PGB_Variable_speed_Experiment8_test.csv             180000 20000 20000 20000 20000 20000 20000 20000 20000 20000\n",
      " PGB_Variable_speed_Experiment8_train.csv             720000 80000 80000 80000 80000 80000 80000 80000 80000 80000\n",
      "  PGB_Variable_speed_Experiment9_test.csv             180000 20000 20000 20000 20000 20000 20000 20000 20000 20000\n",
      " PGB_Variable_speed_Experiment9_train.csv             720000 80000 80000 80000 80000 80000 80000 80000 80000 80000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "def overview_csv_files(directory):\n",
    "    data = []\n",
    "    all_faults = set()\n",
    "\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Check if the CSV is empty (aside from the header)\n",
    "            if df.shape[0] == 0:\n",
    "                # Delete the empty CSV file\n",
    "                os.remove(file_path)\n",
    "                print(f\"Deleted empty file: {file_path}\")\n",
    "                continue  # Skip further processing for this file\n",
    "\n",
    "            num_samples = len(df)\n",
    "            fault_distribution = Counter(df['Fault'])\n",
    "            all_faults.update(fault_distribution.keys())\n",
    "            data.append({'File Name': file, 'Number of Samples': num_samples, **fault_distribution})\n",
    "\n",
    "    if not data:  # If no data has been gathered, exit the function\n",
    "        print(\"No data found.\")\n",
    "        return\n",
    "\n",
    "    overview_df = pd.DataFrame(data)\n",
    "    for fault in all_faults:\n",
    "        if fault not in overview_df.columns:\n",
    "            overview_df[fault] = 0\n",
    "\n",
    "    cols = ['File Name', 'Number of Samples'] + sorted(all_faults)\n",
    "    overview_df = overview_df[cols]\n",
    "    overview_df.fillna(0, inplace=True)\n",
    "    overview_df.loc[:, 'Number of Samples':] = overview_df.loc[:, 'Number of Samples':].astype(int)\n",
    "\n",
    "    overview_df = overview_df.sort_values(by='File Name')\n",
    "    print(overview_df.to_string(index=False))\n",
    "\n",
    "# Example usage\n",
    "directory = '/home/ubuntu/dds_paper/DDS_Paper/data/DDS_Data_SEU/data/csvs'\n",
    "overview_csv_files(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_and_scale_data(csv_path, scaler=None, save_scaler_path=None):\n",
    "    \"\"\"\n",
    "    Loads data from a CSV file, scales the features (excluding the 'Fault' column), \n",
    "    and returns the scaled DataFrame. Optionally saves the scaler model.\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    data = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Separate features and target\n",
    "    features = data.columns[:-1]  # Assuming the last column is the target\n",
    "    X = data[features]\n",
    "    y = data['Fault']\n",
    "\n",
    "    # Apply scaling\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        if save_scaler_path:\n",
    "            joblib.dump(scaler, save_scaler_path)\n",
    "    else:\n",
    "        X_scaled = scaler.transform(X)\n",
    "    \n",
    "    # Combine scaled features with target\n",
    "    scaled_df = pd.DataFrame(X_scaled, columns=features)\n",
    "    scaled_df['Fault'] = y\n",
    "    \n",
    "    return scaled_df\n",
    "\n",
    "\n",
    "\n",
    "# Iterate over your dataset files\n",
    "for root, dirs, files in os.walk(csv_directory):\n",
    "    for file in sorted(files):\n",
    "        if file.endswith('.csv') and not file.endswith('_scaled.csv'):  # Process only unscaled .csv files\n",
    "            csv_path = os.path.join(root, file)\n",
    "            if 'train' in file:\n",
    "                # Handle training data\n",
    "                scaler_path = os.path.join(root, 'scaler_' + file.replace('.csv', '.joblib'))\n",
    "                scaled_train_df = load_and_scale_data(csv_path, save_scaler_path=scaler_path)\n",
    "                # Save the scaled training data\n",
    "                scaled_csv_path = csv_path.replace('.csv', '_scaled.csv')\n",
    "                scaled_train_df.to_csv(scaled_csv_path, index=False)\n",
    "            elif 'test' in file:\n",
    "                # Handle testing data\n",
    "                scaler_path = os.path.join(root, 'scaler_' + file.replace('_test.csv', '_train.joblib'))\n",
    "                scaler = joblib.load(scaler_path) if os.path.exists(scaler_path) else None\n",
    "                scaled_test_df = load_and_scale_data(csv_path, scaler=scaler)\n",
    "                # Save the scaled testing data\n",
    "                scaled_csv_path = csv_path.replace('.csv', '_scaled.csv')\n",
    "                scaled_test_df.to_csv(scaled_csv_path, index=False)\n",
    "\n",
    "            # Delete the original unscaled .csv file\n",
    "            os.remove(csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating sequences: 100%|██████████| 58/58 [02:33<00:00,  2.64s/it]\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 10  # Example: Define your desired sequence length\n",
    "\n",
    "\n",
    "def create_sequences(df, sequence_length):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    fault_types = df['Fault'].unique()\n",
    "\n",
    "    for fault in fault_types:\n",
    "        df_fault = df[df['Fault'] == fault]\n",
    "        X = df_fault.drop('Fault', axis=1).values\n",
    "        y = df_fault['Fault'].iloc[0]  # Updated to use iloc for consistency\n",
    "        \n",
    "        for i in range(len(df_fault) - sequence_length + 1):\n",
    "            sequences.append(X[i:i+sequence_length])\n",
    "            labels.append(fault)  # Keep the fault type as is\n",
    "    \n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "\n",
    "def save_sequences(input_directory, output_directory, sequence_length):\n",
    "    \"\"\"\n",
    "    Generates sequences and saves them as NumPy files, one for sequences and one for labels.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_directory: The directory with the original, scaled data files.\n",
    "    - output_directory: The directory where the NumPy sequence files will be saved.\n",
    "    - sequence_length: The number of consecutive samples in each sequence.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    for file_name in tqdm(os.listdir(input_directory), desc=\"Generating sequences\"):\n",
    "        if file_name.endswith('.csv'):\n",
    "            df = pd.read_csv(os.path.join(input_directory, file_name))\n",
    "            sequences, labels = create_sequences(df, sequence_length)\n",
    "            \n",
    "            # File names for sequences and labels\n",
    "            base_name = os.path.splitext(file_name)[0]\n",
    "            sequences_file_path = os.path.join(output_directory, f\"{base_name}_sequences.npy\")\n",
    "            labels_file_path = os.path.join(output_directory, f\"{base_name}_labels.npy\")\n",
    "            \n",
    "            # Save sequences and labels\n",
    "            np.save(sequences_file_path, sequences)\n",
    "            np.save(labels_file_path, labels)\n",
    "\n",
    "\n",
    "input_directory = \"/home/ubuntu/dds_paper/DDS_Paper/data/DDS_Data_SEU/data/csvs\"\n",
    "output_directory = \"/home/ubuntu/dds_paper/DDS_Paper/data/DDS_Data_SEU/data/csvs/sequences\"\n",
    "# Example call to save_sequences_as_csv (paths and sequence_length need to be defined)\n",
    "save_sequences(input_directory, output_directory, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: PGB_20_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/ltn/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:205: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m 1560/22498\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6:44\u001b[0m 19ms/step - accuracy: 0.7025 - loss: 0.7801"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "sequences_directory = \"/home/ubuntu/dds_paper/DDS_Paper/data/DDS_Data_SEU/data/csvs/sequences\"\n",
    "num_features = 8  # Based on the original number of features before sequencing\n",
    "sequence_length = 10  # Define your desired sequence length\n",
    "\n",
    "def lr_schedule(epoch, lr):\n",
    "    if epoch % 10 == 0 and epoch > 0:\n",
    "        return lr * 0.1\n",
    "    return lr\n",
    "\n",
    "def load_sequences(sequence_file_path, label_file_path):\n",
    "    sequences = np.load(sequence_file_path)\n",
    "    labels = np.load(label_file_path)\n",
    "    encoder = LabelEncoder()\n",
    "    labels_encoded = encoder.fit_transform(labels)\n",
    "    labels_onehot = to_categorical(labels_encoded)\n",
    "    return sequences, labels_onehot\n",
    "\n",
    "def create_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        LSTM(100, return_sequences=True, input_shape=input_shape),\n",
    "        Dropout(0.2),\n",
    "        LSTM(100),\n",
    "        Dropout(0.2),\n",
    "        Dense(100, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Prepare a list of base names to avoid redundancy\n",
    "processed_bases = set()\n",
    "\n",
    "# Iterate through the directory to find matching train and test pairs\n",
    "for file in sorted(os.listdir(sequences_directory)):\n",
    "    if \"_train_scaled_sequences.npy\" in file:\n",
    "        base_name = file.replace(\"_train_scaled_sequences.npy\", \"\")\n",
    "        if base_name in processed_bases:\n",
    "            continue  # Skip if this set has already been processed\n",
    "\n",
    "        train_sequence_file_path = os.path.join(sequences_directory, f\"{base_name}_train_scaled_sequences.npy\")\n",
    "        train_label_file_path = os.path.join(sequences_directory, f\"{base_name}_train_scaled_labels.npy\")\n",
    "        test_sequence_file_path = os.path.join(sequences_directory, f\"{base_name}_test_scaled_sequences.npy\")\n",
    "        test_label_file_path = os.path.join(sequences_directory, f\"{base_name}_test_scaled_labels.npy\")\n",
    "\n",
    "        # Check if corresponding test files exist\n",
    "        if os.path.exists(test_sequence_file_path) and os.path.exists(test_label_file_path):\n",
    "            print(f\"Processing: {base_name}\")\n",
    "            X_train, y_train = load_sequences(train_sequence_file_path, train_label_file_path)\n",
    "            X_test, y_test = load_sequences(test_sequence_file_path, test_label_file_path)\n",
    "\n",
    "            num_classes = y_train.shape[1]\n",
    "            input_shape = (sequence_length, num_features)\n",
    "            \n",
    "            model = create_model(input_shape, num_classes)\n",
    "            lr_scheduler = LearningRateScheduler(lr_schedule, verbose=0)\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "            model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, callbacks=[early_stopping, lr_scheduler], verbose=1)\n",
    "            \n",
    "            processed_bases.add(base_name)  # Mark this set as processed\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ltn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
