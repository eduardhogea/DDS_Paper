{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-15 15:10:47.325848: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-15 15:10:47.339608: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-15 15:10:47.460413: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-15 15:10:51.489281: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import tensorflow as tf\n",
    "from joblib import dump, load\n",
    "import ltn\n",
    "import csv\n",
    "import math\n",
    "import wandb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import os\n",
    "import joblib  # For saving the scaler model\n",
    "import os\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "dataset_path = '/home/ubuntu/dds_paper/DDS_Paper/data/DDS_Data_SEU'\n",
    "\n",
    "PGB_path = '/home/ubuntu/dds_paper/DDS_Paper/data/DDS_Data_SEU/PGB/PGB'\n",
    "RGB_path = '/home/ubuntu/dds_paper/DDS_Paper/data/DDS_Data_SEU/RGB/RGB'\n",
    "\n",
    "# Specify the CSV file path\n",
    "csv_file = '/home/ubuntu/dds_paper/DDS_Paper/data/data_robust.csv'\n",
    "preprocessor_file = 'preprocessor.joblib'\n",
    "\n",
    "train_path = '/home/ubuntu/dds_paper/DDS_Paper/data/train.csv'\n",
    "val_path = '/home/ubuntu/dds_paper/DDS_Paper/data/val.csv'\n",
    "\n",
    "np.random.seed(45)\n",
    "\n",
    "# Set the chunk size for reading the CSV\n",
    "chunk_size = 100000  # Adjust the chunk size according to your memory limitations\n",
    "\n",
    "\n",
    "# Directory containing your scaled CSV files\n",
    "csv_directory = '/home/ubuntu/dds_paper/DDS_Paper/data/DDS_Data_SEU/data/csvs'\n",
    "# Define your dataset directory\n",
    "data_root_folder = '/home/ubuntu/dds_paper/DDS_Paper/data/DDS_Data_SEU/PGB/PGB'\n",
    "\n",
    "sequence_length = 30  # Example: Define your desired sequence length\n",
    "sequences_directory = \"/home/ubuntu/dds_paper/DDS_Paper/data/DDS_Data_SEU/data/csvs/sequences\"\n",
    "num_features = 8  # Based on the original number of features before sequencing\n",
    "input_directory = \"/home/ubuntu/dds_paper/DDS_Paper/data/DDS_Data_SEU/data/csvs\"\n",
    "output_directory = \"/home/ubuntu/dds_paper/DDS_Paper/data/DDS_Data_SEU/data/csvs/sequences\"\n",
    "# Example call to save_sequences_as_csv (paths and sequence_length need to be defined)\n",
    "# Example usage\n",
    "directory = '/home/ubuntu/dds_paper/DDS_Paper/data/DDS_Data_SEU/data/csvs'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Variable_speed Variable_speed: 0file [00:00, ?file/s]\n",
      "Processing Variable_speed Experiment8:   0%|          | 0/9 [00:00<?, ?file/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Variable_speed Experiment8: 100%|██████████| 9/9 [00:09<00:00,  1.00s/file]\n",
      "Processing Variable_speed Experiment4: 100%|██████████| 9/9 [00:06<00:00,  1.46file/s]\n",
      "Processing Variable_speed Experiment6: 100%|██████████| 9/9 [00:06<00:00,  1.45file/s]\n",
      "Processing Variable_speed Experiment2: 100%|██████████| 9/9 [00:08<00:00,  1.08file/s]\n",
      "Processing Variable_speed Experiment5: 100%|██████████| 9/9 [00:07<00:00,  1.21file/s]\n",
      "Processing Variable_speed Experiment1: 100%|██████████| 9/9 [00:10<00:00,  1.11s/file]\n",
      "Processing Variable_speed Experiment9: 100%|██████████| 9/9 [00:07<00:00,  1.23file/s]\n",
      "Processing Variable_speed Experiment7: 100%|██████████| 9/9 [00:07<00:00,  1.13file/s]\n",
      "Processing Variable_speed Experiment3: 100%|██████████| 9/9 [00:07<00:00,  1.15file/s]\n",
      "Processing Variable_speed Experiment10: 100%|██████████| 9/9 [00:08<00:00,  1.05file/s]\n",
      "Processing 40_0 : 100%|██████████| 9/9 [00:08<00:00,  1.04file/s]\n",
      "Processing 30_4 : 100%|██████████| 9/9 [00:08<00:00,  1.02file/s]\n",
      "Processing 30_3 : 100%|██████████| 9/9 [00:09<00:00,  1.03s/file]\n",
      "Processing 20_0 : 100%|██████████| 9/9 [00:07<00:00,  1.16file/s]\n",
      "Processing 30_5 : 100%|██████████| 9/9 [00:06<00:00,  1.31file/s]\n",
      "Processing 30_2 : 100%|██████████| 9/9 [00:06<00:00,  1.33file/s]\n",
      "Processing 50_0 : 100%|██████████| 9/9 [00:06<00:00,  1.35file/s]\n",
      "Processing 30_0 : 100%|██████████| 9/9 [00:10<00:00,  1.16s/file]\n",
      "Processing 30_1 : 100%|██████████| 9/9 [00:07<00:00,  1.28file/s]\n"
     ]
    }
   ],
   "source": [
    "def extract_fault(file_name):\n",
    "    fault_mapping = {\n",
    "        '0Health': 'HEA', '1Chipped': 'CTF', '2Miss': 'MTF', \n",
    "        '3Root': 'RCF', '4Surface': 'SWF', '5Ball': 'BWF', \n",
    "        '6Combination': 'CWF', '7Inner': 'IRF', '8Outer': 'ORF'\n",
    "    }\n",
    "    for key, value in fault_mapping.items():\n",
    "        if key in file_name:\n",
    "            return value\n",
    "    return None\n",
    "\n",
    "def make_csv_writer(csv_file):\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow(['Channel1', 'Channel2', 'Channel3', 'Channel4', 'Channel5', 'Channel6', 'Channel7', 'Channel8', 'Fault'])\n",
    "    return csv_writer\n",
    "\n",
    "def generate_csv(output_directory, root_path, speed, experiment, files):\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    train_filename_suffix = f\"{speed}_{experiment}_train\" if experiment else f\"{speed}_train\"\n",
    "    test_filename_suffix = f\"{speed}_{experiment}_test\" if experiment else f\"{speed}_test\"\n",
    "    \n",
    "    train_output_file_path = os.path.join(output_directory, f\"PGB_{train_filename_suffix}.csv\")\n",
    "    test_output_file_path = os.path.join(output_directory, f\"PGB_{test_filename_suffix}.csv\")\n",
    "    \n",
    "    with open(train_output_file_path, 'w', newline='', encoding='utf-8') as train_csvfile, \\\n",
    "        open(test_output_file_path, 'w', newline='', encoding='utf-8') as test_csvfile:\n",
    "        train_csv_writer = make_csv_writer(train_csvfile)\n",
    "        test_csv_writer = make_csv_writer(test_csvfile)\n",
    "        \n",
    "        for file in tqdm(files, desc=f\"Processing {speed} {experiment}\", unit=\"file\"):\n",
    "            fault_type = extract_fault(file)\n",
    "            # Only append 'speed' directory for non-variable speed cases\n",
    "            if experiment:\n",
    "                file_path = os.path.join(root_path, file)  # Already includes 'Variable_speed/Experiment#'\n",
    "            else:\n",
    "                file_path = os.path.join(root_path, file)  # 'root_path' already includes 'speed' directory\n",
    "            \n",
    "            data = pd.read_csv(file_path, sep='\\t', header=None, encoding='ISO-8859-1', skiprows=1, nrows=10000)\n",
    "            train_samples, test_samples = data.iloc[:80000, :], data.iloc[8000:10000, :]\n",
    "            \n",
    "            for index, row in train_samples.iterrows():\n",
    "                train_csv_writer.writerow(row[:8].tolist() + [fault_type])\n",
    "            \n",
    "            for index, row in test_samples.iterrows():\n",
    "                test_csv_writer.writerow(row[:8].tolist() + [fault_type])\n",
    "\n",
    "def process_pgb_data(data_root_folder, csv_directory):\n",
    "    for root, dirs, files in os.walk(data_root_folder):\n",
    "        parts = root.split(os.sep)\n",
    "        if 'Variable_speed' in parts:\n",
    "            speed = \"Variable_speed\"\n",
    "            experiment_dir = parts[-1]  # Get the last part as the experiment name\n",
    "            exp_files = [f for f in os.listdir(root) if f.endswith('.txt')]\n",
    "            # Pass the 'root' directly without modifying it for variable speed\n",
    "            generate_csv(csv_directory, root, speed, experiment_dir, exp_files)\n",
    "        elif 'PGB' in parts and files:\n",
    "            speed = parts[-1]  # Last part of 'root' is the speed directory\n",
    "            # For non-variable speed, pass the 'root' directly\n",
    "            generate_csv(csv_directory, root, speed, '', files)\n",
    "\n",
    "\n",
    "\n",
    "process_pgb_data(data_root_folder, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted empty file: /home/ubuntu/dds_paper/DDS_Paper/data/DDS_Data_SEU/data/csvs/sequences/PGB_Variable_speed_Variable_speed_train.csv\n",
      "Deleted empty file: /home/ubuntu/dds_paper/DDS_Paper/data/DDS_Data_SEU/data/csvs/sequences/PGB_Variable_speed_Variable_speed_test.csv\n",
      "                                       File Name  Number of Samples   BWF   CTF   CWF   HEA   IRF   MTF   ORF   RCF   SWF\n",
      "                        PGB_20_0_test_scaled.csv              18000  2000  2000  2000  2000  2000  2000  2000  2000  2000\n",
      "                       PGB_20_0_train_scaled.csv              90000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
      "                        PGB_30_0_test_scaled.csv              18000  2000  2000  2000  2000  2000  2000  2000  2000  2000\n",
      "                       PGB_30_0_train_scaled.csv              90000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
      "                        PGB_30_1_test_scaled.csv              18000  2000  2000  2000  2000  2000  2000  2000  2000  2000\n",
      "                       PGB_30_1_train_scaled.csv              90000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
      "                        PGB_30_2_test_scaled.csv              18000  2000  2000  2000  2000  2000  2000  2000  2000  2000\n",
      "                       PGB_30_2_train_scaled.csv              90000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
      "                        PGB_30_3_test_scaled.csv              18000  2000  2000  2000  2000  2000  2000  2000  2000  2000\n",
      "                       PGB_30_3_train_scaled.csv              90000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
      "                        PGB_30_4_test_scaled.csv              18000  2000  2000  2000  2000  2000  2000  2000  2000  2000\n",
      "                       PGB_30_4_train_scaled.csv              90000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
      "                        PGB_30_5_test_scaled.csv              18000  2000  2000  2000  2000  2000  2000  2000  2000  2000\n",
      "                       PGB_30_5_train_scaled.csv              90000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
      "                        PGB_40_0_test_scaled.csv              18000  2000  2000  2000  2000  2000  2000  2000  2000  2000\n",
      "                       PGB_40_0_train_scaled.csv              90000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
      "                        PGB_50_0_test_scaled.csv              18000  2000  2000  2000  2000  2000  2000  2000  2000  2000\n",
      "                       PGB_50_0_train_scaled.csv              90000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
      " PGB_Variable_speed_Experiment10_test_scaled.csv              18000  2000  2000  2000  2000  2000  2000  2000  2000  2000\n",
      "PGB_Variable_speed_Experiment10_train_scaled.csv              90000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
      "  PGB_Variable_speed_Experiment1_test_scaled.csv              18000  2000  2000  2000  2000  2000  2000  2000  2000  2000\n",
      " PGB_Variable_speed_Experiment1_train_scaled.csv              90000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
      "  PGB_Variable_speed_Experiment2_test_scaled.csv              18000  2000  2000  2000  2000  2000  2000  2000  2000  2000\n",
      " PGB_Variable_speed_Experiment2_train_scaled.csv              90000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
      "  PGB_Variable_speed_Experiment3_test_scaled.csv              18000  2000  2000  2000  2000  2000  2000  2000  2000  2000\n",
      " PGB_Variable_speed_Experiment3_train_scaled.csv              90000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
      "  PGB_Variable_speed_Experiment4_test_scaled.csv              18000  2000  2000  2000  2000  2000  2000  2000  2000  2000\n",
      " PGB_Variable_speed_Experiment4_train_scaled.csv              90000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
      "  PGB_Variable_speed_Experiment5_test_scaled.csv              18000  2000  2000  2000  2000  2000  2000  2000  2000  2000\n",
      " PGB_Variable_speed_Experiment5_train_scaled.csv              90000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
      "  PGB_Variable_speed_Experiment6_test_scaled.csv              18000  2000  2000  2000  2000  2000  2000  2000  2000  2000\n",
      " PGB_Variable_speed_Experiment6_train_scaled.csv              90000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
      "  PGB_Variable_speed_Experiment7_test_scaled.csv              18000  2000  2000  2000  2000  2000  2000  2000  2000  2000\n",
      " PGB_Variable_speed_Experiment7_train_scaled.csv              90000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
      "  PGB_Variable_speed_Experiment8_test_scaled.csv              18000  2000  2000  2000  2000  2000  2000  2000  2000  2000\n",
      " PGB_Variable_speed_Experiment8_train_scaled.csv              90000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
      "  PGB_Variable_speed_Experiment9_test_scaled.csv              18000  2000  2000  2000  2000  2000  2000  2000  2000  2000\n",
      " PGB_Variable_speed_Experiment9_train_scaled.csv              90000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "def overview_csv_files(directory):\n",
    "    data = []\n",
    "    all_faults = set()\n",
    "\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Check if the CSV is empty (aside from the header)\n",
    "            if df.shape[0] == 0:\n",
    "                # Delete the empty CSV file\n",
    "                os.remove(file_path)\n",
    "                print(f\"Deleted empty file: {file_path}\")\n",
    "                continue  # Skip further processing for this file\n",
    "\n",
    "            num_samples = len(df)\n",
    "            fault_distribution = Counter(df['Fault'])\n",
    "            all_faults.update(fault_distribution.keys())\n",
    "            data.append({'File Name': file, 'Number of Samples': num_samples, **fault_distribution})\n",
    "\n",
    "    if not data:  # If no data has been gathered, exit the function\n",
    "        print(\"No data found.\")\n",
    "        return\n",
    "\n",
    "    overview_df = pd.DataFrame(data)\n",
    "    for fault in all_faults:\n",
    "        if fault not in overview_df.columns:\n",
    "            overview_df[fault] = 0\n",
    "\n",
    "    cols = ['File Name', 'Number of Samples'] + sorted(all_faults)\n",
    "    overview_df = overview_df[cols]\n",
    "    overview_df.fillna(0, inplace=True)\n",
    "    overview_df.loc[:, 'Number of Samples':] = overview_df.loc[:, 'Number of Samples':].astype(int)\n",
    "\n",
    "    overview_df = overview_df.sort_values(by='File Name')\n",
    "    print(overview_df.to_string(index=False))\n",
    "\n",
    "overview_csv_files(output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_and_scale_data(csv_path, scaler=None, save_scaler_path=None):\n",
    "    \"\"\"\n",
    "    Loads data from a CSV file, scales the features (excluding the 'Fault' column), \n",
    "    and returns the scaled DataFrame. Optionally saves the scaler model.\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    data = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Separate features and target\n",
    "    features = data.columns[:-1]  # Assuming the last column is the target\n",
    "    X = data[features]\n",
    "    y = data['Fault']\n",
    "\n",
    "    # Apply scaling\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        if save_scaler_path:\n",
    "            joblib.dump(scaler, save_scaler_path)\n",
    "    else:\n",
    "        X_scaled = scaler.transform(X)\n",
    "    \n",
    "    # Combine scaled features with target\n",
    "    scaled_df = pd.DataFrame(X_scaled, columns=features)\n",
    "    scaled_df['Fault'] = y\n",
    "    \n",
    "    return scaled_df\n",
    "\n",
    "\n",
    "\n",
    "# Iterate over your dataset files\n",
    "for root, dirs, files in os.walk(csv_directory):\n",
    "    for file in sorted(files):\n",
    "        if file.endswith('.csv') and not file.endswith('_scaled.csv'):  # Process only unscaled .csv files\n",
    "            csv_path = os.path.join(root, file)\n",
    "            if 'train' in file:\n",
    "                # Handle training data\n",
    "                scaler_path = os.path.join(root, 'scaler_' + file.replace('.csv', '.joblib'))\n",
    "                scaled_train_df = load_and_scale_data(csv_path, save_scaler_path=scaler_path)\n",
    "                # Save the scaled training data\n",
    "                scaled_csv_path = csv_path.replace('.csv', '_scaled.csv')\n",
    "                scaled_train_df.to_csv(scaled_csv_path, index=False)\n",
    "            elif 'test' in file:\n",
    "                # Handle testing data\n",
    "                scaler_path = os.path.join(root, 'scaler_' + file.replace('_test.csv', '_train.joblib'))\n",
    "                scaler = joblib.load(scaler_path) if os.path.exists(scaler_path) else None\n",
    "                scaled_test_df = load_and_scale_data(csv_path, scaler=scaler)\n",
    "                # Save the scaled testing data\n",
    "                scaled_csv_path = csv_path.replace('.csv', '_scaled.csv')\n",
    "                scaled_test_df.to_csv(scaled_csv_path, index=False)\n",
    "\n",
    "            # Delete the original unscaled .csv file\n",
    "            os.remove(csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating sequences: 100%|██████████| 57/57 [00:14<00:00,  3.94it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def create_sequences(df, sequence_length):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    fault_types = df['Fault'].unique()\n",
    "\n",
    "    for fault in fault_types:\n",
    "        df_fault = df[df['Fault'] == fault]\n",
    "        X = df_fault.drop('Fault', axis=1).values\n",
    "        y = df_fault['Fault'].iloc[0]  # Updated to use iloc for consistency\n",
    "        \n",
    "        for i in range(len(df_fault) - sequence_length + 1):\n",
    "            sequences.append(X[i:i+sequence_length])\n",
    "            labels.append(fault)  # Keep the fault type as is\n",
    "    \n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "\n",
    "def save_sequences(input_directory, output_directory, sequence_length):\n",
    "    \"\"\"\n",
    "    Generates sequences and saves them as NumPy files, one for sequences and one for labels.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_directory: The directory with the original, scaled data files.\n",
    "    - output_directory: The directory where the NumPy sequence files will be saved.\n",
    "    - sequence_length: The number of consecutive samples in each sequence.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    for file_name in tqdm(os.listdir(input_directory), desc=\"Generating sequences\"):\n",
    "        if file_name.endswith('.csv'):\n",
    "            df = pd.read_csv(os.path.join(input_directory, file_name))\n",
    "            sequences, labels = create_sequences(df, sequence_length)\n",
    "            \n",
    "            # File names for sequences and labels\n",
    "            base_name = os.path.splitext(file_name)[0]\n",
    "            sequences_file_path = os.path.join(output_directory, f\"{base_name}_sequences.npy\")\n",
    "            labels_file_path = os.path.join(output_directory, f\"{base_name}_labels.npy\")\n",
    "            \n",
    "            # Save sequences and labels\n",
    "            np.save(sequences_file_path, sequences)\n",
    "            np.save(labels_file_path, labels)\n",
    "\n",
    "\n",
    "save_sequences(output_directory, output_directory, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 133/133 [00:18<00:00,  7.35it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "\n",
    "def extract_speed_from_filename(file_name):\n",
    "    \"\"\"\n",
    "    Extracts the speed from the filename.\n",
    "    Returns the numeric speed for fixed speeds, or -1 for variable speeds.\n",
    "    \"\"\"\n",
    "    fixed_speed_match = re.search(r\"PGB_(\\d+)_\", file_name)\n",
    "    if fixed_speed_match:\n",
    "        return int(fixed_speed_match.group(1))\n",
    "    variable_speed_match = re.search(r\"Variable_speed\", file_name)\n",
    "    if variable_speed_match:\n",
    "        return -1  # Special value for variable speeds\n",
    "    return None\n",
    "\n",
    "def add_speed_feature_and_save(input_directory, output_directory, sequence_length):\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    for file_name in tqdm(os.listdir(input_directory), desc=\"Processing files\"):\n",
    "        if file_name.endswith('.csv'):\n",
    "            speed = extract_speed_from_filename(file_name)\n",
    "            df = pd.read_csv(os.path.join(input_directory, file_name))\n",
    "            df['Speed'] = speed  # Add speed as a new column\n",
    "            \n",
    "            sequences, labels = create_sequences(df, sequence_length)\n",
    "            \n",
    "            base_name = os.path.splitext(file_name)[0]\n",
    "            sequences_file_path = os.path.join(output_directory, f\"{base_name}_sequences.npy\")\n",
    "            labels_file_path = os.path.join(output_directory, f\"{base_name}_labels.npy\")\n",
    "            \n",
    "            np.save(sequences_file_path, sequences)\n",
    "            np.save(labels_file_path, labels)\n",
    "\n",
    "# Remember to replace `input_directory`, `output_directory`, and `sequence_length` with your actual values\n",
    "add_speed_feature_and_save(output_directory, output_directory, sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      "Sequence:\n",
      "[[ 0.8001289  -0.31234846 -0.08071305  0.03526632 -1.36544478 -0.09371\n",
      "   0.10288366  0.59093989 -1.        ]\n",
      " [ 0.7938483  -0.26989087 -0.19563853 -0.06400364 -1.33334195 -0.78049913\n",
      "  -0.62416221 -0.11157456 -1.        ]\n",
      " [ 0.78421284 -0.40112343 -0.10723432 -0.08173042 -1.23893428 -0.01211129\n",
      "  -0.18264708  0.60778099 -1.        ]\n",
      " [ 0.78175522 -0.44358103  0.17565918 -0.35117746 -1.26068817 -0.73969978\n",
      "  -0.54749192  0.01112488 -1.        ]\n",
      " [ 0.77820531 -0.55937446  0.06957412 -0.05336757 -1.2399903  -0.5799023\n",
      "  -0.44173979  0.19637698 -1.        ]\n",
      " [ 0.77890749 -0.15409743  0.19334002 -0.23418072 -1.32848428 -0.16170892\n",
      "  -0.04781312  0.76897437 -1.        ]\n",
      " [ 0.78171621 -0.6134114  -0.47411182 -0.43272065 -1.47632627 -0.61390177\n",
      "  -0.49197205 -0.17653309 -1.        ]\n",
      " [ 0.7852271  -0.28533    -0.3061438  -0.28736106 -1.44021058 -0.10730978\n",
      "  -0.10333299  0.55725769 -1.        ]\n",
      " [ 0.78901107 -0.22743328 -0.17353748 -0.41499387 -1.38360822  0.22928489\n",
      "  -0.09011397  0.59334576 -1.        ]\n",
      " [ 0.79626691 -0.3470865  -0.04535136 -0.24127143 -1.40599572 -0.74649967\n",
      "  -0.90176154 -0.02255732 -1.        ]\n",
      " [ 0.79860751  0.01187316 -0.21773959  0.14871771 -1.46217568  0.11708667\n",
      "  -0.32541245  0.41531128 -1.        ]\n",
      " [ 0.79802236 -0.44744081 -0.31056401  0.02463026 -1.45964124 -0.0869101\n",
      "  -0.15620905  0.4802698  -1.        ]\n",
      " [ 0.80114316 -0.30848868 -0.33708528 -0.30863319 -1.34284608 -0.79069897\n",
      "  -0.80658463  0.04240121 -1.        ]\n",
      " [ 0.80059702 -0.17725612 -0.2221598  -0.14200147 -1.14896187 -0.17530871\n",
      "   0.10023986  0.74491566 -1.        ]\n",
      " [ 0.79259998 -0.31234846  0.00327096  0.06008382 -1.08560102 -0.20250828\n",
      "  -0.43116458  0.35757036 -1.        ]\n",
      " [ 0.7934582  -0.37024518  0.06957412 -0.34763211 -1.00745597 -0.22290795\n",
      "  -0.2804678   0.48748742 -1.        ]\n",
      " [ 0.78581225 -0.42428212 -0.3857076  -0.42208458 -1.02963227 -0.0053114\n",
      "  -0.05838833  0.67995713 -1.        ]\n",
      " [ 0.78401779 -0.42428212 -0.20447895 -0.34763211 -1.14009135 -0.66490096\n",
      "  -0.41001416  0.00390727 -1.        ]\n",
      " [ 0.77430432 -0.17339634 -0.40780865 -0.29090641 -1.23639985  0.01508828\n",
      "   0.28001847  0.7497274  -1.        ]\n",
      " [ 0.77360214 -0.44744081  0.02979222 -0.19872716 -1.40557332 -0.51190338\n",
      "  -0.75635237  0.12660671 -1.        ]\n",
      " [ 0.77211976 -0.1193594  -0.27520233 -0.17390967 -1.53123901  0.06948742\n",
      "   0.14782831  0.48267568 -1.        ]\n",
      " [ 0.77492848 -0.32006803 -0.32824486 -0.19872716 -1.62226743  0.13068645\n",
      "  -0.10068918  0.45621109 -1.        ]\n",
      " [ 0.77875145 -0.21199415 -0.00556947 -0.11718398 -1.67042167 -0.76009945\n",
      "  -0.58186136  0.26133551 -1.        ]\n",
      " [ 0.78253541 -0.54393534  0.02537201 -0.11718398 -1.63937486  0.00828839\n",
      "   0.2244986   0.58612815 -1.        ]\n",
      " [ 0.78511007 -0.20041481 -0.36360654 -0.35826817 -1.49998099 -0.62750155\n",
      "  -0.71933913  0.04961883 -1.        ]\n",
      " [ 0.7915077  -0.13093875 -0.31498423  0.06717453 -1.22034843 -0.24330763\n",
      "   0.02621337  0.44658761 -1.        ]\n",
      " [ 0.79583781 -0.3046289   0.15797834 -0.02855008 -0.96204736  0.222485\n",
      "   0.03678858  0.67755126 -1.        ]\n",
      " [ 0.79595484 -0.52849621 -0.11165453 -0.24481679 -0.90037614 -0.87569762\n",
      "  -0.62680602  0.05443057 -1.        ]\n",
      " [ 0.80114316 -0.09620072  0.01653159 -0.20227252 -0.87693262 -0.51870327\n",
      "  -0.08747017  0.39365843 -1.        ]\n",
      " [ 0.79997286 -0.74850374 -0.27520233 -0.12427469 -0.98021081 -0.43710456\n",
      "  -0.3650695   0.44418173 -1.        ]]\n",
      "Label:\n",
      "MTF\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def display_samples(sequences_file_path, labels_file_path, num_samples=1):\n",
    "    \"\"\"\n",
    "    Displays a specified number of samples from the sequences and labels .npy files.\n",
    "    \n",
    "    Parameters:\n",
    "    - sequences_file_path: Path to the .npy file containing sequences.\n",
    "    - labels_file_path: Path to the .npy file containing labels.\n",
    "    - num_samples: Number of samples to display. Default is 5.\n",
    "    \"\"\"\n",
    "    # Load the sequences and labels\n",
    "    sequences = np.load(sequences_file_path)\n",
    "    labels = np.load(labels_file_path)\n",
    "    \n",
    "    # Determine the number of samples to display (cannot exceed the length of the data)\n",
    "    num_samples = min(num_samples, len(sequences))\n",
    "    \n",
    "    # Display the specified number of samples\n",
    "    for i in range(num_samples):\n",
    "        print(f\"Sample {i+1}:\")\n",
    "        print(\"Sequence:\")\n",
    "        print(sequences[i])\n",
    "        print(\"Label:\")\n",
    "        print(labels[i])\n",
    "        print(\"-\" * 50)  # Separator for readability\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "display_samples('/home/ubuntu/dds_paper/DDS_Paper/data/DDS_Data_SEU/data/csvs/sequences/PGB_Variable_speed_Experiment10_train_scaled_sequences.npy', '/home/ubuntu/dds_paper/DDS_Paper/data/DDS_Data_SEU/data/csvs/sequences/PGB_Variable_speed_Experiment10_test_scaled_labels.npy', num_samples=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging train sequences: 100%|██████████| 19/19 [00:15<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sequences data merged and saved to /home/ubuntu/dds_paper/DDS_Paper/data/DDS_Data_SEU/data/csvs/sequences/train_merged_sequences.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging train labels: 100%|██████████| 19/19 [00:03<00:00,  5.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels data merged and saved to /home/ubuntu/dds_paper/DDS_Paper/data/DDS_Data_SEU/data/csvs/sequences/train_merged_labels.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging test sequences: 100%|██████████| 19/19 [00:04<00:00,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sequences data merged and saved to /home/ubuntu/dds_paper/DDS_Paper/data/DDS_Data_SEU/data/csvs/sequences/test_merged_sequences.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging test labels: 100%|██████████| 19/19 [00:00<00:00, 46.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test labels data merged and saved to /home/ubuntu/dds_paper/DDS_Paper/data/DDS_Data_SEU/data/csvs/sequences/test_merged_labels.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def memmap_append_and_save(input_directory, output_directory, dataset_type, file_type):\n",
    "    output_file_path = os.path.join(output_directory, f\"{dataset_type}_merged_{file_type}.npy\")\n",
    "\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    merged_data = None\n",
    "    current_size = 0\n",
    "\n",
    "    file_names = [fn for fn in os.listdir(input_directory) if fn.endswith(f'_{file_type}.npy') and dataset_type in fn]\n",
    "    for file_name in tqdm(file_names, desc=f\"Merging {dataset_type} {file_type}\"):\n",
    "        path = os.path.join(input_directory, file_name)\n",
    "        data = np.load(path)\n",
    "\n",
    "        # Adjust for both 1D and 2D+ data\n",
    "        new_shape = (current_size + data.shape[0],) + data.shape[1:] if len(data.shape) > 1 else (current_size + data.shape[0],)\n",
    "        if merged_data is None:\n",
    "            merged_data = np.memmap(output_file_path, dtype=data.dtype, mode='w+', shape=new_shape)\n",
    "            merged_data[:current_size+data.shape[0]] = data\n",
    "        else:\n",
    "            # Ensure any changes are written and the file is properly resized\n",
    "            merged_data.flush()\n",
    "            merged_data._mmap.close()\n",
    "            os.truncate(output_file_path, np.prod(new_shape) * data.itemsize)\n",
    "            merged_data = np.memmap(output_file_path, dtype=data.dtype, mode='r+', shape=new_shape)\n",
    "\n",
    "        merged_data[current_size:current_size+data.shape[0]] = data\n",
    "        current_size += data.shape[0]\n",
    "\n",
    "    print(f\"{dataset_type.capitalize()} {file_type} data merged and saved to {output_file_path}\")\n",
    "\n",
    "def merge_npy_files_with_memmap_separated(input_directory, output_directory):\n",
    "    for dataset_type in ['train', 'test']:\n",
    "        for file_type in ['sequences', 'labels']:\n",
    "            memmap_append_and_save(input_directory, output_directory, dataset_type, file_type)\n",
    "\n",
    "# Replace `input_directory` and `output_directory` with your actual paths\n",
    "merge_npy_files_with_memmap_separated(output_directory, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from keras.regularizers import l2\n",
    "\n",
    "\n",
    "def lr_schedule(epoch, lr):\n",
    "    if epoch % 10 == 0 and epoch > 0:\n",
    "        return lr * 0.1\n",
    "    return lr\n",
    "\n",
    "def load_sequences(sequence_file_path, label_file_path):\n",
    "    sequences = np.load(sequence_file_path)\n",
    "    labels = np.load(label_file_path)\n",
    "    encoder = LabelEncoder()\n",
    "    labels_encoded = encoder.fit_transform(labels)\n",
    "    labels_onehot = to_categorical(labels_encoded)\n",
    "    return sequences, labels_onehot\n",
    "\n",
    "# def create_model(input_shape, num_classes):\n",
    "#     model = Sequential([\n",
    "#         LSTM(100, return_sequences=True, input_shape=input_shape),\n",
    "#         Dropout(0.2),\n",
    "#         LSTM(100),\n",
    "#         Dropout(0.2),\n",
    "#         Dense(100, activation='relu'),\n",
    "#         Dense(num_classes, activation='softmax')\n",
    "#     ])\n",
    "#     model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "def create_model(input_shape, num_classes, l2_reg=0.001):\n",
    "    model = Sequential([\n",
    "        LSTM(256, return_sequences=True, input_shape=input_shape, kernel_regularizer=l2(l2_reg)),\n",
    "        Dropout(0.3),\n",
    "        LSTM(128, return_sequences=True, kernel_regularizer=l2(l2_reg)),\n",
    "        Dropout(0.3),\n",
    "        LSTM(64, kernel_regularizer=l2(l2_reg)),\n",
    "        Dropout(0.3),\n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Prepare a list of base names to avoid redundancy\n",
    "processed_bases = set()\n",
    "\n",
    "# Iterate through the directory to find matching train and test pairs\n",
    "for file in sorted(os.listdir(sequences_directory)):\n",
    "    if \"_train_scaled_sequences.npy\" in file:\n",
    "        base_name = file.replace(\"_train_scaled_sequences.npy\", \"\")\n",
    "        if base_name in processed_bases:\n",
    "            continue  # Skip if this set has already been processed\n",
    "\n",
    "        train_sequence_file_path = os.path.join(sequences_directory, f\"{base_name}_train_scaled_sequences.npy\")\n",
    "        train_label_file_path = os.path.join(sequences_directory, f\"{base_name}_train_scaled_labels.npy\")\n",
    "        test_sequence_file_path = os.path.join(sequences_directory, f\"{base_name}_test_scaled_sequences.npy\")\n",
    "        test_label_file_path = os.path.join(sequences_directory, f\"{base_name}_test_scaled_labels.npy\")\n",
    "\n",
    "        # Check if corresponding test files exist\n",
    "        if os.path.exists(test_sequence_file_path) and os.path.exists(test_label_file_path):\n",
    "            print(f\"Processing: {base_name}\")\n",
    "            X_train, y_train = load_sequences(train_sequence_file_path, train_label_file_path)\n",
    "            X_test, y_test = load_sequences(test_sequence_file_path, test_label_file_path)\n",
    "\n",
    "            num_classes = y_train.shape[1]\n",
    "            input_shape = (sequence_length, num_features)\n",
    "            \n",
    "            model = create_model(input_shape, num_classes)\n",
    "            lr_scheduler = LearningRateScheduler(lr_schedule, verbose=0)\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
    "\n",
    "            model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=512, callbacks=[early_stopping, lr_scheduler], verbose=1)\n",
    "            \n",
    "            processed_bases.add(base_name)  # Mark this set as processed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ltn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
