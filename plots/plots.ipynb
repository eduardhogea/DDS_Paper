{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data\n",
    "cf1_results = pd.read_csv('/content/cf1_results.csv')\n",
    "cf1_results_ltn = pd.read_csv('/content/cf1_results_ltn.csv')\n",
    "\n",
    "# Calculate the last epoch of the normal model\n",
    "last_normal_epoch = cf1_results['Epoch'].max()\n",
    "\n",
    "# Adjust the LTN model epochs to start right after the last epoch of the normal model\n",
    "cf1_results_ltn['Adjusted Epoch'] = cf1_results_ltn['Epoch'] + last_normal_epoch\n",
    "\n",
    "# Group by Epoch and calculate mean accuracy for the normal model\n",
    "avg_cf1_results = cf1_results.groupby('Epoch').agg({'Validation Accuracy': 'mean'}).reset_index()\n",
    "\n",
    "# Find the maximum test accuracy for each fold and speed in the LTN model\n",
    "max_accuracy_per_fold_speed = cf1_results_ltn.groupby(['Fold', 'Speed', 'Adjusted Epoch']).agg({'test_accuracy': 'max'}).reset_index()\n",
    "\n",
    "# Calculate the mean of these maximum accuracies for each adjusted epoch\n",
    "avg_max_accuracy_per_epoch = max_accuracy_per_fold_speed.groupby('Adjusted Epoch').agg({'test_accuracy': 'mean'}).reset_index()\n",
    "\n",
    "# Combine averaged data for a continuous plot\n",
    "combined_avg_results = pd.concat([\n",
    "    avg_cf1_results.rename(columns={'Validation Accuracy': 'Accuracy', 'Epoch': 'Extended Epoch'}),\n",
    "    avg_max_accuracy_per_epoch.rename(columns={'test_accuracy': 'Accuracy', 'Adjusted Epoch': 'Extended Epoch'})\n",
    "])\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.plot(combined_avg_results['Extended Epoch'], combined_avg_results['Accuracy'], label='Average Model Accuracy', marker='o')\n",
    "plt.title('Average Model Accuracy Across Extended Epochs')\n",
    "plt.xlabel('Extended Epoch (Normal + LTN)')\n",
    "plt.ylabel('Average Test Accuracy')\n",
    "plt.axvline(x=last_normal_epoch, color='r', linestyle='--', label='Start of LTN Model')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1252591/3846186171.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Third-party library imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mltn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'joblib'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Standard library imports\n",
    "import argparse\n",
    "import csv\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "\n",
    "# Append config directory to sys.path\n",
    "script_dir = os.path.abspath(\"/home/ubuntu/dds_paper/DDS_Paper/plots/plots.ipynb\")  # Absolute dir the script is in\n",
    "sys.path.append(os.path.join(script_dir, '..', 'config'))\n",
    "\n",
    "# Third-party library imports\n",
    "import joblib\n",
    "import ltn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks, regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Input\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy, SparseCategoricalCrossentropy\n",
    "from tqdm import tqdm\n",
    "from numpy import mean\n",
    "\n",
    "# Local module imports\n",
    "import config as config\n",
    "from model_creation import LSTMModel, lr_schedule\n",
    "from sequence_generation import load_sequences, save_sequences\n",
    "from model_evaluation import kfold_cross_validation, normalize_importances, permutation_importance_per_class\n",
    "from pgb_data_processing import overview_csv_files, process_pgb_data\n",
    "from data_scaling import load_and_scale_data\n",
    "from util import concatenate_and_delete_ltn_csv_files\n",
    "import commons as commons\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from itertools import cycle\n",
    "\n",
    "# Assuming 'num_classes' is defined (number of unique classes)\n",
    "y_val_bin = label_binarize(y_val_fold, classes=[i for i in range(num_classes)])\n",
    "\n",
    "# Colors cycle for plotting\n",
    "colors = cycle(['blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'black', 'pink', 'lightblue'])\n",
    "\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=n_splits, shuffle=False)\n",
    "counter = 0\n",
    "console = Console()\n",
    "processed_bases = set()\n",
    "\n",
    "if os.path.exists(processed_file_tracker):\n",
    "    with open(processed_file_tracker, \"r\") as file:\n",
    "        processed_bases = set(file.read().splitlines())\n",
    "\n",
    "metrics_summary = []\n",
    "\n",
    "for file in sorted(os.listdir(sequences_directory)):\n",
    "    if \"_train_scaled_sequences.npy\" in file:\n",
    "        \n",
    "        if counter >= S:\n",
    "            break\n",
    "        \n",
    "        base_name = file.replace(\"_train_scaled_sequences.npy\", \"\")\n",
    "        if base_name in processed_bases:\n",
    "            continue\n",
    "        processed_bases.add(base_name)\n",
    "        counter+=1\n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "        # Load sequences and labels\n",
    "        train_sequence_file_path = os.path.join(sequences_directory, f\"{base_name}_train_scaled_sequences.npy\")\n",
    "        train_label_file_path = os.path.join(sequences_directory, f\"{base_name}_train_scaled_labels.npy\")\n",
    "        X_train, y_train = load_sequences(train_sequence_file_path, train_label_file_path)\n",
    "        \n",
    "        test_sequence_file_path = os.path.join(sequences_directory, f\"{base_name}_test_scaled_sequences.npy\")\n",
    "        test_label_file_path = os.path.join(sequences_directory, f\"{base_name}_test_scaled_labels.npy\")\n",
    "        X_test, y_test = load_sequences(test_sequence_file_path, test_label_file_path)\n",
    "\n",
    "        # Shuffle the sequences and corresponding labels. Before this they were kept ordered.\n",
    "        train_indices = np.arange(len(X_train))\n",
    "        np.random.shuffle(train_indices)\n",
    "        X_train = X_train[train_indices]\n",
    "        y_train = y_train[train_indices]\n",
    "\n",
    "        test_indices = np.arange(len(X_test))\n",
    "        np.random.shuffle(test_indices)\n",
    "        X_test = X_test[test_indices]\n",
    "        y_test = y_test[test_indices]\n",
    "\n",
    "        # Merge for cross-validation\n",
    "        X = np.concatenate((X_train, X_test), axis=0)\n",
    "        y = np.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "        input_shape = (sequence_length, num_features)\n",
    "        fold_metrics = []\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "            metrics_logger = MetricsLogger(results_path, fold_number=fold+1, base_name=base_name)\n",
    "            console.print(f\"[bold green]Training fold {fold + 1}/{n_splits} for {base_name}[/]\")\n",
    "            X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "            y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
    "\n",
    "\n",
    "            \n",
    "            # load the model\n",
    "            \n",
    "            model = tf.keras.models.load_model('/home/ubuntu/dds_paper/DDS_Paper/model_weights/ltn_tf_model_PGB_20_0_fold_1.tf')\n",
    "\n",
    "            # Plotting\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            for i, color in zip(range(num_classes), colors):\n",
    "                # Predict probabilities for each class\n",
    "                probs = model.predict(X_val_fold)\n",
    "                # Compute ROC curve and area the curve\n",
    "                fpr, tpr, thresholds = roc_curve(y_val_bin[:, i], probs[:, i])\n",
    "                roc_auc = auc(fpr, tpr)\n",
    "                plt.plot(fpr, tpr, color=color, lw=1.5,\n",
    "                        label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc))\n",
    "\n",
    "            plt.plot([0, 1], [0, 1], 'k--', lw=2.5)\n",
    "            plt.xlim([-0.05, 1.0])\n",
    "            plt.ylim([0.0, 1.05])\n",
    "            plt.xlabel('False Positive Rate', fontsize=16)\n",
    "            plt.ylabel('True Positive Rate', fontsize=16)\n",
    "            plt.title('Receiver Operating Characteristic for Each Class', fontsize=16)\n",
    "            plt.legend(loc=\"lower right\", fontsize=14)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting joblib\n",
      "  Obtaining dependency information for joblib from https://files.pythonhosted.org/packages/ae/e2/4dea6313ef2b38442fccbbaf4017e50a6c3c8a50e8ee9b512783e5c90409/joblib-1.4.0-py3-none-any.whl.metadata\n",
      "  Downloading joblib-1.4.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Downloading joblib-1.4.0-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.2/301.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: joblib\n",
      "Successfully installed joblib-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install joblib"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
