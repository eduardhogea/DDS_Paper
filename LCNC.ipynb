{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import tensorflow as tf\n",
    "from joblib import dump, load\n",
    "import ltn\n",
    "import csv\n",
    "import math\n",
    "\n",
    "dataset_path = 'C:\\\\work_ssd\\\\DDS_Paper\\\\data\\\\DDS_Data_SEU'\n",
    "\n",
    "PGB_path = 'C:\\\\work_ssd\\\\DDS_Paper\\\\data\\\\DDS_Data_SEU\\\\PGB'\n",
    "RGB_path = 'C:\\\\work_ssd\\\\DDS_Paper\\\\data\\\\DDS_Data_SEU\\\\RGB'\n",
    "\n",
    "# Specify the CSV file path\n",
    "csv_file = 'C:\\\\work_ssd\\\\DDS_Paper\\\\data\\\\data_robust.csv'\n",
    "preprocessor_file = 'preprocessor.joblib'\n",
    "\n",
    "\n",
    "np.random.seed(45)\n",
    "\n",
    "# Set the chunk size for reading the CSV\n",
    "chunk_size = 100000  # Adjust the chunk size according to your memory limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fault(file_name):\n",
    "    match = re.search(r'\\d+', file_name)\n",
    "    if match:\n",
    "        return int(match.group(0)[0])  # Extract the first digit\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def process_files_to_csv(data_folders, output_file):\n",
    "    # Check if the file already exists\n",
    "    if os.path.isfile(output_file):\n",
    "        print(f\"File {output_file} already exists. Skipping processing.\")\n",
    "        return\n",
    "\n",
    "    total_files = sum([len(files) for data_folder in data_folders for r, d, files in os.walk(data_folder)])\n",
    "\n",
    "    with open(output_file, 'w', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow(['Channel1', 'Channel2', 'Channel3', 'Channel4', 'Channel5', 'Channel6', 'Channel7', 'Channel8', 'Speed', 'Type', 'Fault'])\n",
    "\n",
    "        with tqdm(total=total_files, desc=\"Processing files\", unit=\"file\") as pbar:\n",
    "            for data_folder in data_folders:\n",
    "                for root, dirs, files in os.walk(data_folder):\n",
    "                    if '.ipynb_checkpoints' in root:\n",
    "                        continue  # Skip .ipynb_checkpoints folders\n",
    "                    for file in files:\n",
    "                        if file.endswith('.txt'):\n",
    "                            file_path = os.path.join(root, file)\n",
    "                            path_parts = file_path.split('\\\\')\n",
    "                            variable_speed = 'Variable_speed' in file_path\n",
    "                            type_index = -4 if variable_speed else -3\n",
    "                            type = path_parts[type_index] if path_parts[type_index] in ['PGB', 'RGB'] else None\n",
    "                            if type is not None:\n",
    "                                speed_index = -3 if variable_speed else -2\n",
    "                                speed = path_parts[speed_index]\n",
    "                                fault = extract_fault(file)\n",
    "\n",
    "                                data = pd.read_csv(file_path, sep='\\t', encoding='ISO-8859-1')\n",
    "                                reshaped_data = data.values[:, :]\n",
    "\n",
    "                                for row_data in tqdm(reshaped_data, desc=\"Processing rows\", unit=\"row\", leave=False):\n",
    "                                    row = row_data.tolist() + [speed, type, fault]\n",
    "                                    csv_writer.writerow(row)\n",
    "                            pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File C:\\work_ssd\\DDS_Paper\\data\\data_robust.csv already exists. Skipping processing.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "process_files_to_csv([PGB_path, RGB_path], csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_sample_data(file_path, output_train, output_val, train_ratio=0.8, sample_fraction=0.01):\n",
    "    chunksize = 100000\n",
    "    total_lines = sum([80740352, 80740352, 80740352, 80740352, 80740352, 80740352, 80740352, 80740352, 80740352])\n",
    "    total_chunks = math.ceil(total_lines / chunksize)\n",
    "\n",
    "    reader = pd.read_csv(file_path, chunksize=chunksize)\n",
    "\n",
    "    with open(output_train, 'w', newline='') as train_file, open(output_val, 'w', newline='') as val_file:\n",
    "        train_writer = csv.writer(train_file)\n",
    "        val_writer = csv.writer(val_file)\n",
    "        \n",
    "        for i, chunk in tqdm(enumerate(reader), total=total_chunks, desc=\"Processing chunks\", unit=\"chunk\"):\n",
    "            chunk_sample = chunk.sample(frac=sample_fraction, random_state=1)\n",
    "            if i == 0:\n",
    "                train_writer.writerow(chunk_sample.columns.values)\n",
    "                val_writer.writerow(chunk_sample.columns.values)\n",
    "\n",
    "            train_data = chunk_sample.iloc[:int(train_ratio*len(chunk_sample))].values\n",
    "            val_data = chunk_sample.iloc[int(train_ratio*len(chunk_sample)):].values\n",
    "\n",
    "            train_writer.writerows(train_data)\n",
    "            val_writer.writerows(val_data)\n",
    "\n",
    "# example usage\n",
    "split_and_sample_data(csv_file, 'train.csv', 'val.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize a dictionary to store the fault counts\n",
    "fault_counts = {}\n",
    "\n",
    "# Iterate through the CSV file using chunksize\n",
    "with tqdm(total=1, unit='chunk', desc='Processing CSV') as pbar:\n",
    "    for chunk in pd.read_csv('train.csv', chunksize=chunk_size):\n",
    "        \n",
    "        #print(chunk)\n",
    "        # Assuming there is a column named 'fault' in the CSV representing the fault type\n",
    "        fault_column = 'Fault'\n",
    "\n",
    "        # Count the occurrences of each fault in the current chunk\n",
    "        fault_chunk_counts = chunk[fault_column].value_counts()\n",
    "\n",
    "        # Aggregate the counts with the overall fault_counts dictionary\n",
    "        for fault, count in fault_chunk_counts.items():\n",
    "            fault_counts[fault] = fault_counts.get(fault, 0) + count\n",
    "\n",
    "        pbar.update()\n",
    "\n",
    "# Print the fault counts\n",
    "for fault, count in fault_counts.items():\n",
    "    print(f\"Fault: {fault}, Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries to store the counts\n",
    "speed_counts = {}\n",
    "type_counts = {}\n",
    "\n",
    "# Iterate through the CSV file using chunksize\n",
    "with tqdm(total=1, unit='chunk', desc='Processing CSV') as pbar:\n",
    "    for chunk in pd.read_csv('train.csv', chunksize=chunk_size):\n",
    "        # Assuming there is a column named 'Speed' in the CSV representing the speed values\n",
    "        \n",
    "        speed_column = 'Speed'\n",
    "\n",
    "        # Count the occurrences of each speed value in the current chunk\n",
    "        speed_chunk_counts = chunk[speed_column].value_counts()\n",
    "\n",
    "        # Aggregate the counts with the overall speed_counts dictionary\n",
    "        for speed, count in speed_chunk_counts.items():\n",
    "            speed_counts[speed] = speed_counts.get(speed, 0) + count\n",
    "\n",
    "        # Assuming there is a column named 'Type' in the CSV representing the types\n",
    "        type_column = 'Type'\n",
    "\n",
    "        # Count the occurrences of each type in the current chunk\n",
    "        type_chunk_counts = chunk[type_column].value_counts()\n",
    "\n",
    "        # Aggregate the counts with the overall type_counts dictionary\n",
    "        for typ, count in type_chunk_counts.items():\n",
    "            type_counts[typ] = type_counts.get(typ, 0) + count\n",
    "\n",
    "        pbar.update()\n",
    "\n",
    "# Print the speed counts\n",
    "print(\"Speed Counts:\")\n",
    "for speed, count in speed_counts.items():\n",
    "    print(f\"Speed: {speed}, Count: {count}\")\n",
    "\n",
    "# Print the type counts\n",
    "print(\"Type Counts:\")\n",
    "for typ, count in type_counts.items():\n",
    "    print(f\"Type: {typ}, Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(batch_size, data = csv_file):\n",
    "    chunksize = batch_size\n",
    "    for chunk in pd.read_csv(data, chunksize=chunksize):\n",
    "        # One-hot encode the categorical features\n",
    "        categorical_features = chunk[categorical_features_columns]\n",
    "        categorical_features = one_hot_encoder.transform(categorical_features).toarray()\n",
    "        \n",
    "        # Concatenate with numerical features\n",
    "        numerical_features = chunk[numerical_features_columns]\n",
    "        X = np.concatenate([numerical_features, categorical_features], axis=1)\n",
    "\n",
    "        sample_size = X.shape[0]\n",
    "        #print(sample_size)\n",
    "        time_steps = X.shape[1]\n",
    "        #print(time_steps)\n",
    "        input_dimensions = 1\n",
    "\n",
    "        X_reshaped = X.reshape(sample_size,time_steps,input_dimensions)\n",
    "\n",
    "        # Extract the labels\n",
    "        y = chunk['Fault'].values\n",
    "\n",
    "        yield X_reshaped, y\n",
    "\n",
    "def data_generator_all(batch_size, data = csv_file):\n",
    "    # Read all the data\n",
    "    df = pd.read_csv(data)\n",
    "    \n",
    "    # Shuffle the DataFrame rows \n",
    "    df = df.sample(frac=1)\n",
    "\n",
    "    # Calculate the number of batches\n",
    "    num_batches = len(df) // batch_size\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        batch = df.iloc[i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "        # One-hot encode the categorical features\n",
    "        categorical_features = batch[categorical_features_columns]\n",
    "        categorical_features = one_hot_encoder.transform(categorical_features).toarray()\n",
    "\n",
    "        # Concatenate with numerical features\n",
    "        numerical_features = batch[numerical_features_columns].values\n",
    "        X = np.concatenate([numerical_features, categorical_features], axis=1)\n",
    "\n",
    "        sample_size = X.shape[0]\n",
    "        time_steps = X.shape[1]\n",
    "        input_dimensions = 1\n",
    "\n",
    "        X_reshaped = X.reshape(sample_size, time_steps, input_dimensions)\n",
    "\n",
    "        # Extract the labels\n",
    "        y = batch['Fault'].values\n",
    "\n",
    "        yield X_reshaped, y\n",
    "\n",
    "\n",
    "# Define the categories\n",
    "# Define the categories\n",
    "speed_categories = ['20_0','30_0','30_1','30_2', '30_3','30_4','30_5','40_0','50_0', 'Variable_speed']\n",
    "type_categories = ['PGB', 'RGB']\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "one_hot_encoder = OneHotEncoder(categories=[speed_categories, type_categories])\n",
    "\n",
    "# Create a dummy dataset\n",
    "dummy_df = pd.DataFrame(data=[['20_0', 'PGB']], columns=['Speed', 'Type'])\n",
    "\n",
    "# Fit the encoder\n",
    "one_hot_encoder.fit(dummy_df)\n",
    "\n",
    "# Define your feature column names\n",
    "categorical_features_columns = ['Speed', 'Type']\n",
    "numerical_features_columns = ['Channel1', 'Channel2', 'Channel3', 'Channel4', 'Channel5', 'Channel6', 'Channel7', 'Channel8']\n",
    "\n",
    "# batch_size = 2  # Adjust as necessary\n",
    "# dataset = tf.data.Dataset.from_generator(data_generator, args=[batch_size], output_signature=(\n",
    "#     tf.TensorSpec(shape=(batch_size, 1, 20), dtype=tf.float32),  # Update this to match the shape of X\n",
    "#     tf.TensorSpec(shape=(batch_size,), dtype=tf.int32)\n",
    "# ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 20, 1)\n",
      "[[[-1.44579e-01]\n",
      "  [ 5.44400e-03]\n",
      "  [ 8.12200e-03]\n",
      "  ...\n",
      "  [ 0.00000e+00]\n",
      "  [ 0.00000e+00]\n",
      "  [ 1.00000e+00]]\n",
      "\n",
      " [[-1.54454e-01]\n",
      "  [-8.46000e-04]\n",
      "  [ 8.25200e-03]\n",
      "  ...\n",
      "  [ 0.00000e+00]\n",
      "  [ 0.00000e+00]\n",
      "  [ 1.00000e+00]]\n",
      "\n",
      " [[-1.26055e-01]\n",
      "  [ 2.50720e-02]\n",
      "  [-5.09020e-02]\n",
      "  ...\n",
      "  [ 0.00000e+00]\n",
      "  [ 1.00000e+00]\n",
      "  [ 0.00000e+00]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.56269e-01]\n",
      "  [ 1.63200e-03]\n",
      "  [-4.66000e-04]\n",
      "  ...\n",
      "  [ 0.00000e+00]\n",
      "  [ 1.00000e+00]\n",
      "  [ 0.00000e+00]]\n",
      "\n",
      " [[-1.38204e-01]\n",
      "  [-2.42000e-04]\n",
      "  [-6.48300e-03]\n",
      "  ...\n",
      "  [ 0.00000e+00]\n",
      "  [ 0.00000e+00]\n",
      "  [ 1.00000e+00]]\n",
      "\n",
      " [[-1.30302e-01]\n",
      "  [-2.14300e-03]\n",
      "  [ 7.10200e-03]\n",
      "  ...\n",
      "  [ 0.00000e+00]\n",
      "  [ 0.00000e+00]\n",
      "  [ 1.00000e+00]]]\n",
      "[8 1 7 4 7 4 2 3 6 2 8 3 1 6 3 2 7 7 7 8 2 5 2 4 8 7 4 2 2 1 0 1 8 1 8 4 5\n",
      " 4 8 5 3 8 4 6 4 0 7 0 4 3 0 7 1 8 7 1 0 8 2 7 5 8 6 8 1 5 4 2 1 3 2 1 6 8\n",
      " 2 6 6 3 7 1 7 3 0 8 0 7 6 1 1 4 8 3 0 3 8 7 3 5 2 7 7 5 1 7 3 1 1 8 5 8 5\n",
      " 0 7 0 0 5 8 2 6 5 1 8 4 4 7 2 4 0 2 5 3 8 6 0 7 2 4 2 1 4 6 6 8 7 3 6 3 3\n",
      " 1 3 8 5 7 0 7 4 2 3 0 1 7 8 1 6 2 3 2 8 1 7 0 1 6 4 2 3 2 3 0 1 5 6 4 8 1\n",
      " 7 6 5 1 7 4 1 0 5 8 2 4 5 2 1 8 2 0 4 1 3 0 2 8 4 6 1 8 3 1 4 1 3 1 7 7 4\n",
      " 4 8 7 5 5 4 3 3 6 7 0 5 0 4 0 3 2 1 1 5 3 7 1 3 1 0 4 3 6 5 2 1 2 1]\n"
     ]
    }
   ],
   "source": [
    "generator = data_generator_all(batch_size=256, data = 'train.csv')\n",
    "last_X, last_y = None, None\n",
    "\n",
    "for sample_X, sample_y in generator:\n",
    "    last_X, last_y = sample_X, sample_y\n",
    "    print(last_X.shape)\n",
    "    break\n",
    "\n",
    "print(last_X)\n",
    "print(last_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN as backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "def build_cnn_backbone():\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Input(shape=(last_X.shape[1],last_X.shape[2])))\n",
    "    model.add(keras.layers.Conv1D(filters=64, kernel_size=4, activation='elu', name=\"Conv1D_1\"))\n",
    "    model.add(keras.layers.Dropout(0.5))\n",
    "    model.add(keras.layers.Conv1D(filters=32, kernel_size=3, activation='elu', name=\"Conv1D_2\"))\n",
    "    model.add(keras.layers.Conv1D(filters=16, kernel_size=2, activation='elu', name=\"Conv1D_3\"))\n",
    "    model.add(keras.layers.MaxPooling1D(pool_size=2, name=\"MaxPooling1D\"))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    #model.add(keras.layers.Dense(8, activation='relu', name=\"Dense_1\"))\n",
    "    model.add(keras.layers.Dense(9, name=\"Dense_2\"))\n",
    "    return model\n",
    "\n",
    "# Build the CNN backbone model\n",
    "model = build_cnn_backbone()\n",
    "\n",
    "# Wrap the model in an LTN predicate\n",
    "p = ltn.Predicate.FromLogits(model, activation_function=\"softmax\", with_class_indexing=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants to index/iterate on the classes\n",
    "HEALTHY = ltn.Constant(0, trainable=False)\n",
    "CTF = ltn.Constant(1, trainable=False)\n",
    "MTF = ltn.Constant(2, trainable=False)\n",
    "RCF = ltn.Constant(3, trainable=False)\n",
    "SWF = ltn.Constant(4, trainable=False)\n",
    "BWF = ltn.Constant(5, trainable=False)\n",
    "CWF = ltn.Constant(6, trainable=False)\n",
    "IRF = ltn.Constant(7, trainable=False)\n",
    "ORF = ltn.Constant(8, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#operators\n",
    "Not = ltn.Wrapper_Connective(ltn.fuzzy_ops.Not_Std())\n",
    "And = ltn.Wrapper_Connective(ltn.fuzzy_ops.And_Prod())\n",
    "Or = ltn.Wrapper_Connective(ltn.fuzzy_ops.Or_ProbSum())\n",
    "Implies = ltn.Wrapper_Connective(ltn.fuzzy_ops.Implies_Reichenbach())\n",
    "Forall = ltn.Wrapper_Quantifier(ltn.fuzzy_ops.Aggreg_pMeanError(p=2),semantics=\"forall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the formula aggregator\n",
    "formula_aggregator = ltn.Wrapper_Formula_Aggregator(ltn.fuzzy_ops.Aggreg_pMeanError(p=2))\n",
    "\n",
    "@tf.function\n",
    "def axioms(features, labels, training=False):\n",
    "    # Variables for each class\n",
    "    x_healthy = ltn.Variable(\"x_healthy\", features[labels == 0])\n",
    "    x_ctf = ltn.Variable(\"x_ctf\", features[labels == 1])\n",
    "    x_mtf = ltn.Variable(\"x_mtf\", features[labels == 2])\n",
    "    x_rcf = ltn.Variable(\"x_rcf\", features[labels == 3])\n",
    "    x_swf = ltn.Variable(\"x_swf\", features[labels == 4])\n",
    "    x_bwf = ltn.Variable(\"x_bwf\", features[labels == 5])\n",
    "    x_cwf = ltn.Variable(\"x_cwf\", features[labels == 6])\n",
    "    x_irf = ltn.Variable(\"x_irf\", features[labels == 7])\n",
    "    x_orf = ltn.Variable(\"x_orf\", features[labels == 8])\n",
    "\n",
    "    # Fault list for mutual exclusivity axioms\n",
    "    faults = [HEALTHY, CTF, MTF, RCF, SWF, BWF, CWF, IRF, ORF]\n",
    "    fault_vars = [x_healthy, x_ctf, x_mtf, x_rcf, x_swf, x_bwf, x_cwf, x_irf, x_orf]\n",
    "\n",
    "    axioms = []\n",
    "    for i, fault in enumerate(faults):\n",
    "        # Add the axiom that for all x of a certain fault, the probability of that fault should be high\n",
    "        axioms.append(Forall(fault_vars[i], p([fault_vars[i], fault], training=training)))\n",
    "\n",
    "        # Add the axioms for mutual exclusivity\n",
    "        for j, other_fault in enumerate(faults):\n",
    "            if i != j:\n",
    "                axioms.append(Forall(fault_vars[i], Not(p([fault_vars[i], other_fault], training=training))))\n",
    "\n",
    "    sat_level = formula_aggregator(axioms).tensor\n",
    "    return sat_level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial sat level 0.68522\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print the initial satisfaction level for each batch of the test dataset\n",
    "for sample_X, sample_y in generator:\n",
    "    #sample_X_reshaped = tf.reshape(sample_X, (sample_X.shape[0], sample_X.shape[1], 1))\n",
    "    #print(sample_X)\n",
    "    print(\"Initial sat level %.5f\" % axioms(sample_X, sample_y))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict = {\n",
    "    'train_sat_kb': tf.keras.metrics.Mean(name='train_sat_kb'),\n",
    "    'test_sat_kb': tf.keras.metrics.Mean(name='test_sat_kb'),\n",
    "    'train_accuracy': tf.keras.metrics.CategoricalAccuracy(name=\"train_accuracy\"),\n",
    "    'test_accuracy': tf.keras.metrics.CategoricalAccuracy(name=\"test_accuracy\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def train_step(features, labels):\n",
    "    # sat and update\n",
    "    with tf.GradientTape() as tape:\n",
    "        sat = axioms(features, labels, training=True)\n",
    "        loss = 1.-sat\n",
    "    gradients = tape.gradient(loss, p.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, p.trainable_variables))\n",
    "    sat = axioms(features, labels) # compute sat without dropout\n",
    "    metrics_dict['train_sat_kb'](sat)\n",
    "    # accuracy\n",
    "    predictions = model([features])\n",
    "    metrics_dict['train_accuracy'](labels, predictions)\n",
    "\n",
    "@tf.function\n",
    "def test_step(features, labels):\n",
    "    # sat\n",
    "    sat = axioms(features, labels)\n",
    "    metrics_dict['test_sat_kb'](sat)\n",
    "    # accuracy\n",
    "    predictions = model([features])\n",
    "    metrics_dict['test_accuracy'](labels, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Revised training function with tqdm progress tracking\n",
    "def train(epochs, metrics_dict, train_generator_func, test_generator_func, train_step, test_step,\n",
    "          num_train_steps, num_test_steps, track_metrics=1, csv_path=None, scheduled_parameters=defaultdict(lambda: {})):\n",
    "\n",
    "    template = \"Epoch {}\"\n",
    "    for metrics_label in metrics_dict.keys():\n",
    "        template += \", %s: {:.4f}\" % metrics_label\n",
    "\n",
    "    if csv_path is not None:\n",
    "        csv_file = open(csv_path, \"w+\")\n",
    "        headers = \",\".join([\"Epoch\"] + list(metrics_dict.keys()))\n",
    "        csv_template = \",\".join([\"{}\" for _ in range(len(metrics_dict) + 1)])\n",
    "        csv_file.write(headers + \"\\n\")\n",
    "\n",
    "    epoch_times = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Starting epoch {epoch}\")\n",
    "        for metrics in metrics_dict.values():\n",
    "            metrics.reset_states()\n",
    "\n",
    "        # Reset the generators for each epoch\n",
    "        train_generator = train_generator_func()\n",
    "\n",
    "        print(f\"Starting training for epoch {epoch}\")\n",
    "        pbar = tqdm(total=num_train_steps)\n",
    "        for batch_elements in train_generator:\n",
    "            train_step(*batch_elements, **scheduled_parameters[epoch])\n",
    "            pbar.set_description(f\"Training sat: {metrics_dict['train_sat_kb'].result():.4f}\")\n",
    "            pbar.update()\n",
    "        pbar.close()\n",
    "\n",
    "        test_generator = test_generator_func()\n",
    "\n",
    "        print(f\"Starting testing for epoch {epoch}\")\n",
    "        pbar = tqdm(total=num_test_steps)\n",
    "        for batch_elements in test_generator:\n",
    "            test_step(*batch_elements, **scheduled_parameters[epoch])\n",
    "            pbar.set_description(f\"Testing sat: {metrics_dict['test_sat_kb'].result():.4f}\")\n",
    "            pbar.update()\n",
    "        pbar.close()\n",
    "\n",
    "        metrics_results = [metrics.result() for metrics in metrics_dict.values()]\n",
    "        if epoch % track_metrics == 0:\n",
    "            print(template.format(epoch, *metrics_results))\n",
    "\n",
    "        if csv_path is not None:\n",
    "            csv_file.write(csv_template.format(epoch, *metrics_results) + \"\\n\")\n",
    "            csv_file.flush()\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "        start_time = end_time\n",
    "        epoch_times.append(epoch_time)\n",
    "\n",
    "        print(f\"Epoch {epoch} completed. Time: {epoch_time}\")\n",
    "\n",
    "    if csv_path is not None:\n",
    "        csv_file.close()\n",
    "\n",
    "    return epoch_times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "# Get the number of training and testing steps per epoch\n",
    "num_train_steps = sum(1 for _ in data_generator(batch_size, data='train.csv'))\n",
    "num_test_steps = sum(1 for _ in data_generator(batch_size, data='val.csv'))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Define your generator functions\n",
    "def train_generator_func():\n",
    "    return data_generator_all(batch_size=batch_size, data='train.csv')\n",
    "\n",
    "def val_generator_func():\n",
    "    return data_generator_all(batch_size=batch_size, data='val.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Starting training for epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training sat: 0.6955: 100%|█████████▉| 22708/22709 [40:25<00:00,  9.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting testing for epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing sat: 0.6977: 100%|█████████▉| 5677/5678 [02:10<00:00, 43.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train_sat_kb: 0.6955, test_sat_kb: 0.6977, train_accuracy: 0.0696, test_accuracy: 0.0708\n",
      "Epoch 0 completed. Time: 2555.9184296131134\n",
      "Starting epoch 1\n",
      "Starting training for epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training sat: 0.6986: 100%|█████████▉| 22708/22709 [39:10<00:00,  9.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting testing for epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing sat: 0.6987: 100%|█████████▉| 5677/5678 [01:59<00:00, 47.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, train_sat_kb: 0.6986, test_sat_kb: 0.6987, train_accuracy: 0.0706, test_accuracy: 0.0723\n",
      "Epoch 1 completed. Time: 2470.4121112823486\n",
      "Starting epoch 2\n",
      "Starting training for epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training sat: 0.6991: 100%|█████████▉| 22708/22709 [39:06<00:00,  9.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting testing for epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing sat: 0.6991: 100%|█████████▉| 5677/5678 [01:59<00:00, 47.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, train_sat_kb: 0.6991, test_sat_kb: 0.6991, train_accuracy: 0.0708, test_accuracy: 0.0710\n",
      "Epoch 2 completed. Time: 2465.9900348186493\n",
      "Starting epoch 3\n",
      "Starting training for epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training sat: 0.6993:  42%|████▏     | 9631/22709 [16:14<22:04,  9.87it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m training \u001b[39m=\u001b[39m train(\n\u001b[0;32m      2\u001b[0m     EPOCHS,\n\u001b[0;32m      3\u001b[0m     metrics_dict,\n\u001b[0;32m      4\u001b[0m     train_generator_func,\n\u001b[0;32m      5\u001b[0m     val_generator_func,\n\u001b[0;32m      6\u001b[0m     train_step,\n\u001b[0;32m      7\u001b[0m     test_step,\n\u001b[0;32m      8\u001b[0m     csv_path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mfinal.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      9\u001b[0m     track_metrics\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m     10\u001b[0m     num_train_steps\u001b[39m=\u001b[39;49mnum_train_steps,\n\u001b[0;32m     11\u001b[0m     num_test_steps\u001b[39m=\u001b[39;49mnum_test_steps\n\u001b[0;32m     12\u001b[0m )\n",
      "Cell \u001b[1;32mIn[14], line 33\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(epochs, metrics_dict, train_generator_func, test_generator_func, train_step, test_step, num_train_steps, num_test_steps, track_metrics, csv_path, scheduled_parameters)\u001b[0m\n\u001b[0;32m     31\u001b[0m pbar \u001b[39m=\u001b[39m tqdm(total\u001b[39m=\u001b[39mnum_train_steps)\n\u001b[0;32m     32\u001b[0m \u001b[39mfor\u001b[39;00m batch_elements \u001b[39min\u001b[39;00m train_generator:\n\u001b[1;32m---> 33\u001b[0m     train_step(\u001b[39m*\u001b[39mbatch_elements, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mscheduled_parameters[epoch])\n\u001b[0;32m     34\u001b[0m     pbar\u001b[39m.\u001b[39mset_description(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining sat: \u001b[39m\u001b[39m{\u001b[39;00mmetrics_dict[\u001b[39m'\u001b[39m\u001b[39mtrain_sat_kb\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mresult()\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     35\u001b[0m     pbar\u001b[39m.\u001b[39mupdate()\n",
      "File \u001b[1;32mc:\\Users\\edy45\\miniconda3\\envs\\work_stable\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\edy45\\miniconda3\\envs\\work_stable\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\edy45\\miniconda3\\envs\\work_stable\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    923\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    924\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    925\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    929\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\edy45\\miniconda3\\envs\\work_stable\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\edy45\\miniconda3\\envs\\work_stable\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\edy45\\miniconda3\\envs\\work_stable\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\edy45\\miniconda3\\envs\\work_stable\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "training = train(\n",
    "    EPOCHS,\n",
    "    metrics_dict,\n",
    "    train_generator_func,\n",
    "    val_generator_func,\n",
    "    train_step,\n",
    "    test_step,\n",
    "    csv_path=\"final.csv\",\n",
    "    track_metrics=1,\n",
    "    num_train_steps=num_train_steps,\n",
    "    num_test_steps=num_test_steps\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work_stable",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
