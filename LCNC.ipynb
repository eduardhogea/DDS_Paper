{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-11 16:31:37.217104: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-11 16:31:37.300777: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-11 16:31:37.302096: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-11 16:31:38.502226: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33meduard-hogea00\u001b[0m (\u001b[33mdds-paper\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/dds_paper/DDS_Paper/wandb/run-20230911_163141-49hethvp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dds-paper/LCNC/runs/49hethvp' target=\"_blank\">CNN_backbone</a></strong> to <a href='https://wandb.ai/dds-paper/LCNC' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dds-paper/LCNC' target=\"_blank\">https://wandb.ai/dds-paper/LCNC</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dds-paper/LCNC/runs/49hethvp' target=\"_blank\">https://wandb.ai/dds-paper/LCNC/runs/49hethvp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import tensorflow as tf\n",
    "from joblib import dump, load\n",
    "import ltn\n",
    "import csv\n",
    "import math\n",
    "import wandb\n",
    "\n",
    "WANDB_START_METHOD=\"thread\"\n",
    "wandb.init(project=\"LCNC\", name=\"CNN_backbone\")\n",
    "\n",
    "\n",
    "dataset_path = '/home/ubuntu/dds_paper/DDS_Paper/data/DDS_Data_SEU'\n",
    "\n",
    "PGB_path = '/home/ubuntu/dds_paper/DDS_Paper/data/DDS_Data_SEU/PGB'\n",
    "RGB_path = '/home/ubuntu/dds_paper/DDS_Paper/data/DDS_Data_SEU/RGB'\n",
    "\n",
    "# Specify the CSV file path\n",
    "csv_file = '/home/ubuntu/dds_paper/DDS_Paper/data/data_robust.csv'\n",
    "preprocessor_file = 'preprocessor.joblib'\n",
    "\n",
    "train_path = '/home/ubuntu/dds_paper/DDS_Paper/data/train.csv'\n",
    "val_path = '/home/ubuntu/dds_paper/DDS_Paper/data/val.csv'\n",
    "\n",
    "np.random.seed(45)\n",
    "\n",
    "# Set the chunk size for reading the CSV\n",
    "chunk_size = 100000  # Adjust the chunk size according to your memory limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fault(file_name):\n",
    "    match = re.search(r'\\d+', file_name)\n",
    "    if match:\n",
    "        return int(match.group(0)[0])  # Extract the first digit\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def process_files_to_csv(data_folders, output_file):\n",
    "    # Check if the file already exists\n",
    "    if os.path.isfile(output_file):\n",
    "        print(f\"File {output_file} already exists. Skipping processing.\")\n",
    "        return\n",
    "\n",
    "    total_files = sum([len(files) for data_folder in data_folders for r, d, files in os.walk(data_folder)])\n",
    "\n",
    "    with open(output_file, 'w', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow(['Channel1', 'Channel2', 'Channel3', 'Channel4', 'Channel5', 'Channel6', 'Channel7', 'Channel8', 'Speed', 'Type', 'Fault'])\n",
    "\n",
    "        with tqdm(total=total_files, desc=\"Processing files\", unit=\"file\") as pbar:\n",
    "            for data_folder in data_folders:\n",
    "                for root, dirs, files in os.walk(data_folder):\n",
    "                    if '.ipynb_checkpoints' in root:\n",
    "                        continue  # Skip .ipynb_checkpoints folders\n",
    "                    for file in files:\n",
    "                        if file.endswith('.txt'):\n",
    "                            file_path = os.path.join(root, file)\n",
    "                            path_parts = file_path.split('\\\\')\n",
    "                            variable_speed = 'Variable_speed' in file_path\n",
    "                            type_index = -4 if variable_speed else -3\n",
    "                            type = path_parts[type_index] if path_parts[type_index] in ['PGB', 'RGB'] else None\n",
    "                            if type is not None:\n",
    "                                speed_index = -3 if variable_speed else -2\n",
    "                                speed = path_parts[speed_index]\n",
    "                                fault = extract_fault(file)\n",
    "\n",
    "                                data = pd.read_csv(file_path, sep='\\t', encoding='ISO-8859-1')\n",
    "                                reshaped_data = data.values[:, :]\n",
    "\n",
    "                                for row_data in tqdm(reshaped_data, desc=\"Processing rows\", unit=\"row\", leave=False):\n",
    "                                    row = row_data.tolist() + [speed, type, fault]\n",
    "                                    csv_writer.writerow(row)\n",
    "                            pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /home/ubuntu/dds_paper/DDS_Paper/data/data_robust.csv already exists. Skipping processing.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "process_files_to_csv([PGB_path, RGB_path], csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files /home/ubuntu/dds_paper/DDS_Paper/data/train.csv and /home/ubuntu/dds_paper/DDS_Paper/data/val.csv already exist. Skipping processing.\n"
     ]
    }
   ],
   "source": [
    "def split_and_sample_data(file_path, output_train, output_val, train_ratio=0.8, sample_fraction=0.01):\n",
    "    # Check if the output files already exist\n",
    "    if os.path.isfile(output_train) and os.path.isfile(output_val):\n",
    "        print(f\"Files {output_train} and {output_val} already exist. Skipping processing.\")\n",
    "        return\n",
    "    \n",
    "    chunksize = 100000\n",
    "    total_lines = sum([80740352, 80740352, 80740352, 80740352, 80740352, 80740352, 80740352, 80740352, 80740352])\n",
    "    total_chunks = math.ceil(total_lines / chunksize)\n",
    "\n",
    "    reader = pd.read_csv(file_path, chunksize=chunksize)\n",
    "\n",
    "    with open(output_train, 'w', newline='') as train_file, open(output_val, 'w', newline='') as val_file:\n",
    "        train_writer = csv.writer(train_file)\n",
    "        val_writer = csv.writer(val_file)\n",
    "        \n",
    "        for i, chunk in tqdm(enumerate(reader), total=total_chunks, desc=\"Processing chunks\", unit=\"chunk\"):\n",
    "            chunk_sample = chunk.sample(frac=sample_fraction, random_state=1)\n",
    "            if i == 0:\n",
    "                train_writer.writerow(chunk_sample.columns.values)\n",
    "                val_writer.writerow(chunk_sample.columns.values)\n",
    "\n",
    "            train_data = chunk_sample.iloc[:int(train_ratio*len(chunk_sample))].values\n",
    "            val_data = chunk_sample.iloc[int(train_ratio*len(chunk_sample)):].values\n",
    "\n",
    "            train_writer.writerows(train_data)\n",
    "            val_writer.writerows(val_data)\n",
    "\n",
    "split_and_sample_data(csv_file, train_path, val_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV: 59chunk [00:06,  9.00chunk/s]                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fault: 0, Count: 645879\n",
      "Fault: 1, Count: 645871\n",
      "Fault: 2, Count: 645949\n",
      "Fault: 3, Count: 645932\n",
      "Fault: 4, Count: 645904\n",
      "Fault: 5, Count: 645920\n",
      "Fault: 7, Count: 645943\n",
      "Fault: 6, Count: 646001\n",
      "Fault: 8, Count: 645906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize a dictionary to store the fault counts\n",
    "fault_counts = {}\n",
    "\n",
    "# Iterate through the CSV file using chunksize\n",
    "with tqdm(total=1, unit='chunk', desc='Processing CSV') as pbar:\n",
    "    for chunk in pd.read_csv(train_path, chunksize=chunk_size):\n",
    "        \n",
    "        #print(chunk)\n",
    "        # Assuming there is a column named 'fault' in the CSV representing the fault type\n",
    "        fault_column = 'Fault'\n",
    "\n",
    "        # Count the occurrences of each fault in the current chunk\n",
    "        fault_chunk_counts = chunk[fault_column].value_counts()\n",
    "\n",
    "        # Aggregate the counts with the overall fault_counts dictionary\n",
    "        for fault, count in fault_chunk_counts.items():\n",
    "            fault_counts[fault] = fault_counts.get(fault, 0) + count\n",
    "\n",
    "        pbar.update()\n",
    "\n",
    "# Print the fault counts\n",
    "for fault, count in fault_counts.items():\n",
    "    print(f\"Fault: {fault}, Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV: 59chunk [00:06,  8.71chunk/s]                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed Counts:\n",
      "Speed: 20_0, Count: 604001\n",
      "Speed: 30_0, Count: 603957\n",
      "Speed: 30_1, Count: 604001\n",
      "Speed: 30_2, Count: 603954\n",
      "Speed: 30_3, Count: 604010\n",
      "Speed: 30_4, Count: 603944\n",
      "Speed: 30_5, Count: 604010\n",
      "Speed: 40_0, Count: 603952\n",
      "Speed: 50_0, Count: 603999\n",
      "Speed: Variable_speed, Count: 377477\n",
      "Type Counts:\n",
      "Type: PGB, Count: 2906670\n",
      "Type: RGB, Count: 2906635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize dictionaries to store the counts\n",
    "speed_counts = {}\n",
    "type_counts = {}\n",
    "\n",
    "# Iterate through the CSV file using chunksize\n",
    "with tqdm(total=1, unit='chunk', desc='Processing CSV') as pbar:\n",
    "    for chunk in pd.read_csv(train_path, chunksize=chunk_size):\n",
    "        # Assuming there is a column named 'Speed' in the CSV representing the speed values\n",
    "        \n",
    "        speed_column = 'Speed'\n",
    "\n",
    "        # Count the occurrences of each speed value in the current chunk\n",
    "        speed_chunk_counts = chunk[speed_column].value_counts()\n",
    "\n",
    "        # Aggregate the counts with the overall speed_counts dictionary\n",
    "        for speed, count in speed_chunk_counts.items():\n",
    "            speed_counts[speed] = speed_counts.get(speed, 0) + count\n",
    "\n",
    "        # Assuming there is a column named 'Type' in the CSV representing the types\n",
    "        type_column = 'Type'\n",
    "\n",
    "        # Count the occurrences of each type in the current chunk\n",
    "        type_chunk_counts = chunk[type_column].value_counts()\n",
    "\n",
    "        # Aggregate the counts with the overall type_counts dictionary\n",
    "        for typ, count in type_chunk_counts.items():\n",
    "            type_counts[typ] = type_counts.get(typ, 0) + count\n",
    "\n",
    "        pbar.update()\n",
    "\n",
    "# Print the speed counts\n",
    "print(\"Speed Counts:\")\n",
    "for speed, count in speed_counts.items():\n",
    "    print(f\"Speed: {speed}, Count: {count}\")\n",
    "\n",
    "# Print the type counts\n",
    "print(\"Type Counts:\")\n",
    "for typ, count in type_counts.items():\n",
    "    print(f\"Type: {typ}, Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(batch_size, data = csv_file):\n",
    "    chunksize = batch_size\n",
    "    for chunk in pd.read_csv(data, chunksize=chunksize):\n",
    "        # One-hot encode the categorical features\n",
    "        categorical_features = chunk[categorical_features_columns]\n",
    "        categorical_features = one_hot_encoder.transform(categorical_features).toarray()\n",
    "        \n",
    "        # Concatenate with numerical features\n",
    "        numerical_features = chunk[numerical_features_columns]\n",
    "        X = np.concatenate([numerical_features, categorical_features], axis=1)\n",
    "\n",
    "        sample_size = X.shape[0]\n",
    "        #print(sample_size)\n",
    "        time_steps = X.shape[1]\n",
    "        #print(time_steps)\n",
    "        input_dimensions = 1\n",
    "\n",
    "        X_reshaped = X.reshape(sample_size,time_steps,input_dimensions)\n",
    "\n",
    "        # Extract the labels\n",
    "        y = chunk['Fault'].values\n",
    "\n",
    "        yield X_reshaped, y\n",
    "\n",
    "def data_generator_all(batch_size, data = csv_file):\n",
    "    # Read all the data\n",
    "    df = pd.read_csv(data)\n",
    "    \n",
    "    # Shuffle the DataFrame rows \n",
    "    df = df.sample(frac=1)\n",
    "\n",
    "    # Calculate the number of batches\n",
    "    num_batches = len(df) // batch_size\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        batch = df.iloc[i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "        # One-hot encode the categorical features\n",
    "        categorical_features = batch[categorical_features_columns]\n",
    "        categorical_features = one_hot_encoder.transform(categorical_features).toarray()\n",
    "\n",
    "        # Concatenate with numerical features\n",
    "        numerical_features = batch[numerical_features_columns].values\n",
    "        X = np.concatenate([numerical_features, categorical_features], axis=1)\n",
    "\n",
    "        sample_size = X.shape[0]\n",
    "        time_steps = X.shape[1]\n",
    "        input_dimensions = 1\n",
    "\n",
    "        X_reshaped = X.reshape(sample_size, time_steps, input_dimensions)\n",
    "\n",
    "        # Extract the labels\n",
    "        y = batch['Fault'].values\n",
    "\n",
    "        yield X_reshaped, y\n",
    "\n",
    "\n",
    "# Define the categories\n",
    "# Define the categories\n",
    "speed_categories = ['20_0','30_0','30_1','30_2', '30_3','30_4','30_5','40_0','50_0', 'Variable_speed']\n",
    "type_categories = ['PGB', 'RGB']\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "one_hot_encoder = OneHotEncoder(categories=[speed_categories, type_categories])\n",
    "\n",
    "# Create a dummy dataset\n",
    "dummy_df = pd.DataFrame(data=[['20_0', 'PGB']], columns=['Speed', 'Type'])\n",
    "\n",
    "# Fit the encoder\n",
    "one_hot_encoder.fit(dummy_df)\n",
    "\n",
    "# Define your feature column names\n",
    "categorical_features_columns = ['Speed', 'Type']\n",
    "numerical_features_columns = ['Channel1', 'Channel2', 'Channel3', 'Channel4', 'Channel5', 'Channel6', 'Channel7', 'Channel8']\n",
    "\n",
    "# batch_size = 2  # Adjust as necessary\n",
    "# dataset = tf.data.Dataset.from_generator(data_generator, args=[batch_size], output_signature=(\n",
    "#     tf.TensorSpec(shape=(batch_size, 1, 20), dtype=tf.float32),  # Update this to match the shape of X\n",
    "#     tf.TensorSpec(shape=(batch_size,), dtype=tf.int32)\n",
    "# ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 20, 1)\n",
      "[[[-1.63977e-01]\n",
      "  [-7.50700e-03]\n",
      "  [ 5.11600e-03]\n",
      "  ...\n",
      "  [ 0.00000e+00]\n",
      "  [ 0.00000e+00]\n",
      "  [ 1.00000e+00]]\n",
      "\n",
      " [[-2.02326e-01]\n",
      "  [-4.72200e-03]\n",
      "  [ 1.15180e-02]\n",
      "  ...\n",
      "  [ 0.00000e+00]\n",
      "  [ 0.00000e+00]\n",
      "  [ 1.00000e+00]]\n",
      "\n",
      " [[-1.13385e-01]\n",
      "  [-3.02100e-03]\n",
      "  [ 2.12600e-03]\n",
      "  ...\n",
      "  [ 0.00000e+00]\n",
      "  [ 1.00000e+00]\n",
      "  [ 0.00000e+00]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.39151e-01]\n",
      "  [-1.02660e-02]\n",
      "  [-3.91800e-03]\n",
      "  ...\n",
      "  [ 0.00000e+00]\n",
      "  [ 1.00000e+00]\n",
      "  [ 0.00000e+00]]\n",
      "\n",
      " [[-1.17069e-01]\n",
      "  [ 4.91500e-03]\n",
      "  [ 1.49870e-02]\n",
      "  ...\n",
      "  [ 0.00000e+00]\n",
      "  [ 1.00000e+00]\n",
      "  [ 0.00000e+00]]\n",
      "\n",
      " [[-1.29793e-01]\n",
      "  [ 6.57000e-04]\n",
      "  [-7.79900e-03]\n",
      "  ...\n",
      "  [ 0.00000e+00]\n",
      "  [ 0.00000e+00]\n",
      "  [ 1.00000e+00]]]\n",
      "[5 1 7 6 1 1 7 1 0 7 0 8 2 4 1 4 3 8 1 2 7 0 2 0 6 5 4 8 3 5 8 3 2 4 6 8 1\n",
      " 5 3 7 1 1 1 3 0 6 8 0 3 6 3 7 1 2 7 0 7 3 3 6 7 8 0 7 5 8 3 4 8 4 1 2 5 3\n",
      " 4 3 4 4 4 5 2 4 8 6 0 7 0 5 8 2 4 5 4 3 6 3 8 4 2 3 6 2 3 4 2 8 4 6 1 0 7\n",
      " 2 0 1 5 2 0 4 5 4 6 0 2 7 4 0 0 5 4 8 5 1 2 8 4 5 2 3 4 8 7 6 6 7 4 1 2 0\n",
      " 0 1 6 6 8 5 2 1 2 0 5 5 8 2 7 3 1 8 1 0 6 4 2 6 8 7 1 4 5 7 8 2 4 6 2 6 6\n",
      " 0 4 3 4 6 4 3 6 3 3 1 4 3 4 3 2 5 6 2 2 1 6 7 5 5 7 0 4 1 2 3 3 0 8 5 5 4\n",
      " 8 3 2 2 1 7 6 5 4 7 6 5 5 8 1 2 3 7 3 0 0 6 1 1 1 1 4 4 7 7 2 1 3 4]\n"
     ]
    }
   ],
   "source": [
    "generator = data_generator_all(batch_size=256, data = train_path)\n",
    "last_X, last_y = None, None\n",
    "\n",
    "for sample_X, sample_y in generator:\n",
    "    last_X, last_y = sample_X, sample_y\n",
    "    print(last_X.shape)\n",
    "    break\n",
    "\n",
    "print(last_X)\n",
    "print(last_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN as backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "def build_cnn_backbone():\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Input(shape=(last_X.shape[1],last_X.shape[2])))\n",
    "    model.add(keras.layers.Conv1D(filters=64, kernel_size=4, activation='elu', name=\"Conv1D_1\"))\n",
    "    model.add(keras.layers.Dropout(0.5))\n",
    "    model.add(keras.layers.Conv1D(filters=32, kernel_size=3, activation='elu', name=\"Conv1D_2\"))\n",
    "    model.add(keras.layers.Conv1D(filters=16, kernel_size=2, activation='elu', name=\"Conv1D_3\"))\n",
    "    model.add(keras.layers.MaxPooling1D(pool_size=2, name=\"MaxPooling1D\"))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    #model.add(keras.layers.Dense(8, activation='relu', name=\"Dense_1\"))\n",
    "    model.add(keras.layers.Dense(9, name=\"Dense_2\"))\n",
    "    return model\n",
    "\n",
    "# Build the CNN backbone model\n",
    "model = build_cnn_backbone()\n",
    "\n",
    "# Wrap the model in an LTN predicate\n",
    "p = ltn.Predicate.FromLogits(model, activation_function=\"softmax\", with_class_indexing=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants to index/iterate on the classes\n",
    "HEALTHY = ltn.Constant(0, trainable=False)\n",
    "CTF = ltn.Constant(1, trainable=False)\n",
    "MTF = ltn.Constant(2, trainable=False)\n",
    "RCF = ltn.Constant(3, trainable=False)\n",
    "SWF = ltn.Constant(4, trainable=False)\n",
    "BWF = ltn.Constant(5, trainable=False)\n",
    "CWF = ltn.Constant(6, trainable=False)\n",
    "IRF = ltn.Constant(7, trainable=False)\n",
    "ORF = ltn.Constant(8, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#operators\n",
    "Not = ltn.Wrapper_Connective(ltn.fuzzy_ops.Not_Std())\n",
    "And = ltn.Wrapper_Connective(ltn.fuzzy_ops.And_Prod())\n",
    "Or = ltn.Wrapper_Connective(ltn.fuzzy_ops.Or_ProbSum())\n",
    "Implies = ltn.Wrapper_Connective(ltn.fuzzy_ops.Implies_Reichenbach())\n",
    "Forall = ltn.Wrapper_Quantifier(ltn.fuzzy_ops.Aggreg_pMeanError(p=2),semantics=\"forall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the formula aggregator\n",
    "formula_aggregator = ltn.Wrapper_Formula_Aggregator(ltn.fuzzy_ops.Aggreg_pMeanError(p=2))\n",
    "\n",
    "@tf.function\n",
    "def axioms(features, labels, training=False):\n",
    "    # Variables for each class\n",
    "    x_healthy = ltn.Variable(\"x_healthy\", features[labels == 0])\n",
    "    x_ctf = ltn.Variable(\"x_ctf\", features[labels == 1])\n",
    "    x_mtf = ltn.Variable(\"x_mtf\", features[labels == 2])\n",
    "    x_rcf = ltn.Variable(\"x_rcf\", features[labels == 3])\n",
    "    x_swf = ltn.Variable(\"x_swf\", features[labels == 4])\n",
    "    x_bwf = ltn.Variable(\"x_bwf\", features[labels == 5])\n",
    "    x_cwf = ltn.Variable(\"x_cwf\", features[labels == 6])\n",
    "    x_irf = ltn.Variable(\"x_irf\", features[labels == 7])\n",
    "    x_orf = ltn.Variable(\"x_orf\", features[labels == 8])\n",
    "\n",
    "    # Fault list for mutual exclusivity axioms\n",
    "    faults = [HEALTHY, CTF, MTF, RCF, SWF, BWF, CWF, IRF, ORF]\n",
    "    fault_vars = [x_healthy, x_ctf, x_mtf, x_rcf, x_swf, x_bwf, x_cwf, x_irf, x_orf]\n",
    "\n",
    "    axioms = []\n",
    "    for i, fault in enumerate(faults):\n",
    "        # Add the axiom that for all x of a certain fault, the probability of that fault should be high\n",
    "        axioms.append(Forall(fault_vars[i], p([fault_vars[i], fault], training=training)))\n",
    "\n",
    "        # Add the axioms for mutual exclusivity\n",
    "        for j, other_fault in enumerate(faults):\n",
    "            if i != j:\n",
    "                axioms.append(Forall(fault_vars[i], Not(p([fault_vars[i], other_fault], training=training))))\n",
    "\n",
    "    sat_level = formula_aggregator(axioms).tensor\n",
    "    return sat_level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial sat level 0.68571\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print the initial satisfaction level for each batch of the test dataset\n",
    "for sample_X, sample_y in generator:\n",
    "    #sample_X_reshaped = tf.reshape(sample_X, (sample_X.shape[0], sample_X.shape[1], 1))\n",
    "    #print(sample_X)\n",
    "    print(\"Initial sat level %.5f\" % axioms(sample_X, sample_y))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict = {\n",
    "    'train_sat_kb': tf.keras.metrics.Mean(name='train_sat_kb'),\n",
    "    'test_sat_kb': tf.keras.metrics.Mean(name='test_sat_kb'),\n",
    "    'train_accuracy': tf.keras.metrics.CategoricalAccuracy(name=\"train_accuracy\"),\n",
    "    'test_accuracy': tf.keras.metrics.CategoricalAccuracy(name=\"test_accuracy\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(features, labels, optimizer):\n",
    "    # sat and update\n",
    "    with tf.GradientTape() as tape:\n",
    "        sat = axioms(features, labels, training=True)\n",
    "        loss = 1. - sat\n",
    "    gradients = tape.gradient(loss, p.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, p.trainable_variables))\n",
    "    sat = axioms(features, labels)  # compute sat without dropout\n",
    "    metrics_dict['train_sat_kb'](sat)\n",
    "    # accuracy\n",
    "    predictions = model([features])\n",
    "    metrics_dict['train_accuracy'](labels, predictions)\n",
    "\n",
    "@tf.function\n",
    "def test_step(features, labels, optimizer):\n",
    "    # sat\n",
    "    sat = axioms(features, labels)\n",
    "    metrics_dict['test_sat_kb'](sat)\n",
    "    # accuracy\n",
    "    predictions = model([features])\n",
    "    metrics_dict['test_accuracy'](labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: ukr3c65a\n",
      "Sweep URL: https://wandb.ai/dds-paper/dds-paper/sweeps/ukr3c65a\n"
     ]
    }
   ],
   "source": [
    "# Sweep configuration\n",
    "sweep_config = {\n",
    "    \"name\": \"dds-sweep\",\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\n",
    "        \"name\": \"test_sat_kb\",\n",
    "        \"goal\": \"maximize\"\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"batch_size\": {\n",
    "            \"values\": [32, 64, 128, 256, 1024, 4096]\n",
    "        },\n",
    "        \"epochs\": {\n",
    "            \"values\": [5]\n",
    "        },\n",
    "        \"learning_rate\": {\n",
    "            \"min\": 0.0001,\n",
    "            \"max\": 0.1\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize the sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"dds-paper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import wandb  # Import wandb\n",
    "\n",
    "def train(epochs, metrics_dict, train_generator_func, test_generator_func, train_step, test_step,\n",
    "          num_train_steps, num_test_steps, track_metrics=1, csv_path=None, scheduled_parameters=defaultdict(lambda: {}), optimizer=None):\n",
    "\n",
    "    if csv_path is not None:\n",
    "        csv_file = open(csv_path, \"w+\")\n",
    "        headers = \",\".join([\"Epoch\"] + list(metrics_dict.keys()))\n",
    "        csv_template = \",\".join([\"{}\" for _ in range(len(metrics_dict) + 1)])\n",
    "        csv_file.write(headers + \"\\n\")\n",
    "\n",
    "    epoch_times = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Initialize accumulators for training metrics\n",
    "        total_train_sat_kb = 0\n",
    "        total_train_accuracy = 0\n",
    "        num_train_steps = 0\n",
    "\n",
    "        # Initialize accumulators for validation metrics\n",
    "        total_test_sat_kb = 0\n",
    "        total_test_accuracy = 0\n",
    "        num_test_steps = 0\n",
    "\n",
    "        # Reset metrics\n",
    "        for metrics in metrics_dict.values():\n",
    "            metrics.reset_states()\n",
    "\n",
    "        # Training loop\n",
    "        train_generator = train_generator_func()\n",
    "        pbar = tqdm(total=num_train_steps)\n",
    "        for batch_elements in train_generator:\n",
    "            train_step(*batch_elements, optimizer=optimizer, **scheduled_parameters[epoch])\n",
    "            \n",
    "            # Accumulate training metrics\n",
    "            total_train_sat_kb += metrics_dict['train_sat_kb'].result().numpy()\n",
    "            total_train_accuracy += metrics_dict['train_accuracy'].result().numpy()\n",
    "            num_train_steps += 1\n",
    "\n",
    "            pbar.update()\n",
    "        pbar.close()\n",
    "\n",
    "        # Compute average training metrics for the epoch\n",
    "        avg_train_sat_kb = total_train_sat_kb / num_train_steps\n",
    "        avg_train_accuracy = total_train_accuracy / num_train_steps\n",
    "\n",
    "        # Validation loop\n",
    "        test_generator = test_generator_func()\n",
    "        pbar = tqdm(total=num_test_steps)\n",
    "        for batch_elements in test_generator:\n",
    "            test_step(*batch_elements, optimizer=optimizer, **scheduled_parameters[epoch])\n",
    "\n",
    "            # Accumulate validation metrics\n",
    "            total_test_sat_kb += metrics_dict['test_sat_kb'].result().numpy()\n",
    "            total_test_accuracy += metrics_dict['test_accuracy'].result().numpy()\n",
    "            num_test_steps += 1\n",
    "\n",
    "            pbar.update()\n",
    "        pbar.close()\n",
    "\n",
    "        # Compute average validation metrics for the epoch\n",
    "        avg_test_sat_kb = total_test_sat_kb / num_test_steps\n",
    "        avg_test_accuracy = total_test_accuracy / num_test_steps\n",
    "\n",
    "        # Log the average metrics to Wandb\n",
    "        wandb.log({\n",
    "            'Epoch': epoch,\n",
    "            'Average Train Satisfaction': avg_train_sat_kb,\n",
    "            'Average Train Accuracy': avg_train_accuracy,\n",
    "            'Average Validation Satisfaction': avg_test_sat_kb,\n",
    "            'Average Validation Accuracy': avg_test_accuracy\n",
    "        })\n",
    "\n",
    "        # Additional logging\n",
    "        if csv_path is not None:\n",
    "            metrics_results = [metrics.result() for metrics in metrics_dict.values()]\n",
    "            csv_file.write(csv_template.format(epoch, *metrics_results) + \"\\n\")\n",
    "            csv_file.flush()\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_times.append(end_time - start_time)\n",
    "        start_time = end_time\n",
    "\n",
    "    if csv_path is not None:\n",
    "        csv_file.close()\n",
    "\n",
    "    return epoch_times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the learning rate as a tf.Variable so it can be updated\n",
    "learning_rate = tf.Variable(initial_value=0.001, trainable=False, dtype=tf.float32)\n",
    "\n",
    "# Initialize the optimizer with this learning rate variable\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "def train_wrapper():\n",
    "    # Initialize Wandb for this sweep run\n",
    "    run = wandb.init()\n",
    "\n",
    "    # Use hyperparameters from wandb.config\n",
    "    epochs = wandb.config.epochs\n",
    "    batch_size = wandb.config.batch_size\n",
    "    # Update the learning rate using the hyperparameter from wandb.config\n",
    "    learning_rate.assign(wandb.config.learning_rate)\n",
    "\n",
    "    # Rest of your existing setup code\n",
    "    num_train_steps = sum(1 for _ in data_generator(batch_size, data=train_path))\n",
    "    num_test_steps = sum(1 for _ in data_generator(batch_size, data=val_path))\n",
    "\n",
    "    def train_generator_func():\n",
    "        return data_generator_all(batch_size=wandb.config.batch_size, data=train_path)\n",
    "\n",
    "    def val_generator_func():\n",
    "        return data_generator_all(batch_size=wandb.config.batch_size, data=val_path)\n",
    "\n",
    "    # Call your existing train function\n",
    "    epoch_times = train(\n",
    "        wandb.config.epochs,\n",
    "        metrics_dict,\n",
    "        train_generator_func,\n",
    "        val_generator_func,\n",
    "        train_step,\n",
    "        test_step,\n",
    "        csv_path=\"/home/ubuntu/dds_paper/DDS_Paper/data/final.csv\",\n",
    "        track_metrics=1,\n",
    "        num_train_steps=num_train_steps,\n",
    "        num_test_steps=num_test_steps,\n",
    "        optimizer=optimizer \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "<IPython.core.display.HTML object>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread ChkStopThr:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 285, in check_stop_status\n",
      "    self._loop_check_status(\n",
      "  File \"/home/ubuntu/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 223, in _loop_check_status\n",
      "    local_handle = request()\n",
      "  File \"/home/ubuntu/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/interface/interface.py\", line 727, in deliver_stop_status\n",
      "    return self._deliver_stop_status(status)\n",
      "  File \"/home/ubuntu/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py\", line 450, in _deliver_stop_status\n",
      "    return self._deliver_record(record)\n",
      "  File \"/home/ubuntu/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py\", line 425, in _deliver_record\n",
      "    handle = mailbox._deliver_record(record, interface=self)\n",
      "  File \"/home/ubuntu/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/lib/mailbox.py\", line 455, in _deliver_record\n",
      "    interface._publish(record)\n",
      "  File \"/home/ubuntu/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/interface/interface_sock.py\", line 51, in _publish\n",
      "    self._sock_client.send_record_publish(record)\n",
      "  File \"/home/ubuntu/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py\", line 221, in send_record_publish\n",
      "    self.send_server_request(server_req)\n",
      "  File \"/home/ubuntu/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py\", line 155, in send_server_request\n",
      "    self._send_message(msg)\n",
      "  File \"/home/ubuntu/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py\", line 152, in _send_message\n",
      "    self._sendall_with_error_handle(header + data)\n",
      "  File \"/home/ubuntu/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n",
      "Exception in thread NetStatThr:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    sent = self._sock.send(data)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 267, in check_network_status\n",
      "    self._loop_check_status(\n",
      "  File \"/home/ubuntu/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 223, in _loop_check_status\n",
      "Exception in thread IntMsgThr:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    local_handle = request()\n",
      "  File \"/home/ubuntu/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/interface/interface.py\", line 735, in deliver_network_status\n",
      "    return self._deliver_network_status(status)\n",
      "  File \"/home/ubuntu/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py\", line 466, in _deliver_network_status\n",
      "    return self._deliver_record(record)\n",
      "  File \"/home/ubuntu/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py\", line 425, in _deliver_record\n",
      "    handle = mailbox._deliver_record(record, interface=self)\n",
      "  File \"/home/ubuntu/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/lib/mailbox.py\", line 455, in _deliver_record\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
      "    interface._publish(record)\n",
      "  File \"/home/ubuntu/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/interface/interface_sock.py\", line 51, in _publish\n",
      "    self._sock_client.send_record_publish(record)\n",
      "  File \"/home/ubuntu/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py\", line 221, in send_record_publish\n",
      "    self.send_server_request(server_req)\n",
      "  File \"/home/ubuntu/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py\", line 155, in send_server_request\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 299, in check_internal_messages\n",
      "    self._send_message(msg)\n",
      "  File \"/home/ubuntu/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py\", line 152, in _send_message\n",
      "    self._loop_check_status(\n",
      "  File \"/home/ubuntu/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 223, in _loop_check_status\n",
      "    self._sendall_with_error_handle(header + data)\n",
      "  File \"/home/ubuntu/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n",
      "    local_handle = request()\n",
      "  File \"/home/ubuntu/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/interface/interface.py\", line 743, in deliver_internal_messages\n",
      "    sent = self._sock.send(data)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "    return self._deliver_internal_messages(internal_message)\n",
      "  File \"/home/ubuntu/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py\", line 472, in _deliver_internal_messages\n",
      "    return self._deliver_record(record)\n",
      "  File \"/home/ubuntu/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py\", line 425, in _deliver_record\n",
      "    handle = mailbox._deliver_record(record, interface=self)\n",
      "  File \"/home/ubuntu/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/lib/mailbox.py\", line 455, in _deliver_record\n",
      "    interface._publish(record)\n",
      "  File \"/home/ubuntu/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/interface/interface_sock.py\", line 51, in _publish\n",
      "    self._sock_client.send_record_publish(record)\n",
      "  File \"/home/ubuntu/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py\", line 221, in send_record_publish\n",
      "    self.send_server_request(server_req)\n",
      "  File \"/home/ubuntu/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py\", line 155, in send_server_request\n",
      "    self._send_message(msg)\n",
      "  File \"/home/ubuntu/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py\", line 152, in _send_message\n",
      "    self._sendall_with_error_handle(header + data)\n",
      "  File \"/home/ubuntu/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n",
      "    sent = self._sock.send(data)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: iczr3ifc with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.03744735484725125\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/dds_paper/DDS_Paper/wandb/run-20230911_163229-iczr3ifc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dds-paper/dds-paper/runs/iczr3ifc' target=\"_blank\">unique-sweep-1</a></strong> to <a href='https://wandb.ai/dds-paper/dds-paper' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/dds-paper/dds-paper/sweeps/ukr3c65a' target=\"_blank\">https://wandb.ai/dds-paper/dds-paper/sweeps/ukr3c65a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dds-paper/dds-paper' target=\"_blank\">https://wandb.ai/dds-paper/dds-paper</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/dds-paper/dds-paper/sweeps/ukr3c65a' target=\"_blank\">https://wandb.ai/dds-paper/dds-paper/sweeps/ukr3c65a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dds-paper/dds-paper/runs/iczr3ifc' target=\"_blank\">https://wandb.ai/dds-paper/dds-paper/runs/iczr3ifc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 90832/90833 [37:46<00:00, 40.08it/s]  \n",
      "100%|█████████▉| 22708/22709 [04:27<00:00, 84.87it/s] \n",
      " 28%|██▊       | 25374/90833 [10:20<27:46, 39.27it/s]\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n",
      " 28%|██▊       | 25378/90833 [10:20<29:47, 36.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function _WandbInit._pause_backend at 0x7f88b7e73ac0> (for post_run_cell):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 25415/90833 [10:22<46:12, 23.59it/s]"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/backcall/backcall.py:104\u001b[0m, in \u001b[0;36mcallback_prototype.<locals>.adapt.<locals>.adapted\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 kwargs\u001b[39m.\u001b[39mpop(name)\n\u001b[1;32m    103\u001b[0m \u001b[39m#            print(args, kwargs, unmatched_pos, cut_positional, unmatched_kw)\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m             \u001b[39mreturn\u001b[39;00m callback(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:436\u001b[0m, in \u001b[0;36m_WandbInit._pause_backend\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackend\u001b[39m.\u001b[39minterface \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    435\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mpausing backend\u001b[39m\u001b[39m\"\u001b[39m)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 436\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackend\u001b[39m.\u001b[39;49minterface\u001b[39m.\u001b[39;49mpublish_pause()\n",
      "File \u001b[0;32m~/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/interface/interface.py:626\u001b[0m, in \u001b[0;36mInterfaceBase.publish_pause\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpublish_pause\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    625\u001b[0m     pause \u001b[39m=\u001b[39m pb\u001b[39m.\u001b[39mPauseRequest()\n\u001b[0;32m--> 626\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_publish_pause(pause)\n",
      "File \u001b[0;32m~/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py:343\u001b[0m, in \u001b[0;36mInterfaceShared._publish_pause\u001b[0;34m(self, pause)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_publish_pause\u001b[39m(\u001b[39mself\u001b[39m, pause: pb\u001b[39m.\u001b[39mPauseRequest) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    342\u001b[0m     rec \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_request(pause\u001b[39m=\u001b[39mpause)\n\u001b[0;32m--> 343\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_publish(rec)\n",
      "File \u001b[0;32m~/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/interface/interface_sock.py:51\u001b[0m, in \u001b[0;36mInterfaceSock._publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_publish\u001b[39m(\u001b[39mself\u001b[39m, record: \u001b[39m\"\u001b[39m\u001b[39mpb.Record\u001b[39m\u001b[39m\"\u001b[39m, local: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     50\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_assign(record)\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock_client\u001b[39m.\u001b[39;49msend_record_publish(record)\n",
      "File \u001b[0;32m~/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:221\u001b[0m, in \u001b[0;36mSockClient.send_record_publish\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m    219\u001b[0m server_req \u001b[39m=\u001b[39m spb\u001b[39m.\u001b[39mServerRequest()\n\u001b[1;32m    220\u001b[0m server_req\u001b[39m.\u001b[39mrecord_publish\u001b[39m.\u001b[39mCopyFrom(record)\n\u001b[0;32m--> 221\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend_server_request(server_req)\n",
      "File \u001b[0;32m~/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:155\u001b[0m, in \u001b[0;36mSockClient.send_server_request\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msend_server_request\u001b[39m(\u001b[39mself\u001b[39m, msg: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 155\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_message(msg)\n",
      "File \u001b[0;32m~/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:152\u001b[0m, in \u001b[0;36mSockClient._send_message\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    150\u001b[0m header \u001b[39m=\u001b[39m struct\u001b[39m.\u001b[39mpack(\u001b[39m\"\u001b[39m\u001b[39m<BI\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mord\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mW\u001b[39m\u001b[39m\"\u001b[39m), raw_size)\n\u001b[1;32m    151\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m--> 152\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sendall_with_error_handle(header \u001b[39m+\u001b[39;49m data)\n",
      "File \u001b[0;32m~/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:130\u001b[0m, in \u001b[0;36mSockClient._sendall_with_error_handle\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    128\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mmonotonic()\n\u001b[1;32m    129\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     sent \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49msend(data)\n\u001b[1;32m    131\u001b[0m     \u001b[39m# sent equal to 0 indicates a closed socket\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[39mif\u001b[39;00m sent \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 25518/90833 [10:24<24:27, 44.50it/s]"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id, function=train_wrapper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function _WandbInit._resume_backend at 0x7f88b800a680> (for pre_run_cell):\n"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/backcall/backcall.py:104\u001b[0m, in \u001b[0;36mcallback_prototype.<locals>.adapt.<locals>.adapted\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 kwargs\u001b[39m.\u001b[39mpop(name)\n\u001b[1;32m    103\u001b[0m \u001b[39m#            print(args, kwargs, unmatched_pos, cut_positional, unmatched_kw)\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m             \u001b[39mreturn\u001b[39;00m callback(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:441\u001b[0m, in \u001b[0;36m_WandbInit._resume_backend\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackend \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackend\u001b[39m.\u001b[39minterface \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    440\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mresuming backend\u001b[39m\u001b[39m\"\u001b[39m)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 441\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackend\u001b[39m.\u001b[39;49minterface\u001b[39m.\u001b[39;49mpublish_resume()\n",
      "File \u001b[0;32m~/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/interface/interface.py:634\u001b[0m, in \u001b[0;36mInterfaceBase.publish_resume\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpublish_resume\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    633\u001b[0m     resume \u001b[39m=\u001b[39m pb\u001b[39m.\u001b[39mResumeRequest()\n\u001b[0;32m--> 634\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_publish_resume(resume)\n",
      "File \u001b[0;32m~/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py:347\u001b[0m, in \u001b[0;36mInterfaceShared._publish_resume\u001b[0;34m(self, resume)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_publish_resume\u001b[39m(\u001b[39mself\u001b[39m, resume: pb\u001b[39m.\u001b[39mResumeRequest) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    346\u001b[0m     rec \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_request(resume\u001b[39m=\u001b[39mresume)\n\u001b[0;32m--> 347\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_publish(rec)\n",
      "File \u001b[0;32m~/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/interface/interface_sock.py:51\u001b[0m, in \u001b[0;36mInterfaceSock._publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_publish\u001b[39m(\u001b[39mself\u001b[39m, record: \u001b[39m\"\u001b[39m\u001b[39mpb.Record\u001b[39m\u001b[39m\"\u001b[39m, local: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     50\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_assign(record)\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock_client\u001b[39m.\u001b[39;49msend_record_publish(record)\n",
      "File \u001b[0;32m~/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:221\u001b[0m, in \u001b[0;36mSockClient.send_record_publish\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m    219\u001b[0m server_req \u001b[39m=\u001b[39m spb\u001b[39m.\u001b[39mServerRequest()\n\u001b[1;32m    220\u001b[0m server_req\u001b[39m.\u001b[39mrecord_publish\u001b[39m.\u001b[39mCopyFrom(record)\n\u001b[0;32m--> 221\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend_server_request(server_req)\n",
      "File \u001b[0;32m~/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:155\u001b[0m, in \u001b[0;36mSockClient.send_server_request\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msend_server_request\u001b[39m(\u001b[39mself\u001b[39m, msg: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 155\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_message(msg)\n",
      "File \u001b[0;32m~/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:152\u001b[0m, in \u001b[0;36mSockClient._send_message\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    150\u001b[0m header \u001b[39m=\u001b[39m struct\u001b[39m.\u001b[39mpack(\u001b[39m\"\u001b[39m\u001b[39m<BI\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mord\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mW\u001b[39m\u001b[39m\"\u001b[39m), raw_size)\n\u001b[1;32m    151\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m--> 152\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sendall_with_error_handle(header \u001b[39m+\u001b[39;49m data)\n",
      "File \u001b[0;32m~/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:130\u001b[0m, in \u001b[0;36mSockClient._sendall_with_error_handle\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    128\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mmonotonic()\n\u001b[1;32m    129\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     sent \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49msend(data)\n\u001b[1;32m    131\u001b[0m     \u001b[39m# sent equal to 0 indicates a closed socket\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[39mif\u001b[39;00m sent \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function _WandbInit._pause_backend at 0x7f88b7e73ac0> (for post_run_cell):\n"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/backcall/backcall.py:104\u001b[0m, in \u001b[0;36mcallback_prototype.<locals>.adapt.<locals>.adapted\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 kwargs\u001b[39m.\u001b[39mpop(name)\n\u001b[1;32m    103\u001b[0m \u001b[39m#            print(args, kwargs, unmatched_pos, cut_positional, unmatched_kw)\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m             \u001b[39mreturn\u001b[39;00m callback(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:436\u001b[0m, in \u001b[0;36m_WandbInit._pause_backend\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackend\u001b[39m.\u001b[39minterface \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    435\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mpausing backend\u001b[39m\u001b[39m\"\u001b[39m)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 436\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackend\u001b[39m.\u001b[39;49minterface\u001b[39m.\u001b[39;49mpublish_pause()\n",
      "File \u001b[0;32m~/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/interface/interface.py:626\u001b[0m, in \u001b[0;36mInterfaceBase.publish_pause\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpublish_pause\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    625\u001b[0m     pause \u001b[39m=\u001b[39m pb\u001b[39m.\u001b[39mPauseRequest()\n\u001b[0;32m--> 626\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_publish_pause(pause)\n",
      "File \u001b[0;32m~/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py:343\u001b[0m, in \u001b[0;36mInterfaceShared._publish_pause\u001b[0;34m(self, pause)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_publish_pause\u001b[39m(\u001b[39mself\u001b[39m, pause: pb\u001b[39m.\u001b[39mPauseRequest) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    342\u001b[0m     rec \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_request(pause\u001b[39m=\u001b[39mpause)\n\u001b[0;32m--> 343\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_publish(rec)\n",
      "File \u001b[0;32m~/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/interface/interface_sock.py:51\u001b[0m, in \u001b[0;36mInterfaceSock._publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_publish\u001b[39m(\u001b[39mself\u001b[39m, record: \u001b[39m\"\u001b[39m\u001b[39mpb.Record\u001b[39m\u001b[39m\"\u001b[39m, local: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     50\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_assign(record)\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock_client\u001b[39m.\u001b[39;49msend_record_publish(record)\n",
      "File \u001b[0;32m~/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:221\u001b[0m, in \u001b[0;36mSockClient.send_record_publish\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m    219\u001b[0m server_req \u001b[39m=\u001b[39m spb\u001b[39m.\u001b[39mServerRequest()\n\u001b[1;32m    220\u001b[0m server_req\u001b[39m.\u001b[39mrecord_publish\u001b[39m.\u001b[39mCopyFrom(record)\n\u001b[0;32m--> 221\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend_server_request(server_req)\n",
      "File \u001b[0;32m~/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:155\u001b[0m, in \u001b[0;36mSockClient.send_server_request\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msend_server_request\u001b[39m(\u001b[39mself\u001b[39m, msg: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 155\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_message(msg)\n",
      "File \u001b[0;32m~/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:152\u001b[0m, in \u001b[0;36mSockClient._send_message\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    150\u001b[0m header \u001b[39m=\u001b[39m struct\u001b[39m.\u001b[39mpack(\u001b[39m\"\u001b[39m\u001b[39m<BI\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mord\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mW\u001b[39m\u001b[39m\"\u001b[39m), raw_size)\n\u001b[1;32m    151\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m--> 152\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sendall_with_error_handle(header \u001b[39m+\u001b[39;49m data)\n",
      "File \u001b[0;32m~/dds_paper/DDS_Paper/.venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:130\u001b[0m, in \u001b[0;36mSockClient._sendall_with_error_handle\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    128\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mmonotonic()\n\u001b[1;32m    129\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     sent \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49msend(data)\n\u001b[1;32m    131\u001b[0m     \u001b[39m# sent equal to 0 indicates a closed socket\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[39mif\u001b[39;00m sent \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 256\n",
    "\n",
    "# EPOCHS = 5\n",
    "\n",
    "# # Get the number of training and testing steps per epoch\n",
    "# num_train_steps = sum(1 for _ in data_generator(batch_size, data='train.csv'))\n",
    "# num_test_steps = sum(1 for _ in data_generator(batch_size, data='val.csv'))\n",
    "\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# # Define your generator functions\n",
    "# def train_generator_func():\n",
    "#     return data_generator_all(batch_size=batch_size, data='train.csv')\n",
    "\n",
    "# def val_generator_func():\n",
    "#     return data_generator_all(batch_size=batch_size, data='val.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# training = train(\n",
    "#     EPOCHS,\n",
    "#     metrics_dict,\n",
    "#     train_generator_func,\n",
    "#     val_generator_func,\n",
    "#     train_step,\n",
    "#     test_step,\n",
    "#     csv_path=\"final.csv\",\n",
    "#     track_metrics=1,\n",
    "#     num_train_steps=num_train_steps,\n",
    "#     num_test_steps=num_test_steps\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work_stable",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
