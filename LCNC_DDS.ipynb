{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import tensorflow as tf\n",
    "from joblib import dump, load\n",
    "import ltn\n",
    "\n",
    "dataset_path = 'C:\\\\work_ssd\\\\DDS_Paper\\\\data\\\\DDS_Data_SEU'\n",
    "\n",
    "PGB_path = 'C:\\\\work_ssd\\\\DDS_Paper\\\\data\\\\DDS_Data_SEU\\\\PGB'\n",
    "RGB_path = 'C:\\\\work_ssd\\\\DDS_Paper\\\\data\\\\DDS_Data_SEU\\\\RGB'\n",
    "\n",
    "# Specify the CSV file path\n",
    "csv_file = 'C:\\\\work_ssd\\\\DDS_Paper\\\\data\\\\data_robust.csv'\n",
    "preprocessor_file = 'preprocessor.joblib'\n",
    "\n",
    "\n",
    "np.random.seed(45)\n",
    "\n",
    "# Set the chunk size for reading the CSV\n",
    "chunk_size = 100000  # Adjust the chunk size according to your memory limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fault(file_name):\n",
    "    match = re.search(r'\\d+', file_name)\n",
    "    if match:\n",
    "        return int(match.group(0)[0])  # Extract the first digit\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def process_files_to_csv(data_folders, output_file):\n",
    "    # Check if the file already exists\n",
    "    if os.path.isfile(output_file):\n",
    "        print(f\"File {output_file} already exists. Skipping processing.\")\n",
    "        return\n",
    "\n",
    "    total_files = sum([len(files) for data_folder in data_folders for r, d, files in os.walk(data_folder)])\n",
    "\n",
    "    with open(output_file, 'w', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow(['Channel1', 'Channel2', 'Channel3', 'Channel4', 'Channel5', 'Channel6', 'Channel7', 'Channel8', 'Speed', 'Type', 'Fault'])\n",
    "\n",
    "        with tqdm(total=total_files, desc=\"Processing files\", unit=\"file\") as pbar:\n",
    "            for data_folder in data_folders:\n",
    "                for root, dirs, files in os.walk(data_folder):\n",
    "                    if '.ipynb_checkpoints' in root:\n",
    "                        continue  # Skip .ipynb_checkpoints folders\n",
    "                    for file in files:\n",
    "                        if file.endswith('.txt'):\n",
    "                            file_path = os.path.join(root, file)\n",
    "                            path_parts = file_path.split('\\\\')\n",
    "                            variable_speed = 'Variable_speed' in file_path\n",
    "                            type_index = -4 if variable_speed else -3\n",
    "                            type = path_parts[type_index] if path_parts[type_index] in ['PGB', 'RGB'] else None\n",
    "                            if type is not None:\n",
    "                                speed_index = -3 if variable_speed else -2\n",
    "                                speed = path_parts[speed_index]\n",
    "                                fault = extract_fault(file)\n",
    "\n",
    "                                data = pd.read_csv(file_path, sep='\\t', encoding='ISO-8859-1')\n",
    "                                reshaped_data = data.values[:, :]\n",
    "\n",
    "                                for row_data in tqdm(reshaped_data, desc=\"Processing rows\", unit=\"row\", leave=False):\n",
    "                                    row = row_data.tolist() + [speed, type, fault]\n",
    "                                    csv_writer.writerow(row)\n",
    "                            pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File C:\\work_ssd\\DDS_Paper\\data\\data_robust.csv already exists. Skipping processing.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "process_files_to_csv([PGB_path, RGB_path], csv_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV: 7267chunk [09:46, 12.39chunk/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fault: 0, Count: 80740352\n",
      "Fault: 1, Count: 80740352\n",
      "Fault: 2, Count: 80740352\n",
      "Fault: 3, Count: 80740352\n",
      "Fault: 4, Count: 80740352\n",
      "Fault: 5, Count: 80740352\n",
      "Fault: 6, Count: 80740352\n",
      "Fault: 7, Count: 80740352\n",
      "Fault: 8, Count: 80740352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize a dictionary to store the fault counts\n",
    "fault_counts = {}\n",
    "\n",
    "# Iterate through the CSV file using chunksize\n",
    "with tqdm(total=1, unit='chunk', desc='Processing CSV') as pbar:\n",
    "    for chunk in pd.read_csv(csv_file, chunksize=chunk_size):\n",
    "        \n",
    "        #print(chunk)\n",
    "        # Assuming there is a column named 'fault' in the CSV representing the fault type\n",
    "        fault_column = 'Fault'\n",
    "\n",
    "        # Count the occurrences of each fault in the current chunk\n",
    "        fault_chunk_counts = chunk[fault_column].value_counts()\n",
    "\n",
    "        # Aggregate the counts with the overall fault_counts dictionary\n",
    "        for fault, count in fault_chunk_counts.items():\n",
    "            fault_counts[fault] = fault_counts.get(fault, 0) + count\n",
    "\n",
    "        pbar.update()\n",
    "\n",
    "# Print the fault counts\n",
    "for fault, count in fault_counts.items():\n",
    "    print(f\"Fault: {fault}, Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV: 7267chunk [10:25, 11.62chunk/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed Counts:\n",
      "Speed: 20_0, Count: 75497472\n",
      "Speed: 30_0, Count: 75497472\n",
      "Speed: 30_1, Count: 75497472\n",
      "Speed: 30_2, Count: 75497472\n",
      "Speed: 30_3, Count: 75497472\n",
      "Speed: 30_4, Count: 75497472\n",
      "Speed: 30_5, Count: 75497472\n",
      "Speed: 40_0, Count: 75497472\n",
      "Speed: 50_0, Count: 75497472\n",
      "Speed: Variable_speed, Count: 47185920\n",
      "Type Counts:\n",
      "Type: PGB, Count: 363331584\n",
      "Type: RGB, Count: 363331584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize dictionaries to store the counts\n",
    "speed_counts = {}\n",
    "type_counts = {}\n",
    "\n",
    "# Iterate through the CSV file using chunksize\n",
    "with tqdm(total=1, unit='chunk', desc='Processing CSV') as pbar:\n",
    "    for chunk in pd.read_csv(csv_file, chunksize=chunk_size):\n",
    "        # Assuming there is a column named 'Speed' in the CSV representing the speed values\n",
    "        \n",
    "        speed_column = 'Speed'\n",
    "\n",
    "        # Count the occurrences of each speed value in the current chunk\n",
    "        speed_chunk_counts = chunk[speed_column].value_counts()\n",
    "\n",
    "        # Aggregate the counts with the overall speed_counts dictionary\n",
    "        for speed, count in speed_chunk_counts.items():\n",
    "            speed_counts[speed] = speed_counts.get(speed, 0) + count\n",
    "\n",
    "        # Assuming there is a column named 'Type' in the CSV representing the types\n",
    "        type_column = 'Type'\n",
    "\n",
    "        # Count the occurrences of each type in the current chunk\n",
    "        type_chunk_counts = chunk[type_column].value_counts()\n",
    "\n",
    "        # Aggregate the counts with the overall type_counts dictionary\n",
    "        for typ, count in type_chunk_counts.items():\n",
    "            type_counts[typ] = type_counts.get(typ, 0) + count\n",
    "\n",
    "        pbar.update()\n",
    "\n",
    "# Print the speed counts\n",
    "print(\"Speed Counts:\")\n",
    "for speed, count in speed_counts.items():\n",
    "    print(f\"Speed: {speed}, Count: {count}\")\n",
    "\n",
    "# Print the type counts\n",
    "print(\"Type Counts:\")\n",
    "for typ, count in type_counts.items():\n",
    "    print(f\"Type: {typ}, Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the numeric and categorical features\n",
    "numerical_features = ['Channel1', 'Channel2', 'Channel3', 'Channel4', 'Channel5', 'Channel6', 'Channel7', 'Channel8']\n",
    "categorical_features = ['Speed', 'Type']\n",
    "\n",
    "# initialize the scalers\n",
    "scaler = StandardScaler()\n",
    "encoder = OneHotEncoder(categories=[['20_0','30_0','30_1','30_2', '30_3','30_4','30_5','40_0','50_0', 'Variable_speed'], ['PGB', 'RGB']])  # specify the unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', scaler, numerical_features),\n",
    "        ('cat', encoder, categorical_features)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the total size of the file in bytes\n",
    "total_size = os.path.getsize(csv_file)\n",
    "\n",
    "# define the number of lines to read for the estimate\n",
    "sample_lines = 1000\n",
    "\n",
    "# read the first few lines of the file to get the average bytes per line\n",
    "with open(csv_file, 'r') as file:\n",
    "    lines = [next(file) for _ in range(sample_lines)]\n",
    "avg_bytes_per_line = sum(len(line) for line in lines) / sample_lines\n",
    "\n",
    "# estimate the total number of lines in the file\n",
    "estimated_lines = total_size // avg_bytes_per_line\n",
    "\n",
    "# calculate the estimated total number of chunks\n",
    "total_chunks = estimated_lines // chunk_size\n",
    "if estimated_lines % chunk_size != 0:\n",
    "    total_chunks += 1\n",
    "    \n",
    "# generate a boolean mask for the entire dataset\n",
    "mask = np.random.rand(int(estimated_lines)) < 0.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessor file already exists.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(preprocessor_file):\n",
    "    # initialize a progress bar\n",
    "    progress_bar = tqdm(total=total_chunks, desc=\"Fitting Scaler and Encoder\", unit=\"chunk\")\n",
    "\n",
    "    # fit the scaler and encoder incrementally\n",
    "    for chunk in pd.read_csv(csv_file, chunksize=chunk_size):\n",
    "        chunk_mask = np.random.rand(len(chunk)) < 0.8\n",
    "        train_chunk = chunk[chunk_mask]\n",
    "        scaler.partial_fit(train_chunk[numerical_features])\n",
    "        encoder.fit(train_chunk[categorical_features])\n",
    "        progress_bar.update()\n",
    "\n",
    "    # close the progress bar\n",
    "    progress_bar.close()\n",
    "else:\n",
    "    print(\"Preprocessor file already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessor file already exists.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(preprocessor_file):\n",
    "    # Save the preprocessor\n",
    "    dump(preprocessor, 'preprocessor.joblib')\n",
    "\n",
    "    # Load the preprocessor when needed\n",
    "    preprocessor = load('preprocessor.joblib')\n",
    "else:\n",
    "    print(\"Preprocessor file already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting preprocessor:  98%|█████████▊| 7267/7398.0 [14:56<00:16,  8.10chunk/s]\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(preprocessor_file):\n",
    "    progress_bar = tqdm(total=total_chunks, desc=\"Fitting preprocessor\", unit=\"chunk\")\n",
    "\n",
    "    # fit the preprocessor on the entire training data\n",
    "    for chunk in pd.read_csv(csv_file, chunksize=chunk_size):\n",
    "        chunk_mask = np.random.rand(len(chunk)) < 0.8\n",
    "        train_chunk = chunk[chunk_mask]\n",
    "        preprocessor.fit(train_chunk)\n",
    "        progress_bar.update()\n",
    "    # close the progress bar\n",
    "    progress_bar.close()\n",
    "else:\n",
    "    print(\"There is no preprocessor file saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator():\n",
    "    for chunk in pd.read_csv(csv_file, chunksize=chunk_size):\n",
    "        train_chunk = chunk.sample(frac=0.8)  # Select a random 80% fraction of the chunk\n",
    "        # perform the transformations\n",
    "        transformed_chunk = preprocessor.transform(train_chunk)\n",
    "        labels = train_chunk['Fault']  # Assuming 'Fault' is your label column\n",
    "        yield transformed_chunk, labels\n",
    "\n",
    "def val_generator():\n",
    "    for chunk in pd.read_csv(csv_file, chunksize=chunk_size):\n",
    "        val_chunk = chunk.drop(train_chunk.index)  # Use the rest 20% for validation\n",
    "        # perform the transformations\n",
    "        transformed_chunk = preprocessor.transform(val_chunk)\n",
    "        labels = val_chunk['Fault']  # Assuming 'Fault' is your label column\n",
    "        yield transformed_chunk, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create tf.data.Datasets for training and validation\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    train_generator,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(None, 20), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.int32)\n",
    "    )\n",
    ").batch(batch_size)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_generator(\n",
    "    val_generator,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(None, 20), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.int32)\n",
    "    )\n",
    ").batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[-3.9093413  -0.72891164  2.5583293  ...  0.          1.\n",
      "    0.        ]\n",
      "  [ 0.47791898  0.3051755   0.4655278  ...  0.          1.\n",
      "    0.        ]\n",
      "  [ 0.514894    1.240472   -0.24753553 ...  0.          1.\n",
      "    0.        ]\n",
      "  ...\n",
      "  [-0.37117445 -0.87446713  0.13548978 ...  0.          1.\n",
      "    0.        ]\n",
      "  [-2.556921    0.6962464  -1.2177547  ...  0.          1.\n",
      "    0.        ]\n",
      "  [-1.7036083  -0.24002437  1.6545904  ...  0.          1.\n",
      "    0.        ]]\n",
      "\n",
      " [[-2.1897244  -1.8590616  -0.726982   ...  0.          1.\n",
      "    0.        ]\n",
      "  [-4.0798926  -0.56640387  0.0042034  ...  0.          1.\n",
      "    0.        ]\n",
      "  [ 0.93638724  2.2505922  -1.5946736  ...  0.          1.\n",
      "    0.        ]\n",
      "  ...\n",
      "  [-4.074008   -0.1963772   1.1260962  ...  0.          1.\n",
      "    0.        ]\n",
      "  [ 1.5678498   0.41351405  0.4821725  ...  0.          1.\n",
      "    0.        ]\n",
      "  [-0.25414234 -0.25892517  2.5041602  ...  0.          1.\n",
      "    0.        ]]], shape=(2, 80000, 20), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[-3.9093413  -0.72891164  2.5583293  ...  0.          1.\n",
      "    0.        ]\n",
      "  [ 0.47791898  0.3051755   0.4655278  ...  0.          1.\n",
      "    0.        ]\n",
      "  [ 0.514894    1.240472   -0.24753553 ...  0.          1.\n",
      "    0.        ]\n",
      "  ...\n",
      "  [-0.37117445 -0.87446713  0.13548978 ...  0.          1.\n",
      "    0.        ]\n",
      "  [-2.556921    0.6962464  -1.2177547  ...  0.          1.\n",
      "    0.        ]\n",
      "  [-1.7036083  -0.24002437  1.6545904  ...  0.          1.\n",
      "    0.        ]]\n",
      "\n",
      " [[-2.1897244  -1.8590616  -0.726982   ...  0.          1.\n",
      "    0.        ]\n",
      "  [-4.0798926  -0.56640387  0.0042034  ...  0.          1.\n",
      "    0.        ]\n",
      "  [ 0.93638724  2.2505922  -1.5946736  ...  0.          1.\n",
      "    0.        ]\n",
      "  ...\n",
      "  [-4.074008   -0.1963772   1.1260962  ...  0.          1.\n",
      "    0.        ]\n",
      "  [ 1.5678498   0.41351405  0.4821725  ...  0.          1.\n",
      "    0.        ]\n",
      "  [-0.25414234 -0.25892517  2.5041602  ...  0.          1.\n",
      "    0.        ]]], shape=(2, 80000, 20), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# # train the model on the training dataset\n",
    "# last_train_batch = None\n",
    "# for train_batch in train_dataset.batch(2):\n",
    "#     # perform a training step here\n",
    "#     print(train_batch)\n",
    "#     last_train_batch = train_batch\n",
    "#     break\n",
    "\n",
    "# if last_train_batch is not None:\n",
    "#     print(last_train_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM as backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm1 = tf.keras.layers.LSTM(64, return_sequences=True)\n",
    "        self.lstm2 = tf.keras.layers.LSTM(32)\n",
    "        self.dense = tf.keras.layers.Dense(9)  # no activation function here\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.lstm1(inputs)\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = self.lstm2(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        return self.dense(x)\n",
    "\n",
    "# Create an instance of the model\n",
    "model = LSTMModel()\n",
    "\n",
    "# Define a predicate using the model\n",
    "p = ltn.Predicate(model, activation_function=\"softmax\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants to index/iterate on the classes\n",
    "HEALTHY = 0\n",
    "CTF = 1\n",
    "MTF = 2\n",
    "RCF = 3\n",
    "SWF = 4\n",
    "BWF = 5\n",
    "CWF = 6\n",
    "IRF = 7\n",
    "ORF = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Variables\n",
    "class_healthy = ltn.Variable(\"class_healthy\", HEALTHY)\n",
    "class_CTF = ltn.Variable(\"class_CTF\", CTF)\n",
    "class_MTF = ltn.Variable(\"class_MTF\", MTF)\n",
    "class_RCF = ltn.Variable(\"class_RCF\", RCF)\n",
    "class_SWF = ltn.Variable(\"class_SWF\", SWF)\n",
    "class_BWF = ltn.Variable(\"class_BWF\", BWF)\n",
    "class_CWF = ltn.Variable(\"class_CWF\", CWF)\n",
    "class_IRF = ltn.Variable(\"class_IRF\", IRF)\n",
    "class_ORF = ltn.Variable(\"class_ORF\", ORF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#operators\n",
    "Not = ltn.Wrapper_Connective(ltn.fuzzy_ops.Not_Std())\n",
    "And = ltn.Wrapper_Connective(ltn.fuzzy_ops.And_Prod())\n",
    "Or = ltn.Wrapper_Connective(ltn.fuzzy_ops.Or_ProbSum())\n",
    "Implies = ltn.Wrapper_Connective(ltn.fuzzy_ops.Implies_Reichenbach())\n",
    "Forall = ltn.Wrapper_Quantifier(ltn.fuzzy_ops.Aggreg_pMeanError(p=2),semantics=\"forall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mutual Exclusivity\n",
    "\n",
    "- Each fault state is mutually exclusive with the other states, meaning that a gearbox can't simultaneously have multiple fault types. You can express this using the logical Not and And operations in LTN.\n",
    "\n",
    "\n",
    "Single Fault State: \n",
    "\n",
    "- Each gearbox has at least one fault state (which could be the healthy state, represented by fault type 0). You could express this using the logical Or operation.\n",
    "\n",
    "\n",
    "Correlation Between Features and Fault Types .... maybe?\n",
    "\n",
    "- If you have knowledge about which features are more likely to be affected by specific faults, you can encode this knowledge into the axioms. For instance, if you know that a particular vibration pattern in Channel 1 is indicative of a 'Missing Tooth Fault' (MTF), then whenever that pattern appears, the MTF state should be more likely. Note that this would require a more complex setup, where you divide your features into groups and apply the predicate to these groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the formula aggregator\n",
    "formula_aggregator = ltn.Wrapper_Formula_Aggregator(ltn.fuzzy_ops.Aggreg_pMeanError(p=2))\n",
    "\n",
    "@tf.function\n",
    "def axioms(features, labels, training=False):\n",
    "    x_healthy = ltn.Variable(\"x_healthy\", features[labels == HEALTHY])\n",
    "    x_CTF = ltn.Variable(\"x_CTF\", features[labels == CTF])\n",
    "    x_MTF = ltn.Variable(\"x_MTF\", features[labels == MTF])\n",
    "    x_RCF = ltn.Variable(\"x_RCF\", features[labels == RCF])\n",
    "    x_SWF = ltn.Variable(\"x_SWF\", features[labels == SWF])\n",
    "    x_BWF = ltn.Variable(\"x_BWF\", features[labels == BWF])\n",
    "    x_CWF = ltn.Variable(\"x_CWF\", features[labels == CWF])\n",
    "    x_IRF = ltn.Variable(\"x_IRF\", features[labels == IRF])\n",
    "    x_ORF = ltn.Variable(\"x_ORF\", features[labels == ORF])\n",
    "\n",
    "    all_faults = [x_healthy, x_CTF, x_MTF, x_RCF, x_SWF, x_BWF, x_CWF, x_IRF, x_ORF]\n",
    "    all_classes = [class_healthy, class_CTF, class_MTF, class_RCF, class_SWF, class_BWF, class_CWF, class_IRF, class_ORF]\n",
    "\n",
    "    axioms = [\n",
    "        Forall(x, p([x, class_], training=training)) for x, class_ in zip(all_faults, all_classes)\n",
    "    ]\n",
    "\n",
    "    # Mutual exclusivity\n",
    "    for i in range(len(all_faults)):\n",
    "        for j in range(i+1, len(all_faults)):\n",
    "            axioms.append(Not(And(p([all_faults[i], all_classes[j]], training=training), p([all_faults[j], all_classes[i]], training=training))))\n",
    "\n",
    "    # Single fault state\n",
    "    axioms.append(Or([p([x, class_], training=training) for x, class_ in zip(all_faults, all_classes)]))\n",
    "\n",
    "    sat_level = formula_aggregator(axioms).tensor\n",
    "    return sat_level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute and print the initial satisfaction level\n",
    "def print_sat_level(data):\n",
    "    features, labels = data\n",
    "    print(\"Initial sat level %.5f\"%axioms(features,labels))\n",
    "\n",
    "# Get the first batch of the test dataset\n",
    "first_batch = next(iter(train_dataset))\n",
    "\n",
    "# Print the initial satisfaction level for the first batch\n",
    "print_sat_level(first_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the initial satisfaction level for each batch of the test dataset\n",
    "for data in train_dataset:\n",
    "    print_sat_level(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict = {\n",
    "    'train_sat_kb': tf.keras.metrics.Mean(name='train_sat_kb'),\n",
    "    'test_sat_kb': tf.keras.metrics.Mean(name='test_sat_kb'),\n",
    "    'train_accuracy': tf.keras.metrics.CategoricalAccuracy(name=\"train_accuracy\"),\n",
    "    'test_accuracy': tf.keras.metrics.CategoricalAccuracy(name=\"test_accuracy\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam optimizer for updating model parameters\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Use the built-in tf.data.Dataset API to shuffle and batch the data\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "val_dataset = val_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train_step\n",
    "@tf.function\n",
    "def train_step(features, labels):\n",
    "    # sat and update\n",
    "    with tf.GradientTape() as tape:\n",
    "        sat = axioms(features, labels, training=True)\n",
    "        loss = 1. - sat\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    # compute sat without dropout\n",
    "    sat = axioms(features, labels) \n",
    "    metrics_dict['train_sat_kb'](sat)\n",
    "    \n",
    "    # accuracy\n",
    "    predictions = model(features, training=False)  # get predictions from the model\n",
    "    metrics_dict['train_accuracy'](tf.one_hot(labels, 9), tf.nn.softmax(predictions))\n",
    "\n",
    "# Define test_step\n",
    "@tf.function\n",
    "def test_step(features, labels):\n",
    "    # sat\n",
    "    sat = axioms(features, labels)\n",
    "    metrics_dict['test_sat_kb'](sat)\n",
    "    \n",
    "    # accuracy\n",
    "    predictions = model(features, training=False)  # get predictions from the model\n",
    "    metrics_dict['test_accuracy'](tf.one_hot(labels, 9), tf.nn.softmax(predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: LSTMs may not be the best approach as a backbone. While LSTM models are excellent for sequence prediction problems where temporal dependencies matter However,  the axioms describe the relationships between the predicates and the variables, and do not involve any consideration of time or sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
