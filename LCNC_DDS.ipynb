{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import tensorflow as tf\n",
    "\n",
    "dataset_path = 'C:\\\\work_ssd\\\\DDS_Paper\\\\data\\\\DDS_Data_SEU'\n",
    "\n",
    "PGB_path = 'C:\\\\work_ssd\\\\DDS_Paper\\\\data\\\\DDS_Data_SEU\\\\PGB'\n",
    "RGB_path = 'C:\\\\work_ssd\\\\DDS_Paper\\\\data\\\\DDS_Data_SEU\\\\RGB'\n",
    "\n",
    "# Specify the CSV file path\n",
    "csv_file = 'C:\\\\work_ssd\\\\DDS_Paper\\\\data\\\\data_robust.csv'\n",
    "\n",
    "np.random.seed(45)\n",
    "\n",
    "# Set the chunk size for reading the CSV\n",
    "chunk_size = 100000  # Adjust the chunk size according to your memory limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fault(file_name):\n",
    "    match = re.search(r'\\d+', file_name)\n",
    "    if match:\n",
    "        return int(match.group(0)[0])  # Extract the first digit\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def process_files_to_csv(data_folders, output_file):\n",
    "    # Check if the file already exists\n",
    "    if os.path.isfile(output_file):\n",
    "        print(f\"File {output_file} already exists. Skipping processing.\")\n",
    "        return\n",
    "\n",
    "    total_files = sum([len(files) for data_folder in data_folders for r, d, files in os.walk(data_folder)])\n",
    "\n",
    "    with open(output_file, 'w', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow(['Channel1', 'Channel2', 'Channel3', 'Channel4', 'Channel5', 'Channel6', 'Channel7', 'Channel8', 'Speed', 'Type', 'Fault'])\n",
    "\n",
    "        with tqdm(total=total_files, desc=\"Processing files\", unit=\"file\") as pbar:\n",
    "            for data_folder in data_folders:\n",
    "                for root, dirs, files in os.walk(data_folder):\n",
    "                    if '.ipynb_checkpoints' in root:\n",
    "                        continue  # Skip .ipynb_checkpoints folders\n",
    "                    for file in files:\n",
    "                        if file.endswith('.txt'):\n",
    "                            file_path = os.path.join(root, file)\n",
    "                            path_parts = file_path.split('\\\\')\n",
    "                            variable_speed = 'Variable_speed' in file_path\n",
    "                            type_index = -4 if variable_speed else -3\n",
    "                            type = path_parts[type_index] if path_parts[type_index] in ['PGB', 'RGB'] else None\n",
    "                            if type is not None:\n",
    "                                speed_index = -3 if variable_speed else -2\n",
    "                                speed = path_parts[speed_index]\n",
    "                                fault = extract_fault(file)\n",
    "\n",
    "                                data = pd.read_csv(file_path, sep='\\t', encoding='ISO-8859-1')\n",
    "                                reshaped_data = data.values[:, :]\n",
    "\n",
    "                                for row_data in tqdm(reshaped_data, desc=\"Processing rows\", unit=\"row\", leave=False):\n",
    "                                    row = row_data.tolist() + [speed, type, fault]\n",
    "                                    csv_writer.writerow(row)\n",
    "                            pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File C:\\work_ssd\\DDS_Paper\\data\\data_robust.csv already exists. Skipping processing.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "process_files_to_csv([PGB_path, RGB_path], csv_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV: 7267chunk [09:06, 13.31chunk/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fault: 0, Count: 80740352\n",
      "Fault: 1, Count: 80740352\n",
      "Fault: 2, Count: 80740352\n",
      "Fault: 3, Count: 80740352\n",
      "Fault: 4, Count: 80740352\n",
      "Fault: 5, Count: 80740352\n",
      "Fault: 6, Count: 80740352\n",
      "Fault: 7, Count: 80740352\n",
      "Fault: 8, Count: 80740352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize a dictionary to store the fault counts\n",
    "fault_counts = {}\n",
    "\n",
    "# Iterate through the CSV file using chunksize\n",
    "with tqdm(total=1, unit='chunk', desc='Processing CSV') as pbar:\n",
    "    for chunk in pd.read_csv(csv_file, chunksize=chunk_size):\n",
    "        \n",
    "        #print(chunk)\n",
    "        # Assuming there is a column named 'fault' in the CSV representing the fault type\n",
    "        fault_column = 'Fault'\n",
    "\n",
    "        # Count the occurrences of each fault in the current chunk\n",
    "        fault_chunk_counts = chunk[fault_column].value_counts()\n",
    "\n",
    "        # Aggregate the counts with the overall fault_counts dictionary\n",
    "        for fault, count in fault_chunk_counts.items():\n",
    "            fault_counts[fault] = fault_counts.get(fault, 0) + count\n",
    "\n",
    "        pbar.update()\n",
    "\n",
    "# Print the fault counts\n",
    "for fault, count in fault_counts.items():\n",
    "    print(f\"Fault: {fault}, Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV: 7267chunk [09:46, 12.39chunk/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed Counts:\n",
      "Speed: 20_0, Count: 75497472\n",
      "Speed: 30_0, Count: 75497472\n",
      "Speed: 30_1, Count: 75497472\n",
      "Speed: 30_2, Count: 75497472\n",
      "Speed: 30_3, Count: 75497472\n",
      "Speed: 30_4, Count: 75497472\n",
      "Speed: 30_5, Count: 75497472\n",
      "Speed: 40_0, Count: 75497472\n",
      "Speed: 50_0, Count: 75497472\n",
      "Speed: Variable_speed, Count: 47185920\n",
      "Type Counts:\n",
      "Type: PGB, Count: 363331584\n",
      "Type: RGB, Count: 363331584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize dictionaries to store the counts\n",
    "speed_counts = {}\n",
    "type_counts = {}\n",
    "\n",
    "# Iterate through the CSV file using chunksize\n",
    "with tqdm(total=1, unit='chunk', desc='Processing CSV') as pbar:\n",
    "    for chunk in pd.read_csv(csv_file, chunksize=chunk_size):\n",
    "        # Assuming there is a column named 'Speed' in the CSV representing the speed values\n",
    "        \n",
    "        speed_column = 'Speed'\n",
    "\n",
    "        # Count the occurrences of each speed value in the current chunk\n",
    "        speed_chunk_counts = chunk[speed_column].value_counts()\n",
    "\n",
    "        # Aggregate the counts with the overall speed_counts dictionary\n",
    "        for speed, count in speed_chunk_counts.items():\n",
    "            speed_counts[speed] = speed_counts.get(speed, 0) + count\n",
    "\n",
    "        # Assuming there is a column named 'Type' in the CSV representing the types\n",
    "        type_column = 'Type'\n",
    "\n",
    "        # Count the occurrences of each type in the current chunk\n",
    "        type_chunk_counts = chunk[type_column].value_counts()\n",
    "\n",
    "        # Aggregate the counts with the overall type_counts dictionary\n",
    "        for typ, count in type_chunk_counts.items():\n",
    "            type_counts[typ] = type_counts.get(typ, 0) + count\n",
    "\n",
    "        pbar.update()\n",
    "\n",
    "# Print the speed counts\n",
    "print(\"Speed Counts:\")\n",
    "for speed, count in speed_counts.items():\n",
    "    print(f\"Speed: {speed}, Count: {count}\")\n",
    "\n",
    "# Print the type counts\n",
    "print(\"Type Counts:\")\n",
    "for typ, count in type_counts.items():\n",
    "    print(f\"Type: {typ}, Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the numeric and categorical features\n",
    "numerical_features = ['Channel1', 'Channel2', 'Channel3', 'Channel4', 'Channel5', 'Channel6', 'Channel7', 'Channel8']\n",
    "categorical_features = ['Speed', 'Type']\n",
    "\n",
    "# initialize the scalers\n",
    "scaler = StandardScaler()\n",
    "encoder = OneHotEncoder(categories=[['20_0','30_0','30_1','30_2', '30_3','30_4','30_5','40_0','50_0', 'Variable_speed'], ['PGB', 'RGB']])  # specify the unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', scaler, numerical_features),\n",
    "        ('cat', encoder, categorical_features)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the total size of the file in bytes\n",
    "total_size = os.path.getsize(csv_file)\n",
    "\n",
    "# define the number of lines to read for the estimate\n",
    "sample_lines = 1000\n",
    "\n",
    "# read the first few lines of the file to get the average bytes per line\n",
    "with open(csv_file, 'r') as file:\n",
    "    lines = [next(file) for _ in range(sample_lines)]\n",
    "avg_bytes_per_line = sum(len(line) for line in lines) / sample_lines\n",
    "\n",
    "# estimate the total number of lines in the file\n",
    "estimated_lines = total_size // avg_bytes_per_line\n",
    "\n",
    "# calculate the estimated total number of chunks\n",
    "total_chunks = estimated_lines // chunk_size\n",
    "if estimated_lines % chunk_size != 0:\n",
    "    total_chunks += 1\n",
    "    \n",
    "# generate a boolean mask for the entire dataset\n",
    "mask = np.random.rand(int(estimated_lines)) < 0.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting Scaler and Encoder:  98%|█████████▊| 7267/7398.0 [10:46<00:11, 11.24chunk/s]\n"
     ]
    }
   ],
   "source": [
    "# initialize a progress bar\n",
    "progress_bar = tqdm(total=total_chunks, desc=\"Fitting Scaler and Encoder\", unit=\"chunk\")\n",
    "\n",
    "# fit the scaler and encoder incrementally\n",
    "for chunk in pd.read_csv(csv_file, chunksize=chunk_size):\n",
    "    chunk_mask = np.random.rand(len(chunk)) < 0.8\n",
    "    train_chunk = chunk[chunk_mask]\n",
    "    scaler.partial_fit(train_chunk[numerical_features])\n",
    "    encoder.fit(train_chunk[categorical_features])\n",
    "    progress_bar.update()\n",
    "\n",
    "# close the progress bar\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting preprocessor:  98%|█████████▊| 7267/7398.0 [14:23<00:15,  8.41chunk/s]\n"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(total=total_chunks, desc=\"Fitting preprocessor\", unit=\"chunk\")\n",
    "\n",
    "# fit the preprocessor on the entire training data\n",
    "for chunk in pd.read_csv(csv_file, chunksize=chunk_size):\n",
    "    chunk_mask = np.random.rand(len(chunk)) < 0.8\n",
    "    train_chunk = chunk[chunk_mask]\n",
    "    preprocessor.fit(train_chunk)\n",
    "    progress_bar.update()\n",
    "# close the progress bar\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define a generator function for the training set\n",
    "# def train_generator():\n",
    "#     for chunk in pd.read_csv(csv_file, chunksize=chunk_size):\n",
    "#         chunk_mask = np.random.rand(len(chunk)) < 0.8\n",
    "#         train_chunk = chunk[chunk_mask]\n",
    "#         if len(train_chunk) == chunk_size:  # only yield full chunks\n",
    "#             # perform the transformations\n",
    "#             transformed_chunk = preprocessor.transform(train_chunk)\n",
    "#             yield transformed_chunk\n",
    "\n",
    "# # define a generator function for the validation set\n",
    "# def val_generator():\n",
    "#     for chunk in pd.read_csv(csv_file, chunksize=chunk_size):\n",
    "#         chunk_mask = np.random.rand(len(chunk)) < 0.8\n",
    "#         val_chunk = chunk[~chunk_mask]\n",
    "#         if len(val_chunk) == chunk_size:  # only yield full chunks\n",
    "#             # perform the transformations\n",
    "#             transformed_chunk = preprocessor.transform(val_chunk)\n",
    "#             yield transformed_chunk \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator():\n",
    "    for chunk in pd.read_csv(csv_file, chunksize=chunk_size):\n",
    "        chunk_mask = np.random.rand(len(chunk)) < 0.8\n",
    "        train_chunk = chunk[chunk_mask]\n",
    "        # perform the transformations\n",
    "        transformed_chunk = preprocessor.transform(train_chunk)\n",
    "        yield transformed_chunk\n",
    "\n",
    "def val_generator():\n",
    "    for chunk in pd.read_csv(csv_file, chunksize=chunk_size):\n",
    "        chunk_mask = np.random.rand(len(chunk)) < 0.8\n",
    "        val_chunk = chunk[~chunk_mask]\n",
    "        # perform the transformations\n",
    "        transformed_chunk = preprocessor.transform(val_chunk)\n",
    "        yield transformed_chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tf.data.Datasets for training and validation\n",
    "train_dataset = tf.data.Dataset.from_generator(train_generator, output_signature=(\n",
    "    tf.TensorSpec(shape=(None, 20), dtype=tf.float32)))\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_generator(val_generator, output_signature=(\n",
    "    tf.TensorSpec(shape=(None, 20), dtype=tf.float32)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_1_device_/job:localhost/replica:0/task:0/device:CPU:0}} Cannot batch tensors with different shapes in component 0. First element had shape [79908,20] and element 1 had shape [79913,20]. [Op:IteratorGetNext]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# train the model on the training dataset\u001b[39;00m\n\u001b[0;32m      2\u001b[0m last_train_batch \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfor\u001b[39;00m train_batch \u001b[39min\u001b[39;00m train_dataset\u001b[39m.\u001b[39mbatch(\u001b[39m2\u001b[39m):\n\u001b[0;32m      4\u001b[0m     \u001b[39m# perform a training step here\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[39mprint\u001b[39m(train_batch)\n\u001b[0;32m      6\u001b[0m     last_train_batch \u001b[39m=\u001b[39m train_batch\n",
      "File \u001b[1;32mc:\\Users\\edy45\\miniconda3\\envs\\work_stable\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:797\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    796\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 797\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_internal()\n\u001b[0;32m    798\u001b[0m   \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mOutOfRangeError:\n\u001b[0;32m    799\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\edy45\\miniconda3\\envs\\work_stable\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:780\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[39m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[0;32m    778\u001b[0m \u001b[39m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[0;32m    779\u001b[0m \u001b[39mwith\u001b[39;00m context\u001b[39m.\u001b[39mexecution_mode(context\u001b[39m.\u001b[39mSYNC):\n\u001b[1;32m--> 780\u001b[0m   ret \u001b[39m=\u001b[39m gen_dataset_ops\u001b[39m.\u001b[39;49miterator_get_next(\n\u001b[0;32m    781\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator_resource,\n\u001b[0;32m    782\u001b[0m       output_types\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_output_types,\n\u001b[0;32m    783\u001b[0m       output_shapes\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_output_shapes)\n\u001b[0;32m    785\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    786\u001b[0m     \u001b[39m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_element_spec\u001b[39m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\edy45\\miniconda3\\envs\\work_stable\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3043\u001b[0m, in \u001b[0;36miterator_get_next\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   3041\u001b[0m   \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   3042\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m-> 3043\u001b[0m   _ops\u001b[39m.\u001b[39;49mraise_from_not_ok_status(e, name)\n\u001b[0;32m   3044\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_FallbackException:\n\u001b[0;32m   3045\u001b[0m   \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\edy45\\miniconda3\\envs\\work_stable\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:7262\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7260\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[0;32m   7261\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 7262\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_1_device_/job:localhost/replica:0/task:0/device:CPU:0}} Cannot batch tensors with different shapes in component 0. First element had shape [79908,20] and element 1 had shape [79913,20]. [Op:IteratorGetNext]"
     ]
    }
   ],
   "source": [
    "# train the model on the training dataset\n",
    "last_train_batch = None\n",
    "for train_batch in train_dataset.batch(2):\n",
    "    # perform a training step here\n",
    "    print(train_batch)\n",
    "    last_train_batch = train_batch\n",
    "    break\n",
    "\n",
    "if last_train_batch is not None:\n",
    "    print(last_train_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(last_train_batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
