
# DDS Paper-LTN Integration

![plot_highdef](https://github.com/eduardhogea/DDS_Paper/assets/72266259/5d4a702e-283d-4e88-8da0-ab22e0ed24f8)


This project integrates DDS Paper with LTN. It includes code for creating sequences and training models.

## Installation

First, clone the repository and navigate to the project directory:

```bash
git clone https://github.com/yourusername/DDS_Paper.git
cd DDS_Paper
```

Then, install the required dependencies:

```bash
pip install -r requirements.txt
```

## Usage

You can run the script with the following commands:

- To only create sequences:
  ```bash
  python code/run.py --create-sequences
  ```

- To only train the model:
  ```bash
  python code/run.py --train-model
  ```

- To perform both tasks:
  ```bash
  python code/run.py --create-sequences --train-model
  ```

## Configuration

The configuration files for this project are located in the `config/` directory. These files allow you to customize various parameters and settings for creating sequences and training models. The primary configuration settings are defined in `config.py`.

### Key Configuration Parameters

- **Paths**
  - `BASE_DIR`: Base directory of the project.
  - `config_path`: Path to the main configuration file.
  - `dataset_path`: Path to the dataset directory.
  - `PGB_path`: Path to the PGB dataset.
  - `RGB_path`: Path to the RGB dataset.
  - `csv_file`: Path to the main CSV data merged files.
  - `preprocessor_file`: Path to the preprocessor file.
  - `train_path`: Path to the training data CSV file.
  - `val_path`: Path to the validation data CSV file.
  - `csv_directory`: Directory containing CSV files.
  - `data_root_folder`: Root folder for data.
  - `sequences_directory`: Directory for sequence data.
  - `model_save_directory`: Directory to save trained models.
  - `model_path`: Path to the saved model file.
  - `results_path`: Path to the results CSV file.
  - `results_path_ltn`: Path to the results directory for LTN.
  - `processed_file_tracker`: Path to the file tracking processed data.

- **Data Processing**
  - `chunk_size`: Size of data chunks for processing.
  - `sequence_length`: Length of sequences to be created.
  - `num_features`: Number of features in the dataset.
  - `processed_bases`: Set of base names to avoid redundancy.

- **Model Training Parameters**
  - `batch_size`: Batch size for training.
  - `epochs`: Number of epochs for training.
  - `patience`: Patience parameter for early stopping.
  - `learning_rate`: Learning rate for the optimizer.
  - `lr_ltn`: Learning rate for LTN.
  - `n_splits`: Number of splits for cross-validation.
  - `reg_value`: Regularization value.
  - `num_train_samples`: Number of training samples per fault type.
  - `num_test_samples`: Number of test samples per fault type.
  - `reg_type`: Type of regularization (e.g., l1, l2).
  - `n_samples`: Number of samples to consider fo feature importance.
  - `num_classes`: Number of output classes.
  - `buffer_size`: Buffer size for data loading.
  - `ltn_batch`: Batch size for LTN.
  - `S`: Number of speeds to consider.

- **Seed for Reproducibility**
  - `np.random.seed(42)`: Seed value for reproducibility.

## Project Structure

- `.github/`: GitHub-related files and workflows.
- `code/`: Contains the main code for running the scripts.
- `config/`: Configuration files for the project.
- `extra_code/`: Additional code that might be useful.
- `plots/`: Directory to store plots generated by the scripts.
- `requirements.txt`: List of dependencies for the project.
- `README.md`: This file.


## Results 

