{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9c5331c-6eb1-4042-882e-743891f6bd80",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa751921-ff13-4405-8738-7b8e126d7cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "abe56242-23bf-490e-9a53-9a7613ae01f4",
   "metadata": {},
   "source": [
    "DDS_Data_SEU/\n",
    "│\n",
    "├── RGB/\n",
    "│   ├── 50_0/\n",
    "│   ├── 40_0/\n",
    "│   ├── 30_4/\n",
    "│   ├── ... (other configurations)\n",
    "│   └── Variable_speed/\n",
    "│       ├── Experiment_1/\n",
    "│       ├── Experiment_2/\n",
    "│       ├── ... (other experiments)\n",
    "│\n",
    "└── PGB/\n",
    "    ├── 50_0/\n",
    "    ├── 40_0/\n",
    "    ├── 30_4/\n",
    "    ├── ... (other configurations)\n",
    "    └── Variable_speed/\n",
    "        ├── Experiment1/\n",
    "        ├── Experiment6/\n",
    "        ├── ... (other experiments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460fa065-bba1-452b-85e0-6bdf0f96b36d",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce25127-6c6f-442f-acac-390529e6a250",
   "metadata": {},
   "source": [
    "## Analyzing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d087e76-9909-46cf-bbf4-bdffbc775cfa",
   "metadata": {},
   "source": [
    "The following reads the data from the given folder structure, calculates descriptive statistics, and generates box plots, histograms, and a correlation matrix plot for each channel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f94283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_file(file_path):\n",
    "    \"\"\"\n",
    "    Load data from a given file path.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The path to the data file.\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: The loaded data as a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(file_path, sep='\\t')\n",
    "    return data\n",
    "\n",
    "def process_data_file(data_path, label):\n",
    "    \"\"\"\n",
    "    Process a data file and return the data as a NumPy array along with its label.\n",
    "    \n",
    "    Args:\n",
    "        data_path (str): The path to the data file.\n",
    "        label (str): The label associated with the data file.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: A tuple containing the data as a NumPy array and its label.\n",
    "    \"\"\"\n",
    "    data = load_data_file(data_path)\n",
    "    return data.values, label\n",
    "\n",
    "base_path = 'DDS_Data_SEU'\n",
    "categories = ['PGB', 'RGB']\n",
    "batch_size = 320\n",
    "data_files = []\n",
    "labels = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1546109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BatchLoader:\n",
    "#     \"\"\"\n",
    "#     A batch loader class to load and process data files in batches.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, data_files, labels, batch_size, mode='train', test_size=0.2, random_state=42, segment_size=1):\n",
    "#         \"\"\"\n",
    "#         Initialize the batch loader.\n",
    "        \n",
    "#         Args:\n",
    "#             data_files (list): A list of data file paths.\n",
    "#             labels (list): A list of labels associated with the data files.\n",
    "#             batch_size (int): The number of samples per batch.\n",
    "#             mode (str, optional): The mode of operation, either 'train' or 'val'. Defaults to 'train'.\n",
    "#             test_size (float, optional): The proportion of the dataset to include in the test split. Defaults to 0.2.\n",
    "#             random_state (int, optional): The seed used by the random number generator. Defaults to 42.\n",
    "#             segment_size (int, optional): The number of rows to select from each data file. Defaults to 1.\n",
    "#         \"\"\"\n",
    "#         self.batch_size = batch_size\n",
    "#         self.mode = mode\n",
    "#         self.segment_size = segment_size\n",
    "        \n",
    "#         # Train-val split\n",
    "#         train_files, val_files, train_labels, val_labels = train_test_split(data_files, labels, test_size=test_size, random_state=random_state)\n",
    "\n",
    "#         if mode == 'train':\n",
    "#             self.file_list = train_files\n",
    "#             self.label_list = train_labels\n",
    "#         else:\n",
    "#             self.file_list = val_files\n",
    "#             self.label_list = val_labels\n",
    "\n",
    "#         self.num_batches = len(self.file_list) // self.batch_size\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         \"\"\"\n",
    "#         Get a batch of data and labels.\n",
    "        \n",
    "#         Args:\n",
    "#             index (int): The index of the batch to fetch.\n",
    "            \n",
    "#         Returns:\n",
    "#             tuple: A tuple containing the batch data as a NumPy array and the corresponding labels.\n",
    "#         \"\"\"\n",
    "#         batch_files = self.file_list[index*self.batch_size:(index+1)*self.batch_size]\n",
    "#         batch_labels = self.label_list[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "#         # Read data from files and stack them in a NumPy array\n",
    "#         batch_data = [pd.read_csv(file, sep='\\t', encoding='ISO-8859-1').values[:self.segment_size, :] for file in tqdm(batch_files, desc=\"Reading data from files\")]\n",
    "\n",
    "#         # Reshape the data\n",
    "#         reshaped_data = [data[np.newaxis, :, :] for data in batch_data]\n",
    "#         stacked_data = np.concatenate(reshaped_data, axis=1)\n",
    "\n",
    "#         # Extract fault state from the file name\n",
    "#         fault_states = [os.path.basename(file).split('_')[0] for file in batch_files]\n",
    "\n",
    "#         return stacked_data, fault_states\n",
    "\n",
    "#     def __len__(self):\n",
    "#         \"\"\"\n",
    "#         Get the total number of batches.\n",
    "        \n",
    "#         Returns:\n",
    "#             int: The total number of batches.\n",
    "#         \"\"\"\n",
    "#         return self.num_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f143c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for category in categories:\n",
    "#     category_path = os.path.join(base_path, category)\n",
    "\n",
    "#     for root, dirs, files in os.walk(category_path):\n",
    "#         for file in files:\n",
    "#             if file.endswith('.txt'):\n",
    "#                 file_path = os.path.join(root, file)\n",
    "#                 data_files.append(file_path)\n",
    "#                 labels.append(category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21b20d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'DDS_Data_SEU'\n",
    "categories = ['PGB', 'RGB']\n",
    "\n",
    "data_files = []\n",
    "labels = []\n",
    "\n",
    "for category in categories:\n",
    "    category_path = os.path.join(base_path, category)\n",
    "\n",
    "    for root, dirs, files in os.walk(category_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                data_files.append(file_path)\n",
    "                labels.append(category)\n",
    "\n",
    "output_file = 'output.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc47cb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty DataFrame with the desired column names\n",
    "output_data = pd.DataFrame(columns=['Channel1', 'Channel2', 'Channel3', 'Channel4', 'Channel5', 'Channel6', 'Channel7', 'Channel8', 'Fault'])\n",
    "\n",
    "# Iterate over the data files and append the data to the output DataFrame\n",
    "for data_file, label in tqdm(zip(data_files, labels), total=len(data_files)):\n",
    "    # Read data from the file\n",
    "    data = pd.read_csv(data_file, sep='\\t', header=None)\n",
    "\n",
    "    # Add the fault type to the data\n",
    "    data['Fault'] = label\n",
    "\n",
    "    # Append the data to the output DataFrame\n",
    "    output_data = output_data.append(data, ignore_index=True)\n",
    "\n",
    "    # Save the updated DataFrame to the output CSV file\n",
    "    output_data.to_csv(output_file, index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0383175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set the segment size to 1\n",
    "# segment_size = 1\n",
    "\n",
    "# train_loader = BatchLoader(train_files, train_labels, batch_size, segment_size=segment_size)\n",
    "# val_loader = BatchLoader(val_files, val_labels, batch_size, segment_size=segment_size)\n",
    "\n",
    "# # Get the first batch of samples\n",
    "# first_batch_samples, first_batch_labels = train_loader[0]\n",
    "\n",
    "# # Get the first sample and its label from the first batch\n",
    "# first_sample, first_label = first_batch_samples[0], first_batch_labels[0]\n",
    "\n",
    "# print(\"First sample:\")\n",
    "# print(first_sample)\n",
    "\n",
    "# print(\"\\nFirst label:\", first_label)\n",
    "# print(\"First sample shape:\", first_sample.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35618df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "base_path = 'DDS_Data_SEU'\n",
    "category = 'PGB'\n",
    "data_files = []\n",
    "\n",
    "# Get all the data files in the PGB folder\n",
    "category_path = os.path.join(base_path, category)\n",
    "for root, dirs, files in os.walk(category_path):\n",
    "    for file in files:\n",
    "        if file.endswith('.txt'):\n",
    "            file_path = os.path.join(root, file)\n",
    "            data_files.append(file_path)\n",
    "\n",
    "# Create an empty DataFrame to store the channel values and fault types\n",
    "column_names = ['Channel1', 'Channel2', 'Channel3', 'Channel4', 'Channel5', 'Channel6', 'Channel7', 'Channel8', 'Fault']\n",
    "all_data = pd.DataFrame(columns=column_names)\n",
    "\n",
    "# Iterate through the data files and process them\n",
    "for file in tqdm(data_files, desc='Processing files'):\n",
    "    # Read the data from the file\n",
    "    data = pd.read_csv(file, sep='\\t', encoding='ISO-8859-1', header=None)\n",
    "    \n",
    "    # Extract the fault state from the file name\n",
    "    fault_state = os.path.basename(file).split('_')[0]\n",
    "    \n",
    "    # Add a new column with the fault state\n",
    "    data['Fault'] = fault_state\n",
    "    \n",
    "    # Rename the columns\n",
    "    data.columns = column_names\n",
    "    \n",
    "    # Append the processed data to the existing DataFrame\n",
    "    all_data = all_data.append(data)\n",
    "    \n",
    "    # Save the current DataFrame to the CSV file\n",
    "    all_data.to_csv('all_data.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e1b3dca6",
   "metadata": {},
   "source": [
    "Well I want to take all the data from all the files located in the PGB folder and add them to the csv with channels and fault types. Save the csv file and add values to the csv file every new file is iterated through, so it does not exceed the memory limit."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c6575cd6",
   "metadata": {},
   "source": [
    "# Process the intermediate file in chunks and save to the output file\n",
    "process_data_in_chunks(intermediate_file, output_file)\n",
    "\n",
    "# Read the output file as a DataFrame\n",
    "pgb_data_processed = pd.read_csv(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9690ab44-f797-4d27-99aa-fef1c17fecce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pgb_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ff1368-3faa-4334-95f5-3c913382f2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef018cf-dfbc-4997-ac68-bb74e0857dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder_path = \"DDS_Data_SEU\"\n",
    "# data = read_data_parallel(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b76338e-214e-4549-89d8-b5c9b37c5ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = data.mean()\n",
    "median = data.median()\n",
    "std_dev = data.std()\n",
    "variance = data.var()\n",
    "min_values = data.min()\n",
    "max_values = data.max()\n",
    "quantiles = data.quantile([0.25, 0.5, 0.75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf59e18-6a7a-4e1e-8761-96149d10bb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(15, 10))\n",
    "ax = sns.boxplot(data=data.iloc[:, :8], palette=\"Set3\")\n",
    "ax.set_title(\"Box Plot of Channels\")\n",
    "ax.set_xlabel(\"Channels\")\n",
    "ax.set_ylabel(\"Values\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680167ef-354e-4fb7-b99c-9a1845b5f30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[:, :8].hist(bins=50, figsize=(15, 10), grid=False, color='#86bf91', zorder=2, rwidth=0.9)\n",
    "plt.suptitle(\"Histogram of Channels\")\n",
    "plt.xlabel(\"Values\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a801bd3-c1ac-4fec-b0c7-a96ffc2a32be",
   "metadata": {},
   "source": [
    "# LTN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024a817d-eac9-4e1b-9e2f-e699ac82ab4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
