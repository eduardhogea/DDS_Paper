{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-27 13:48:47.208283: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-27 13:48:47.280140: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-27 13:48:47.280210: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-27 13:48:47.280279: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-27 13:48:47.297057: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-27 13:48:47.299182: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-27 13:48:49.424186: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import csv\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "\n",
    "# Third-party library imports\n",
    "import config\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from keras import layers, models, optimizers, callbacks, regularizers, utils\n",
    "from keras.callbacks import EarlyStopping, LearningRateScheduler, ModelCheckpoint, TensorBoard\n",
    "from keras.layers import Dense, Dropout, LSTM, Input\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from tqdm import tqdm\n",
    "from numpy import mean\n",
    "from collections import Counter\n",
    "import ltn\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dropout, Dense\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "\n",
    "# Configurations\n",
    "model_path = config.model_path\n",
    "dataset_path = config.dataset_path\n",
    "PGB_path = config.PGB_path\n",
    "RGB_path = config.RGB_path\n",
    "csv_file = config.csv_file\n",
    "preprocessor_file = config.preprocessor_file\n",
    "train_path = config.train_path\n",
    "val_path = config.val_path\n",
    "chunk_size = config.chunk_size\n",
    "csv_directory = config.csv_directory\n",
    "data_root_folder = config.data_root_folder\n",
    "sequence_length = config.sequence_length\n",
    "sequences_directory = config.sequences_directory\n",
    "num_features = config.num_features\n",
    "processed_bases = config.processed_bases\n",
    "batch_size = config.batch_size\n",
    "epochs = config.epochs\n",
    "patience = config.patience\n",
    "learning_rate = config.learning_rate\n",
    "n_splits = config.n_splits\n",
    "model_save_directory = config.model_save_directory\n",
    "reg_value = config.reg_value\n",
    "num_train_samples = config.num_train_samples\n",
    "num_test_samples = config.num_test_samples\n",
    "reg_type = config.reg_type\n",
    "n_samples = config.n_samples\n",
    "num_classes = config.num_classes\n",
    "\n",
    "def reset_random_seeds(seed_value=42):\n",
    "    tf.random.set_seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "\n",
    "# Call this function at the beginning of your script\n",
    "reset_random_seeds()\n",
    "\n",
    "Not = ltn.Wrapper_Connective(ltn.fuzzy_ops.Not_Std())\n",
    "And = ltn.Wrapper_Connective(ltn.fuzzy_ops.And_Prod())\n",
    "Or = ltn.Wrapper_Connective(ltn.fuzzy_ops.Or_ProbSum())\n",
    "Implies = ltn.Wrapper_Connective(ltn.fuzzy_ops.Implies_Reichenbach())\n",
    "Forall = ltn.Wrapper_Quantifier(ltn.fuzzy_ops.Aggreg_pMeanError(p=2),semantics=\"forall\")\n",
    "\n",
    "class_0 = ltn.Constant(0, trainable=False)\n",
    "class_1 = ltn.Constant(1, trainable=False)\n",
    "class_2 = ltn.Constant(2, trainable=False)\n",
    "class_3 = ltn.Constant(3, trainable=False)\n",
    "class_4 = ltn.Constant(4, trainable=False)\n",
    "class_5 = ltn.Constant(5, trainable=False)\n",
    "class_6 = ltn.Constant(6, trainable=False)\n",
    "class_7 = ltn.Constant(7, trainable=False)\n",
    "class_8 = ltn.Constant(8, trainable=False)\n",
    "\n",
    "formula_aggregator = ltn.Wrapper_Formula_Aggregator(ltn.fuzzy_ops.Aggreg_pMeanError(p=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fault(file_name):\n",
    "    fault_mapping = {\n",
    "        '0Health': 'HEA', '1Chipped': 'CTF', '2Miss': 'MTF', \n",
    "        '3Root': 'RCF', '4Surface': 'SWF', '5Ball': 'BWF', \n",
    "        '6Combination': 'CWF', '7Inner': 'IRF', '8Outer': 'ORF'\n",
    "    }\n",
    "    for key, value in fault_mapping.items():\n",
    "        if key in file_name:\n",
    "            return value\n",
    "    return None\n",
    "\n",
    "def make_csv_writer(csv_file):\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow(['Channel1', 'Channel2', 'Channel3', 'Channel4', 'Channel5', 'Channel6', 'Channel7', 'Channel8', 'Fault'])\n",
    "    return csv_writer\n",
    "\n",
    "def generate_csv(output_directory, root_path, speed, experiment, files, num_train_samples, num_test_samples):\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    train_filename_suffix = f\"{speed}_{experiment}_train\" if experiment else f\"{speed}_train\"\n",
    "    test_filename_suffix = f\"{speed}_{experiment}_test\" if experiment else f\"{speed}_test\"\n",
    "    \n",
    "    train_output_file_path = os.path.join(output_directory, f\"PGB_{train_filename_suffix}.csv\")\n",
    "    test_output_file_path = os.path.join(output_directory, f\"PGB_{test_filename_suffix}.csv\")\n",
    "    \n",
    "    with open(train_output_file_path, 'w', newline='', encoding='utf-8') as train_csvfile, \\\n",
    "        open(test_output_file_path, 'w', newline='', encoding='utf-8') as test_csvfile:\n",
    "        train_csv_writer = make_csv_writer(train_csvfile)\n",
    "        test_csv_writer = make_csv_writer(test_csvfile)\n",
    "        \n",
    "        for file in tqdm(files, desc=f\"Processing {speed} {experiment}\", unit=\"file\"):\n",
    "            fault_type = extract_fault(file)\n",
    "            file_path = os.path.join(root_path, file)\n",
    "            \n",
    "            total_rows = num_train_samples + num_test_samples\n",
    "            data = pd.read_csv(file_path, sep='\\t', header=None, encoding='ISO-8859-1', skiprows=1, nrows=total_rows)\n",
    "            train_samples, test_samples = data.iloc[:num_train_samples, :], data.iloc[num_train_samples:total_rows, :]\n",
    "            \n",
    "            for index, row in train_samples.iterrows():\n",
    "                train_csv_writer.writerow(row[:8].tolist() + [fault_type])\n",
    "            \n",
    "            for index, row in test_samples.iterrows():\n",
    "                test_csv_writer.writerow(row[:8].tolist() + [fault_type])\n",
    "\n",
    "def process_pgb_data(data_root_folder, csv_directory, num_train_samples, num_test_samples):\n",
    "    for root, dirs, files in os.walk(data_root_folder):\n",
    "        parts = root.split(os.sep)\n",
    "        if 'Variable_speed' in parts:\n",
    "            speed = \"Variable_speed\"\n",
    "            experiment_dir = parts[-1]  # Get the last part as the experiment name\n",
    "            exp_files = [f for f in os.listdir(root) if f.endswith('.txt')]\n",
    "            generate_csv(csv_directory, root, speed, experiment_dir, exp_files, num_train_samples, num_test_samples)\n",
    "        elif 'PGB' in parts and files:\n",
    "            speed = parts[-1]  # Last part of 'root' is the speed directory\n",
    "            generate_csv(csv_directory, root, speed, '', files, num_train_samples, num_test_samples)\n",
    "            \n",
    "            \n",
    "def overview_csv_files(directory):\n",
    "    data = []\n",
    "    all_faults = set()\n",
    "\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Check if the CSV is empty (aside from the header)\n",
    "            if df.shape[0] == 0:\n",
    "                # Delete the empty CSV file\n",
    "                os.remove(file_path)\n",
    "                print(f\"Deleted empty file: {file_path}\")\n",
    "                continue  # Skip further processing for this file\n",
    "\n",
    "            num_samples = len(df)\n",
    "            fault_distribution = Counter(df['Fault'])\n",
    "            all_faults.update(fault_distribution.keys())\n",
    "            data.append({'File Name': file, 'Number of Samples': num_samples, **fault_distribution})\n",
    "\n",
    "    if not data:  # If no data has been gathered, exit the function\n",
    "        print(\"No data found.\")\n",
    "        return\n",
    "\n",
    "    overview_df = pd.DataFrame(data)\n",
    "    for fault in all_faults:\n",
    "        if fault not in overview_df.columns:\n",
    "            overview_df[fault] = 0\n",
    "\n",
    "    cols = ['File Name', 'Number of Samples'] + sorted(all_faults)\n",
    "    overview_df = overview_df[cols]\n",
    "    overview_df.fillna(0, inplace=True)\n",
    "    overview_df.loc[:, 'Number of Samples':] = overview_df.loc[:, 'Number of Samples':].astype(int)\n",
    "\n",
    "    overview_df = overview_df.sort_values(by='File Name')\n",
    "    print(overview_df.to_string(index=False))\n",
    "    \n",
    "def load_and_scale_data(csv_path, scaler=None, save_scaler_path=None):\n",
    "    \"\"\"\n",
    "    Loads data from a CSV file, scales the features (excluding the 'Fault' column), \n",
    "    and returns the scaled DataFrame. Optionally saves the scaler model.\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    data = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Separate features and target\n",
    "    features = data.columns[:-1]  # Assuming the last column is the target\n",
    "    X = data[features]\n",
    "    y = data['Fault']\n",
    "\n",
    "    # Apply scaling\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        if save_scaler_path:\n",
    "            joblib.dump(scaler, save_scaler_path)\n",
    "    else:\n",
    "        X_scaled = scaler.transform(X)\n",
    "    \n",
    "    # Combine scaled features with target\n",
    "    scaled_df = pd.DataFrame(X_scaled, columns=features)\n",
    "    scaled_df['Fault'] = y\n",
    "    \n",
    "    return scaled_df\n",
    "\n",
    "def create_sequences(df, sequence_length):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    fault_types = df['Fault'].unique()\n",
    "\n",
    "    for fault in fault_types:\n",
    "        df_fault = df[df['Fault'] == fault]\n",
    "        X = df_fault.drop('Fault', axis=1).values\n",
    "        y = df_fault['Fault'].iloc[0]  # Updated to use iloc for consistency\n",
    "        \n",
    "        for i in range(len(df_fault) - sequence_length + 1):\n",
    "            sequences.append(X[i:i+sequence_length])\n",
    "            labels.append(fault)  # Keep the fault type as is\n",
    "    \n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "\n",
    "def save_sequences(input_directory, output_directory, sequence_length):\n",
    "    \"\"\"\n",
    "    Generates sequences and saves them as NumPy files, one for sequences and one for labels.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_directory: The directory with the original, scaled data files.\n",
    "    - output_directory: The directory where the NumPy sequence files will be saved.\n",
    "    - sequence_length: The number of consecutive samples in each sequence.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    for file_name in tqdm(os.listdir(input_directory), desc=\"Generating sequences\"):\n",
    "        if file_name.endswith('.csv'):\n",
    "            df = pd.read_csv(os.path.join(input_directory, file_name))\n",
    "            sequences, labels = create_sequences(df, sequence_length)\n",
    "            \n",
    "            # File names for sequences and labels\n",
    "            base_name = os.path.splitext(file_name)[0]\n",
    "            sequences_file_path = os.path.join(output_directory, f\"{base_name}_sequences.npy\")\n",
    "            labels_file_path = os.path.join(output_directory, f\"{base_name}_labels.npy\")\n",
    "            \n",
    "            # Save sequences and labels\n",
    "            np.save(sequences_file_path, sequences)\n",
    "            np.save(labels_file_path, labels)\n",
    "            \n",
    "def extract_speed_from_filename(file_name):\n",
    "    \"\"\"\n",
    "    Extracts the speed from the filename.\n",
    "    Returns the numeric speed for fixed speeds, or -1 for variable speeds.\n",
    "    \"\"\"\n",
    "    fixed_speed_match = re.search(r\"PGB_(\\d+)_\", file_name)\n",
    "    if fixed_speed_match:\n",
    "        return int(fixed_speed_match.group(1))\n",
    "    variable_speed_match = re.search(r\"Variable_speed\", file_name)\n",
    "    if variable_speed_match:\n",
    "        return -1  # Special value for variable speeds\n",
    "    return None\n",
    "\n",
    "def add_speed_feature_and_save(input_directory, output_directory, sequence_length):\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    for file_name in tqdm(os.listdir(input_directory), desc=\"Processing files\"):\n",
    "        if file_name.endswith('.csv'):\n",
    "            speed = extract_speed_from_filename(file_name)\n",
    "            df = pd.read_csv(os.path.join(input_directory, file_name))\n",
    "            df['Speed'] = speed  # Add speed as a new column\n",
    "            \n",
    "            sequences, labels = create_sequences(df, sequence_length)\n",
    "            \n",
    "            base_name = os.path.splitext(file_name)[0]\n",
    "            sequences_file_path = os.path.join(output_directory, f\"{base_name}_sequences.npy\")\n",
    "            labels_file_path = os.path.join(output_directory, f\"{base_name}_labels.npy\")\n",
    "            \n",
    "            np.save(sequences_file_path, sequences)\n",
    "            np.save(labels_file_path, labels)\n",
    "            \n",
    "def display_samples(sequences_file_path, labels_file_path, num_samples=1):\n",
    "    \"\"\"\n",
    "    Displays a specified number of samples from the sequences and labels .npy files.\n",
    "    \n",
    "    Parameters:\n",
    "    - sequences_file_path: Path to the .npy file containing sequences.\n",
    "    - labels_file_path: Path to the .npy file containing labels.\n",
    "    - num_samples: Number of samples to display. Default is 5.\n",
    "    \"\"\"\n",
    "    # Load the sequences and labels\n",
    "    sequences = np.load(sequences_file_path)\n",
    "    labels = np.load(labels_file_path)\n",
    "    \n",
    "    # Determine the number of samples to display (cannot exceed the length of the data)\n",
    "    num_samples = min(num_samples, len(sequences))\n",
    "    \n",
    "    # Display the specified number of samples\n",
    "    for i in range(num_samples):\n",
    "        print(f\"Sample {i+1}:\")\n",
    "        print(\"Sequence:\")\n",
    "        print(sequences[i])\n",
    "        print(\"Label:\")\n",
    "        print(labels[i])\n",
    "        print(\"-\" * 50)  # Separator for readability\n",
    "        \n",
    "def memmap_append_and_save(input_directory, output_directory, dataset_type, file_type):\n",
    "    # Update the output file path to indicate pickle format\n",
    "    output_file_path = os.path.join(output_directory, f\"{dataset_type}_merged_{file_type}.pkl\")\n",
    "\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    merged_data = None\n",
    "    current_size = 0\n",
    "\n",
    "    file_names = [fn for fn in os.listdir(input_directory) if fn.endswith(f'_{file_type}.npy') and dataset_type in fn]\n",
    "    for file_name in tqdm(file_names, desc=f\"Merging {dataset_type} {file_type}\"):\n",
    "        path = os.path.join(input_directory, file_name)\n",
    "        data = np.load(path)\n",
    "\n",
    "        # Adjust for both 1D and 2D+ data\n",
    "        new_shape = (current_size + data.shape[0],) + data.shape[1:] if len(data.shape) > 1 else (current_size + data.shape[0],)\n",
    "        if merged_data is None:\n",
    "            # Initially, directly use the loaded data\n",
    "            merged_data = data.copy()\n",
    "        else:\n",
    "            # Concatenate new data\n",
    "            merged_data = np.concatenate((merged_data, data), axis=0)\n",
    "        \n",
    "        current_size += data.shape[0]\n",
    "\n",
    "    # After processing all files, save the merged data using pickle\n",
    "    with open(output_file_path, 'wb') as f:\n",
    "        pickle.dump(merged_data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    print(f\"{dataset_type.capitalize()} {file_type} data merged and saved to {output_file_path} in pickle format\")\n",
    "\n",
    "def merge_npy_files_with_memmap_separated(input_directory, output_directory):\n",
    "    for dataset_type in ['train', 'test']:\n",
    "        for file_type in ['sequences', 'labels']:\n",
    "            memmap_append_and_save(input_directory, output_directory, dataset_type, file_type)\n",
    "            \n",
    "\n",
    "def calculate_class_accuracies(predictions, true_labels):\n",
    "    # Assuming predictions are probabilities, get the predicted class indices\n",
    "    pred_labels = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # true_labels are already integer labels, no need for np.argmax\n",
    "\n",
    "    # Initialize a dictionary to store accuracy for each class present in true_labels\n",
    "    class_accuracies = {}\n",
    "    \n",
    "    for class_index in np.unique(true_labels):  # Loop only through classes present in true_labels\n",
    "        class_mask = true_labels == class_index\n",
    "        \n",
    "        # Calculate accuracy for the current class\n",
    "        class_accuracies[class_index] = accuracy_score(true_labels[class_mask], pred_labels[class_mask])\n",
    "    \n",
    "    return class_accuracies\n",
    "\n",
    "\n",
    "def permutation_importance_per_class(model, X_val, y_val, n_repeats=10, n_samples=None):\n",
    "    n_samples = n_samples if n_samples is not None else X_val.shape[0]\n",
    "    random_indices = np.random.choice(X_val.shape[0], size=n_samples, replace=False)\n",
    "    X_val_subset = X_val[random_indices]\n",
    "    y_val_subset = y_val[random_indices]\n",
    "    \n",
    "    # Get baseline class-specific accuracies\n",
    "    baseline_predictions = model.predict(X_val_subset, verbose = 0)\n",
    "    baseline_class_accuracies = calculate_class_accuracies(baseline_predictions, y_val_subset)\n",
    "    \n",
    "    # Prepare storage for importances, using a dictionary to accommodate variable class presence\n",
    "    feature_importances = {class_index: np.zeros((X_val.shape[2], n_repeats)) for class_index in baseline_class_accuracies.keys()}\n",
    "    \n",
    "    for feature_index in tqdm(range(X_val.shape[2]), desc='Calculating Feature Importance'):\n",
    "        for n in range(n_repeats):\n",
    "            saved_feature = X_val_subset[:, :, feature_index].copy()\n",
    "            np.random.shuffle(X_val_subset[:, :, feature_index])\n",
    "            \n",
    "            permuted_predictions = model.predict(X_val_subset, verbose = 0)\n",
    "            permuted_class_accuracies = calculate_class_accuracies(permuted_predictions, y_val_subset)\n",
    "            \n",
    "            for class_index in baseline_class_accuracies.keys():\n",
    "                feature_importances[class_index][feature_index, n] = baseline_class_accuracies[class_index] - permuted_class_accuracies.get(class_index, 0)\n",
    "            \n",
    "            X_val_subset[:, :, feature_index] = saved_feature\n",
    "    \n",
    "    # Average the importance scores across repeats and prepare formatted output\n",
    "    average_importances = {class_index: importances.mean(axis=1) for class_index, importances in feature_importances.items()}\n",
    "    \n",
    "    # Format the output\n",
    "    formatted_importances = {f\"Class {class_index}\": importance.tolist() for class_index, importance in average_importances.items()}\n",
    "    return formatted_importances\n",
    "\n",
    "\n",
    "            \n",
    "def kfold_cross_validation(X, y, num_folds=5):\n",
    "    input_shape = X.shape[1:]  # Assuming X is (num_samples, time_steps, features)\n",
    "    num_classes = y.shape[1]\n",
    "    kfold = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    fold_no = 1\n",
    "    for train, test in kfold.split(X, y):\n",
    "        print(f\"Training on fold {fold_no}...\")\n",
    "        \n",
    "        model = create_model(input_shape, num_classes, l2_reg=reg_value)\n",
    "        lr_scheduler = LearningRateScheduler(lr_schedule, verbose=0)\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
    "        \n",
    "        model.fit(X[train], y[train], validation_data=(X[test], y[test]),\n",
    "                epochs=epochs, batch_size=batch_size, callbacks=[early_stopping, lr_scheduler], verbose=1)\n",
    "        \n",
    "        fold_no += 1\n",
    "        \n",
    "        \n",
    "        \n",
    "# version that takes speed by speed\n",
    "\n",
    "def lr_schedule(epoch, lr):\n",
    "    if epoch % 10 == 0 and epoch > 0:\n",
    "        return lr * 0.1\n",
    "    return lr\n",
    "\n",
    "def load_sequences(sequence_file_path, label_file_path):\n",
    "    sequences = np.load(sequence_file_path)\n",
    "    labels = np.load(label_file_path)\n",
    "    encoder = LabelEncoder()\n",
    "    labels_encoded = encoder.fit_transform(labels)\n",
    "    return sequences, labels_encoded\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, Dense, Input\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "\n",
    "def create_model(input_shape, num_classes, reg_type='l2', reg_value=0.001, return_logits=False):\n",
    "    if reg_type == 'l2':\n",
    "        regularizer = regularizers.l2(reg_value)\n",
    "    elif reg_type == 'l1':\n",
    "        regularizer = regularizers.l1(reg_value)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid regularizer type. Choose 'l1' or 'l2'.\")\n",
    "\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        LSTM(256, return_sequences=True, kernel_regularizer=regularizer),\n",
    "        Dropout(0.3),\n",
    "        LSTM(128, return_sequences=True, kernel_regularizer=regularizer),\n",
    "        Dropout(0.3),\n",
    "        LSTM(64, kernel_regularizer=regularizer),\n",
    "        Dropout(0.3),\n",
    "        Dense(128, activation='relu', kernel_regularizer=regularizer),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu', kernel_regularizer=regularizer),\n",
    "        Dropout(0.3)\n",
    "    ])\n",
    "    \n",
    "    if return_logits:\n",
    "        # If we want the model to return logits, add a Dense layer without activation\n",
    "        model.add(Dense(num_classes))\n",
    "    else:\n",
    "        # Otherwise, add a Dense layer with softmax activation to return probabilities\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    if return_logits == True:\n",
    "        model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "            loss=CategoricalCrossentropy(from_logits=True),\n",
    "            metrics=['accuracy'])\n",
    "    else:\n",
    "        model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "class LSTMModel(Model):\n",
    "    \"\"\"LSTM-based model for sequence processing.\"\"\"\n",
    "    def __init__(self, input_shape, num_classes, reg_type='l2', reg_value=0.001, return_logits=False):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        if reg_type == 'l2':\n",
    "            regularizer = regularizers.l2(reg_value)\n",
    "        elif reg_type == 'l1':\n",
    "            regularizer = regularizers.l1(reg_value)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid regularizer type. Choose 'l1' or 'l2'.\")\n",
    "        \n",
    "        self.sequence = [\n",
    "            LSTM(256, return_sequences=True, kernel_regularizer=regularizer, input_shape=input_shape),\n",
    "            Dropout(0.3),\n",
    "            LSTM(128, return_sequences=True, kernel_regularizer=regularizer),\n",
    "            Dropout(0.3),\n",
    "            LSTM(64, kernel_regularizer=regularizer),\n",
    "            Dropout(0.3),\n",
    "            Dense(128, activation='relu', kernel_regularizer=regularizer),\n",
    "            Dropout(0.3),\n",
    "            Dense(64, activation='relu', kernel_regularizer=regularizer),\n",
    "            Dropout(0.3)\n",
    "        ]\n",
    "        \n",
    "        self.return_logits = return_logits\n",
    "        if return_logits:\n",
    "            self.logits_layer = Dense(num_classes)\n",
    "        else:\n",
    "            self.probabilities_layer = Dense(num_classes, activation='softmax')\n",
    "        \n",
    "        self.compile_model()\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        x = inputs\n",
    "        for layer in self.sequence:\n",
    "            x = layer(x, training=training)\n",
    "        if self.return_logits:\n",
    "            x = self.logits_layer(x)\n",
    "        else:\n",
    "            x = self.probabilities_layer(x)\n",
    "        return x\n",
    "    \n",
    "    def compile_model(self):\n",
    "        if self.return_logits:\n",
    "            self.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                        loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "                        metrics=['accuracy'])\n",
    "        else:\n",
    "            self.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                        loss='sparse_categorical_crossentropy',\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "\n",
    "def normalize_importances(importances):\n",
    "    \"\"\"\n",
    "    Normalize a list of feature importances so that they sum to 1,\n",
    "    ensuring all values are positive. This function now expects\n",
    "    a list (or numpy array) of numerical importances as its input.\n",
    "    \"\"\"\n",
    "    # Convert the importances to a numpy array to ensure compatibility with numpy operations\n",
    "    importances = np.array(importances)\n",
    "    abs_importances = np.abs(importances)  # Take the absolute values to ensure all values are positive\n",
    "    total_importance = np.sum(abs_importances)\n",
    "    \n",
    "    # Avoid division by zero by checking if the total_importance is not zero\n",
    "    if total_importance != 0:\n",
    "        normalized_importances = abs_importances / total_importance\n",
    "    else:\n",
    "        # If the total is 0 (e.g., all importances are 0), return the original importances\n",
    "        normalized_importances = abs_importances\n",
    "    \n",
    "    # Return the normalized importances, ensuring it's converted back to a list if needed\n",
    "    return normalized_importances.tolist()\n",
    "\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def axioms(features, labels, training=False):\n",
    "    x_A = ltn.Variable(\"x_A\", features[labels == 0])\n",
    "    x_B = ltn.Variable(\"x_B\", features[labels == 1])\n",
    "    x_C = ltn.Variable(\"x_C\", features[labels == 2])\n",
    "    x_D = ltn.Variable(\"x_D\", features[labels == 3])\n",
    "    x_E = ltn.Variable(\"x_E\", features[labels == 4])\n",
    "    x_F = ltn.Variable(\"x_F\", features[labels == 5])\n",
    "    x_G = ltn.Variable(\"x_G\", features[labels == 6])\n",
    "    x_H = ltn.Variable(\"x_H\", features[labels == 7])\n",
    "    x_I = ltn.Variable(\"x_I\", features[labels == 8])\n",
    "    axioms = [\n",
    "        Forall(x_A, p([x_A, class_0], training=training)),\n",
    "        Forall(x_B, p([x_B, class_1], training=training)),\n",
    "        Forall(x_C, p([x_C, class_2], training=training)),\n",
    "        Forall(x_D, p([x_D, class_3], training=training)),\n",
    "        Forall(x_E, p([x_E, class_4], training=training)),\n",
    "        Forall(x_F, p([x_F, class_5], training=training)),\n",
    "        Forall(x_G, p([x_G, class_6], training=training)),\n",
    "        Forall(x_H, p([x_H, class_7], training=training)),\n",
    "        Forall(x_I, p([x_I, class_8], training=training))\n",
    "    ]\n",
    "    tf.print(\"x_A: \", x_A.tensor)\n",
    "    tf.print(\"x_B: \", x_B.tensor)\n",
    "    tf.print(\"x_C: \", x_C.tensor)\n",
    "    tf.print(\"x_D: \", x_D.tensor)\n",
    "    tf.print(\"x_E: \", x_E.tensor)\n",
    "    tf.print(\"x_F: \", x_F.tensor)\n",
    "    tf.print(\"x_G: \", x_G.tensor)\n",
    "    tf.print(\"x_H: \", x_H.tensor)\n",
    "    tf.print(\"x_I: \", x_I.tensor)\n",
    "    \n",
    "    sat_level = formula_aggregator(axioms).tensor\n",
    "    return sat_level\n",
    "\n",
    "\n",
    "# @tf.function\n",
    "# def axioms(features, labels, expected_classes, training=False):\n",
    "#     # Initialize an empty list to hold the axioms dynamically\n",
    "#     dynamic_axioms = []\n",
    "\n",
    "#     for class_index in expected_classes:\n",
    "#         # Dynamically create class-specific LTN Variables\n",
    "#         filtered_features = features[labels == class_index]\n",
    "#         class_var = ltn.Variable(f\"x_{class_index}\", filtered_features)\n",
    "\n",
    "#         class_constant = globals().get(f\"class_{class_index}\")\n",
    "        \n",
    "\n",
    "#         if class_constant is not None:\n",
    "#             dynamic_axioms.append(Forall(class_var, p([class_var, class_constant], training=training)))\n",
    "#         else:\n",
    "#             tf.print(f\"Constant for class {class_index} not found.\")\n",
    "\n",
    "#     # Aggregate the satisfaction levels from dynamically constructed axioms\n",
    "#     sat_level = formula_aggregator(dynamic_axioms).tensor\n",
    "\n",
    "#     return sat_level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Variable_speed Variable_speed: 0file [00:00, ?file/s]\n",
      "Processing Variable_speed Experiment8: 100%|██████████| 9/9 [00:19<00:00,  2.19s/file]\n",
      "Processing Variable_speed Experiment4: 100%|██████████| 9/9 [00:18<00:00,  2.10s/file]\n",
      "Processing Variable_speed Experiment6: 100%|██████████| 9/9 [00:17<00:00,  1.99s/file]\n",
      "Processing Variable_speed Experiment2: 100%|██████████| 9/9 [00:17<00:00,  1.93s/file]\n",
      "Processing Variable_speed Experiment5: 100%|██████████| 9/9 [00:14<00:00,  1.62s/file]\n",
      "Processing Variable_speed Experiment1: 100%|██████████| 9/9 [00:16<00:00,  1.81s/file]\n",
      "Processing Variable_speed Experiment9: 100%|██████████| 9/9 [00:15<00:00,  1.72s/file]\n",
      "Processing Variable_speed Experiment7: 100%|██████████| 9/9 [00:15<00:00,  1.76s/file]\n",
      "Processing Variable_speed Experiment3: 100%|██████████| 9/9 [00:17<00:00,  1.92s/file]\n",
      "Processing Variable_speed Experiment10: 100%|██████████| 9/9 [00:15<00:00,  1.77s/file]\n",
      "Processing 40_0 : 100%|██████████| 9/9 [00:14<00:00,  1.67s/file]\n",
      "Processing 30_4 : 100%|██████████| 9/9 [00:16<00:00,  1.80s/file]\n",
      "Processing 30_3 : 100%|██████████| 9/9 [00:15<00:00,  1.76s/file]\n",
      "Processing 20_0 : 100%|██████████| 9/9 [00:15<00:00,  1.76s/file]\n",
      "Processing 30_5 : 100%|██████████| 9/9 [00:14<00:00,  1.66s/file]\n",
      "Processing 30_2 : 100%|██████████| 9/9 [00:17<00:00,  1.93s/file]\n",
      "Processing 50_0 : 100%|██████████| 9/9 [00:15<00:00,  1.73s/file]\n",
      "Processing 30_0 : 100%|██████████| 9/9 [00:15<00:00,  1.73s/file]\n",
      "Processing 30_1 : 100%|██████████| 9/9 [00:16<00:00,  1.89s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted empty file: /home/ubuntu/dds_paper/DDS_Paper/data/DDS_Data_SEU/data/csvs/PGB_Variable_speed_Variable_speed_train.csv\n",
      "Deleted empty file: /home/ubuntu/dds_paper/DDS_Paper/data/DDS_Data_SEU/data/csvs/PGB_Variable_speed_Variable_speed_test.csv\n",
      "                                File Name  Number of Samples   BWF   CTF   CWF   HEA   IRF   MTF   ORF   RCF   SWF\n",
      "                        PGB_20_0_test.csv              36000  4000  4000  4000  4000  4000  4000  4000  4000  4000\n",
      "                       PGB_20_0_train.csv             180000 20000 20000 20000 20000 20000 20000 20000 20000 20000\n",
      "                        PGB_30_0_test.csv              36000  4000  4000  4000  4000  4000  4000  4000  4000  4000\n",
      "                       PGB_30_0_train.csv             180000 20000 20000 20000 20000 20000 20000 20000 20000 20000\n",
      "                        PGB_30_1_test.csv              36000  4000  4000  4000  4000  4000  4000  4000  4000  4000\n",
      "                       PGB_30_1_train.csv             180000 20000 20000 20000 20000 20000 20000 20000 20000 20000\n",
      "                        PGB_30_2_test.csv              36000  4000  4000  4000  4000  4000  4000  4000  4000  4000\n",
      "                       PGB_30_2_train.csv             180000 20000 20000 20000 20000 20000 20000 20000 20000 20000\n",
      "                        PGB_30_3_test.csv              36000  4000  4000  4000  4000  4000  4000  4000  4000  4000\n",
      "                       PGB_30_3_train.csv             180000 20000 20000 20000 20000 20000 20000 20000 20000 20000\n",
      "                        PGB_30_4_test.csv              36000  4000  4000  4000  4000  4000  4000  4000  4000  4000\n",
      "                       PGB_30_4_train.csv             180000 20000 20000 20000 20000 20000 20000 20000 20000 20000\n",
      "                        PGB_30_5_test.csv              36000  4000  4000  4000  4000  4000  4000  4000  4000  4000\n",
      "                       PGB_30_5_train.csv             180000 20000 20000 20000 20000 20000 20000 20000 20000 20000\n",
      "                        PGB_40_0_test.csv              36000  4000  4000  4000  4000  4000  4000  4000  4000  4000\n",
      "                       PGB_40_0_train.csv             180000 20000 20000 20000 20000 20000 20000 20000 20000 20000\n",
      "                        PGB_50_0_test.csv              36000  4000  4000  4000  4000  4000  4000  4000  4000  4000\n",
      "                       PGB_50_0_train.csv             180000 20000 20000 20000 20000 20000 20000 20000 20000 20000\n",
      " PGB_Variable_speed_Experiment10_test.csv              36000  4000  4000  4000  4000  4000  4000  4000  4000  4000\n",
      "PGB_Variable_speed_Experiment10_train.csv             180000 20000 20000 20000 20000 20000 20000 20000 20000 20000\n",
      "  PGB_Variable_speed_Experiment1_test.csv              36000  4000  4000  4000  4000  4000  4000  4000  4000  4000\n",
      " PGB_Variable_speed_Experiment1_train.csv             180000 20000 20000 20000 20000 20000 20000 20000 20000 20000\n",
      "  PGB_Variable_speed_Experiment2_test.csv              36000  4000  4000  4000  4000  4000  4000  4000  4000  4000\n",
      " PGB_Variable_speed_Experiment2_train.csv             180000 20000 20000 20000 20000 20000 20000 20000 20000 20000\n",
      "  PGB_Variable_speed_Experiment3_test.csv              36000  4000  4000  4000  4000  4000  4000  4000  4000  4000\n",
      " PGB_Variable_speed_Experiment3_train.csv             180000 20000 20000 20000 20000 20000 20000 20000 20000 20000\n",
      "  PGB_Variable_speed_Experiment4_test.csv              36000  4000  4000  4000  4000  4000  4000  4000  4000  4000\n",
      " PGB_Variable_speed_Experiment4_train.csv             180000 20000 20000 20000 20000 20000 20000 20000 20000 20000\n",
      "  PGB_Variable_speed_Experiment5_test.csv              36000  4000  4000  4000  4000  4000  4000  4000  4000  4000\n",
      " PGB_Variable_speed_Experiment5_train.csv             180000 20000 20000 20000 20000 20000 20000 20000 20000 20000\n",
      "  PGB_Variable_speed_Experiment6_test.csv              36000  4000  4000  4000  4000  4000  4000  4000  4000  4000\n",
      " PGB_Variable_speed_Experiment6_train.csv             180000 20000 20000 20000 20000 20000 20000 20000 20000 20000\n",
      "  PGB_Variable_speed_Experiment7_test.csv              36000  4000  4000  4000  4000  4000  4000  4000  4000  4000\n",
      " PGB_Variable_speed_Experiment7_train.csv             180000 20000 20000 20000 20000 20000 20000 20000 20000 20000\n",
      "  PGB_Variable_speed_Experiment8_test.csv              36000  4000  4000  4000  4000  4000  4000  4000  4000  4000\n",
      " PGB_Variable_speed_Experiment8_train.csv             180000 20000 20000 20000 20000 20000 20000 20000 20000 20000\n",
      "  PGB_Variable_speed_Experiment9_test.csv              36000  4000  4000  4000  4000  4000  4000  4000  4000  4000\n",
      " PGB_Variable_speed_Experiment9_train.csv             180000 20000 20000 20000 20000 20000 20000 20000 20000 20000\n"
     ]
    }
   ],
   "source": [
    "process_pgb_data(data_root_folder, csv_directory, num_train_samples, num_test_samples)\n",
    "overview_csv_files(csv_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating sequences: 100%|██████████| 58/58 [00:41<00:00,  1.39it/s]\n"
     ]
    }
   ],
   "source": [
    "# scale data\n",
    "\n",
    "# Iterate over your dataset files\n",
    "for root, dirs, files in os.walk(csv_directory):\n",
    "    for file in sorted(files):\n",
    "        if file.endswith('.csv') and not file.endswith('_scaled.csv'):  # Process only unscaled .csv files\n",
    "            csv_path = os.path.join(root, file)\n",
    "            if 'train' in file:\n",
    "                # Handle training data\n",
    "                scaler_path = os.path.join(root, 'scaler_' + file.replace('.csv', '.joblib'))\n",
    "                scaled_train_df = load_and_scale_data(csv_path, save_scaler_path=scaler_path)\n",
    "                # Save the scaled training data\n",
    "                scaled_csv_path = csv_path.replace('.csv', '_scaled.csv')\n",
    "                scaled_train_df.to_csv(scaled_csv_path, index=False)\n",
    "            elif 'test' in file:\n",
    "                # Handle testing data\n",
    "                scaler_path = os.path.join(root, 'scaler_' + file.replace('_test.csv', '_train.joblib'))\n",
    "                scaler = joblib.load(scaler_path) if os.path.exists(scaler_path) else None\n",
    "                scaled_test_df = load_and_scale_data(csv_path, scaler=scaler)\n",
    "                # Save the scaled testing data\n",
    "                scaled_csv_path = csv_path.replace('.csv', '_scaled.csv')\n",
    "                scaled_test_df.to_csv(scaled_csv_path, index=False)\n",
    "\n",
    "            # Delete the original unscaled .csv file\n",
    "            os.remove(csv_path)\n",
    "            \n",
    "#create sequences\n",
    "save_sequences(csv_directory, sequences_directory, sequence_length)\n",
    "\n",
    "#add_speed_feature_and_save(csv_directory, sequences_directory, sequence_length)\n",
    "\n",
    "#merge_npy_files_with_memmap_separated(sequences_directory, sequences_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Training fold </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">/</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> for PGB_20_0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mTraining fold \u001b[0m\u001b[1;32m1\u001b[0m\u001b[1;32m/\u001b[0m\u001b[1;32m2\u001b[0m\u001b[1;32m for PGB_20_0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211/211 [==============================] - 121s 541ms/step - loss: 3.3610 - accuracy: 0.5974 - val_loss: 3.4664 - val_accuracy: 0.3096 - lr: 0.0010\n",
      "3367/3367 [==============================] - 166s 49ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> Accuracy: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3096</span>, Precision: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3300</span>, Recall: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1856</span>, F1: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1998</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold \u001b[1;36m1\u001b[0m Accuracy: \u001b[1;36m0.3096\u001b[0m, Precision: \u001b[1;36m0.3300\u001b[0m, Recall: \u001b[1;36m0.1856\u001b[0m, F1: \u001b[1;36m0.1998\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Feature Importance: 100%|██████████| 8/8 [00:26<00:00,  3.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 Feature Importances: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Class 1 Feature Importances: [0.0, 0.0, -0.04076086956521739, -0.0733695652173913, 0.0, -0.05163043478260869, -0.059782608695652176, -0.01358695652173913]\n",
      "Class 2 Feature Importances: [0.42857142857142855, 0.005102040816326536, 0.002551020408163268, -0.020408163265306145, 0.0, -0.020408163265306145, 0.002551020408163268, 0.0]\n",
      "Class 3 Feature Importances: [0.0, -0.01098901098901099, -0.0027472527472527475, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Class 5 Feature Importances: [0.5416666666666667, 0.05952380952380959, 0.05654761904761907, 0.014880952380952439, 0.011904761904761973, 0.035714285714285726, 0.06250000000000006, -0.011904761904761862]\n",
      "Class 8 Feature Importances: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "x_A:  []\n",
      "x_B:  [[[0.439887 0.200143382 0.1885418 ... 0.181395814 0.155444726 0.163396433]\n",
      "  [0.440510064 0.237116992 0.235405609 ... -0.335412353 0.279405117 0.162955016]\n",
      "  [0.437767 0.263864845 0.221348926 ... -0.189845547 0.324163 0.231194258]\n",
      "  ...\n",
      "  [0.440570801 0.260257095 0.125422657 ... 0.0126002748 0.259445518 0.264313787]\n",
      "  [0.439350128 0.217285901 0.243138 ... 0.00929384 0.259181857 0.255126953]\n",
      "  [0.442504674 0.205934152 0.152696058 ... 0.136403203 0.19941166 0.187894687]]\n",
      "\n",
      " [[0.440510064 0.237116992 0.235405609 ... -0.335412353 0.279405117 0.162955016]\n",
      "  [0.437767 0.263864845 0.221348926 ... -0.189845547 0.324163 0.231194258]\n",
      "  [0.439089537 0.238840446 0.19778116 ... 0.00443882402 0.318362743 0.268369257]\n",
      "  ...\n",
      "  [0.439350128 0.217285901 0.243138 ... 0.00929384 0.259181857 0.255126953]\n",
      "  [0.442504674 0.205934152 0.152696058 ... 0.136403203 0.19941166 0.187894687]\n",
      "  [0.438127518 0.200327218 0.176733211 ... 0.24735707 0.198775798 0.132139087]]\n",
      "\n",
      " [[0.437767 0.263864845 0.221348926 ... -0.189845547 0.324163 0.231194258]\n",
      "  [0.439089537 0.238840446 0.19778116 ... 0.00443882402 0.318362743 0.268369257]\n",
      "  [0.439340323 0.208438888 0.238073647 ... 0.106226765 0.232786193 0.169741705]\n",
      "  ...\n",
      "  [0.442504674 0.205934152 0.152696058 ... 0.136403203 0.19941166 0.187894687]\n",
      "  [0.438127518 0.200327218 0.176733211 ... 0.24735707 0.198775798 0.132139087]\n",
      "  [0.435210049 0.185712412 0.283109337 ... 0.173820317 0.186337888 0.162182555]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.435688108 0.253018647 0.154252425 ... -0.0051875026 0.270565182 0.192695022]\n",
      "  [0.434553653 0.273010582 0.225820392 ... -0.232452512 0.397906452 0.313558608]\n",
      "  [0.437343776 0.204808161 0.163541183 ... -0.118527032 0.267742634 0.227152586]\n",
      "  ...\n",
      "  [0.445500523 0.190101445 0.223671123 ... 0.086137034 0.261415094 0.185908347]\n",
      "  [0.44569841 0.214091778 0.26332131 ... 0.0637872219 0.188633159 0.190956965]\n",
      "  [0.444176 0.216113955 0.184465617 ... 0.0543701611 0.166502371 0.15807192]]\n",
      "\n",
      " [[0.434553653 0.273010582 0.225820392 ... -0.232452512 0.397906452 0.313558608]\n",
      "  [0.437343776 0.204808161 0.163541183 ... -0.118527032 0.267742634 0.227152586]\n",
      "  [0.440984219 0.272068441 0.10649927 ... 0.27698943 0.149287805 0.11093761]\n",
      "  ...\n",
      "  [0.44569841 0.214091778 0.26332131 ... 0.0637872219 0.188633159 0.190956965]\n",
      "  [0.444176 0.216113955 0.184465617 ... 0.0543701611 0.166502371 0.15807192]\n",
      "  [0.447638154 0.164341703 0.214579985 ... 0.0553327911 0.243936896 0.188363686]]\n",
      "\n",
      " [[0.437343776 0.204808161 0.163541183 ... -0.118527032 0.267742634 0.227152586]\n",
      "  [0.440984219 0.272068441 0.10649927 ... 0.27698943 0.149287805 0.11093761]\n",
      "  [0.439749837 0.183322564 0.259195715 ... -0.00648496393 0.277652621 0.133270204]\n",
      "  ...\n",
      "  [0.444176 0.216113955 0.184465617 ... 0.0543701611 0.166502371 0.15807192]\n",
      "  [0.447638154 0.164341703 0.214579985 ... 0.0553327911 0.243936896 0.188363686]\n",
      "  [0.448702067 0.194260687 0.204006612 ... 0.103464425 0.237640396 0.234339297]]]\n",
      "x_C:  []\n",
      "x_D:  []\n",
      "x_E:  []\n",
      "x_F:  []\n",
      "x_G:  []\n",
      "x_H:  []\n",
      "x_I:  []\n",
      "Batch Satisfaction Level: nan\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Training fold </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">/</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> for PGB_20_0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mTraining fold \u001b[0m\u001b[1;32m2\u001b[0m\u001b[1;32m/\u001b[0m\u001b[1;32m2\u001b[0m\u001b[1;32m for PGB_20_0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170/211 [=======================>......] - ETA: 14s - loss: 3.6541 - accuracy: 0.6717"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 55\u001b[0m\n\u001b[1;32m     50\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m ModelCheckpoint(model_filepath, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, save_weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     51\u001b[0m tensorboard \u001b[38;5;241m=\u001b[39m TensorBoard(log_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./logs/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_fold_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 55\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_fold\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensorboard\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Assuming your model outputs class indices directly\u001b[39;00m\n\u001b[1;32m     62\u001b[0m y_val_pred_classes \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_val_fold)\n",
      "File \u001b[0;32m~/miniconda3/envs/ltn/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/ltn/lib/python3.10/site-packages/keras/src/engine/training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1776\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1777\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1780\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1781\u001b[0m ):\n\u001b[1;32m   1782\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1783\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1785\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/envs/ltn/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/ltn/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    828\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 831\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    833\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    834\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/ltn/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:867\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    864\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 867\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/envs/ltn/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ltn/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1260\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1262\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1263\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1264\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1265\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1266\u001b[0m     args,\n\u001b[1;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1268\u001b[0m     executing_eagerly)\n\u001b[1;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/ltn/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflat_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    216\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ltn/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    251\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 252\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    262\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/ltn/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1479\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1480\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1481\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1482\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1483\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1484\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1485\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1487\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1488\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1489\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1493\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1494\u001b[0m   )\n",
      "File \u001b[0;32m~/miniconda3/envs/ltn/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;66;03m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     55\u001b[0m       tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(t)\n\u001b[1;32m     56\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, core_types\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m     57\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[1;32m     58\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[1;32m     59\u001b[0m   ]\n\u001b[0;32m---> 60\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     63\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "kf = KFold(n_splits=n_splits, shuffle=False)\n",
    "counter = 0\n",
    "console = Console()\n",
    "\n",
    "# Placeholder for processed base names and metrics\n",
    "processed_bases = set()\n",
    "metrics_summary = []\n",
    "\n",
    "for file in sorted(os.listdir(sequences_directory)):\n",
    "    if \"_train_scaled_sequences.npy\" in file:\n",
    "        base_name = file.replace(\"_train_scaled_sequences.npy\", \"\")\n",
    "        if base_name in processed_bases:\n",
    "            continue\n",
    "        counter+=1\n",
    "        \n",
    "        model_filepath = os.path.join(model_save_directory, f\"saved.h5\")\n",
    "        \n",
    "            \n",
    "            \n",
    "        # Load sequences and labels\n",
    "        train_sequence_file_path = os.path.join(sequences_directory, f\"{base_name}_train_scaled_sequences.npy\")\n",
    "        train_label_file_path = os.path.join(sequences_directory, f\"{base_name}_train_scaled_labels.npy\")\n",
    "        X_train, y_train = load_sequences(train_sequence_file_path, train_label_file_path)\n",
    "\n",
    "        # Assuming the existence of a test set (adjust if necessary)\n",
    "        test_sequence_file_path = os.path.join(sequences_directory, f\"{base_name}_test_scaled_sequences.npy\")\n",
    "        test_label_file_path = os.path.join(sequences_directory, f\"{base_name}_test_scaled_labels.npy\")\n",
    "        X_test, y_test = load_sequences(test_sequence_file_path, test_label_file_path)\n",
    "\n",
    "        # Merge for cross-validation\n",
    "        X = np.concatenate((X_train, X_test), axis=0)\n",
    "        y = np.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "        input_shape = (sequence_length, num_features)\n",
    "        fold_metrics = []\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "            console.print(f\"[bold green]Training fold {fold + 1}/{n_splits} for {base_name}[/]\")\n",
    "            X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "            y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
    "\n",
    "            model = LSTMModel(input_shape=input_shape, num_classes=num_classes, reg_type=reg_type, reg_value=reg_value, return_logits=True)\n",
    "            \n",
    "            \n",
    "            lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
    "\n",
    "            # Add ModelCheckpoint and TensorBoard for improved monitoring and model saving\n",
    "            model_filepath = os.path.join(model_save_directory, f\"model_{base_name}_fold_{fold+1}.h5\")\n",
    "            checkpoint = ModelCheckpoint(model_filepath, save_best_only=True, monitor='val_loss', save_weights_only=True)\n",
    "            tensorboard = TensorBoard(log_dir=f\"./logs/{base_name}_fold_{fold+1}\")\n",
    "\n",
    "\n",
    "\n",
    "            history = model.fit(X_train_fold, y_train_fold, validation_data=(X_val_fold, y_val_fold),\n",
    "                                epochs=epochs, batch_size=batch_size, callbacks=[early_stopping, lr_scheduler, checkpoint, tensorboard], verbose=1)\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Assuming your model outputs class indices directly\n",
    "            y_val_pred_classes = model.predict(X_val_fold)\n",
    "            y_val_pred_classes = np.argmax(y_val_pred_classes, axis=1)  # Get predicted classes\n",
    "\n",
    "            # Since y_val_fold contains integer labels, there's no need for conversion\n",
    "            y_val_true_classes = y_val_fold  # Directly use the integer labels\n",
    "\n",
    "            # Calculate and store metrics for this fold\n",
    "            accuracy = accuracy_score(y_val_true_classes, y_val_pred_classes)\n",
    "            precision = precision_score(y_val_true_classes, y_val_pred_classes, average='macro', zero_division=0)\n",
    "            recall = recall_score(y_val_true_classes, y_val_pred_classes, average='macro', zero_division=0)\n",
    "            f1 = f1_score(y_val_true_classes, y_val_pred_classes, average='macro')\n",
    "            fold_metrics.append((accuracy, precision, recall, f1))\n",
    "\n",
    "\n",
    "            console.print(f\"Fold {fold+1} Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "            \n",
    "            class_importances = permutation_importance_per_class(model, X_val_fold, y_val_fold, n_repeats=4, n_samples=n_samples)\n",
    "            for class_id, importances in class_importances.items():\n",
    "                print(f\"{class_id} Feature Importances:\", importances)\n",
    "            \n",
    "            \n",
    "            p = ltn.Predicate.FromLogits(model, activation_function=\"softmax\", with_class_indexing=True)\n",
    "            \n",
    "            normalized_importances = {}\n",
    "\n",
    "            # Assuming normalize_importances function logic remains the same and is applicable here.\n",
    "            for class_label, importances in class_importances.items():\n",
    "                normalized = normalize_importances(importances)\n",
    "                normalized_importances[class_label] = normalized\n",
    "\n",
    "\n",
    "            small_batch_size = 10000  # Adjust batch size as needed\n",
    "            ds_val_fold = tf.data.Dataset.from_tensor_slices((X,y)).batch(small_batch_size)\n",
    "            for batch_features, batch_labels in ds_val_fold:\n",
    "\n",
    "                \n",
    "                # Assuming the axioms function and predicate are prepared for batch processing\n",
    "                batch_satisfaction_level = axioms(batch_features, batch_labels, training=False)\n",
    "                \n",
    "                # Print or aggregate the batch satisfaction levels as needed\n",
    "                # .numpy() is used to convert TensorFlow tensors to numpy arrays for printing or further processing\n",
    "                print(f\"Batch Satisfaction Level: {batch_satisfaction_level.numpy():.4f}\")\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "                # After processing all folds for the current CSV pair\n",
    "        if fold_metrics:\n",
    "            # Calculate the average of each metric across all folds\n",
    "            avg_accuracy = mean([metric[0] for metric in fold_metrics])\n",
    "            avg_precision = mean([metric[1] for metric in fold_metrics])\n",
    "            avg_recall = mean([metric[2] for metric in fold_metrics])\n",
    "            avg_f1 = mean([metric[3] for metric in fold_metrics])\n",
    "\n",
    "            # Append averaged metrics to the metrics_summary for overall analysis if needed\n",
    "            metrics_summary.append((base_name, avg_accuracy, avg_precision, avg_recall, avg_f1))\n",
    "\n",
    "            # Print the averages\n",
    "            console.print(f\"[bold magenta]Average metrics for {base_name} across {n_splits} folds:[/]\")\n",
    "            console.print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
    "            console.print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "            console.print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "            console.print(f\"Average F1: {avg_f1:.4f}\\n\")\n",
    "        \n",
    "\n",
    "        \n",
    "    if counter!=0:\n",
    "        break\n",
    "\n",
    "\n",
    "console.print(f\"[bold blue]Model for {base_name} saved.[/]\")\n",
    "# Optionally, after all file pairs have been processed, print a summary of averages across all file pairs\n",
    "console.print(\"[bold blue]Overall Averages Across All File Pairs:[/]\")\n",
    "overall_avg_accuracy = mean([metrics[1] for metrics in metrics_summary])\n",
    "overall_avg_precision = mean([metrics[2] for metrics in metrics_summary])\n",
    "overall_avg_recall = mean([metrics[3] for metrics in metrics_summary])\n",
    "overall_avg_f1 = mean([metrics[4] for metrics in metrics_summary])\n",
    "\n",
    "console.print(f\"Overall Average Accuracy: {overall_avg_accuracy:.4f}\")\n",
    "console.print(f\"Overall Average Precision: {overall_avg_precision:.4f}\")\n",
    "console.print(f\"Overall Average Recall: {overall_avg_recall:.4f}\")\n",
    "console.print(f\"Overall Average F1: {overall_avg_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 30, 8)\n",
      "(128,)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Binding inputs to tf.function failed due to `multiple values for argument 'training'`. Received args: (<tf.Tensor: shape=(128, 30, 8), dtype=float64, numpy=\narray([[[ 0.43988699,  0.20014338,  0.1885418 , ...,  0.18139581,\n          0.15544472,  0.16339643],\n        [ 0.44051006,  0.237117  ,  0.2354056 , ..., -0.33541235,\n          0.2794051 ,  0.16295502],\n        [ 0.43776699,  0.26386485,  0.22134893, ..., -0.18984555,\n          0.32416298,  0.23119425],\n        ...,\n        [ 0.4405708 ,  0.26025711,  0.12542266, ...,  0.01260027,\n          0.25944551,  0.2643138 ],\n        [ 0.43935013,  0.2172859 ,  0.24313801, ...,  0.00929384,\n          0.25918187,  0.25512695],\n        [ 0.44250468,  0.20593415,  0.15269606, ...,  0.1364032 ,\n          0.19941165,  0.18789469]],\n\n       [[ 0.44051006,  0.237117  ,  0.2354056 , ..., -0.33541235,\n          0.2794051 ,  0.16295502],\n        [ 0.43776699,  0.26386485,  0.22134893, ..., -0.18984555,\n          0.32416298,  0.23119425],\n        [ 0.43908954,  0.23884044,  0.19778116, ...,  0.00443882,\n          0.31836275,  0.26836925],\n        ...,\n        [ 0.43935013,  0.2172859 ,  0.24313801, ...,  0.00929384,\n          0.25918187,  0.25512695],\n        [ 0.44250468,  0.20593415,  0.15269606, ...,  0.1364032 ,\n          0.19941165,  0.18789469],\n        [ 0.4381275 ,  0.20032721,  0.17673321, ...,  0.24735708,\n          0.1987758 ,  0.13213908]],\n\n       [[ 0.43776699,  0.26386485,  0.22134893, ..., -0.18984555,\n          0.32416298,  0.23119425],\n        [ 0.43908954,  0.23884044,  0.19778116, ...,  0.00443882,\n          0.31836275,  0.26836925],\n        [ 0.43934034,  0.20843889,  0.23807365, ...,  0.10622676,\n          0.2327862 ,  0.1697417 ],\n        ...,\n        [ 0.44250468,  0.20593415,  0.15269606, ...,  0.1364032 ,\n          0.19941165,  0.18789469],\n        [ 0.4381275 ,  0.20032721,  0.17673321, ...,  0.24735708,\n          0.1987758 ,  0.13213908],\n        [ 0.43521004,  0.18571241,  0.28310934, ...,  0.17382031,\n          0.18633789,  0.16218255]],\n\n       ...,\n\n       [[ 0.45717231,  0.13311289,  0.14380256, ...,  0.03520121,\n          0.20780182,  0.24384341],\n        [ 0.45515223,  0.09489839,  0.18928293, ..., -0.05131016,\n          0.2533041 ,  0.21624149],\n        [ 0.44933494,  0.14476337,  0.22258414, ..., -0.05252392,\n          0.20187752,  0.19148114],\n        ...,\n        [ 0.42381842,  0.1847243 ,  0.10526406, ...,  0.20529422,\n          0.2644548 ,  0.19252949],\n        [ 0.42709445,  0.22307667,  0.19251917, ...,  0.31557843,\n          0.18478703,  0.22791126],\n        [ 0.43121102,  0.19566242,  0.08584659, ...,  0.13698915,\n          0.18520576,  0.23563594]],\n\n       [[ 0.45515223,  0.09489839,  0.18928293, ..., -0.05131016,\n          0.2533041 ,  0.21624149],\n        [ 0.44933494,  0.14476337,  0.22258414, ..., -0.05252392,\n          0.20187752,  0.19148114],\n        [ 0.45216619,  0.14278716,  0.15610523, ..., -0.10224598,\n          0.21768081,  0.19272261],\n        ...,\n        [ 0.42709445,  0.22307667,  0.19251917, ...,  0.31557843,\n          0.18478703,  0.22791126],\n        [ 0.43121102,  0.19566242,  0.08584659, ...,  0.13698915,\n          0.18520576,  0.23563594],\n        [ 0.43447333,  0.20207363,  0.20175853, ...,  0.17327622,\n          0.18554695,  0.29928175]],\n\n       [[ 0.44933494,  0.14476337,  0.22258414, ..., -0.05252392,\n          0.20187752,  0.19148114],\n        [ 0.45216619,  0.14278716,  0.15610523, ..., -0.10224598,\n          0.21768081,  0.19272261],\n        [ 0.44954459,  0.15195588,  0.24061818, ...,  0.00732672,\n          0.20625095,  0.20979966],\n        ...,\n        [ 0.43121102,  0.19566242,  0.08584659, ...,  0.13698915,\n          0.18520576,  0.23563594],\n        [ 0.43447333,  0.20207363,  0.20175853, ...,  0.17327622,\n          0.18554695,  0.29928175],\n        [ 0.43517477,  0.20225747,  0.08246212, ...,  0.07755705,\n          0.24500699,  0.26613461]]])>, <tf.Tensor: shape=(128,), dtype=int64, numpy=\narray([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])>, [0, 1, 2, 3, 5, 8]) and kwargs: {'training': False} for signature: (features, labels, training=<object object at 0x7fb29c865ae0>).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch_labels\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#print(batch_features, batch_labels)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Predict with the model\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Assuming the axioms function and predicate are prepared for batch processing\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m batch_satisfaction_level \u001b[38;5;241m=\u001b[39m \u001b[43maxioms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Print or aggregate the batch satisfaction levels as needed\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# .numpy() is used to convert TensorFlow tensors to numpy arrays for printing or further processing\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch Satisfaction Level: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_satisfaction_level\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ltn/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/ltn/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/function_type_utils.py:441\u001b[0m, in \u001b[0;36mbind_function_inputs\u001b[0;34m(args, kwargs, function_type, default_values)\u001b[0m\n\u001b[1;32m    437\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m function_type\u001b[38;5;241m.\u001b[39mbind_with_defaults(\n\u001b[1;32m    438\u001b[0m       args, sanitized_kwargs, default_values\n\u001b[1;32m    439\u001b[0m   )\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 441\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    442\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBinding inputs to tf.function failed due to `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    443\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived args: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and kwargs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msanitized_kwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for signature:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    444\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    445\u001b[0m   ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bound_arguments\n",
      "\u001b[0;31mTypeError\u001b[0m: Binding inputs to tf.function failed due to `multiple values for argument 'training'`. Received args: (<tf.Tensor: shape=(128, 30, 8), dtype=float64, numpy=\narray([[[ 0.43988699,  0.20014338,  0.1885418 , ...,  0.18139581,\n          0.15544472,  0.16339643],\n        [ 0.44051006,  0.237117  ,  0.2354056 , ..., -0.33541235,\n          0.2794051 ,  0.16295502],\n        [ 0.43776699,  0.26386485,  0.22134893, ..., -0.18984555,\n          0.32416298,  0.23119425],\n        ...,\n        [ 0.4405708 ,  0.26025711,  0.12542266, ...,  0.01260027,\n          0.25944551,  0.2643138 ],\n        [ 0.43935013,  0.2172859 ,  0.24313801, ...,  0.00929384,\n          0.25918187,  0.25512695],\n        [ 0.44250468,  0.20593415,  0.15269606, ...,  0.1364032 ,\n          0.19941165,  0.18789469]],\n\n       [[ 0.44051006,  0.237117  ,  0.2354056 , ..., -0.33541235,\n          0.2794051 ,  0.16295502],\n        [ 0.43776699,  0.26386485,  0.22134893, ..., -0.18984555,\n          0.32416298,  0.23119425],\n        [ 0.43908954,  0.23884044,  0.19778116, ...,  0.00443882,\n          0.31836275,  0.26836925],\n        ...,\n        [ 0.43935013,  0.2172859 ,  0.24313801, ...,  0.00929384,\n          0.25918187,  0.25512695],\n        [ 0.44250468,  0.20593415,  0.15269606, ...,  0.1364032 ,\n          0.19941165,  0.18789469],\n        [ 0.4381275 ,  0.20032721,  0.17673321, ...,  0.24735708,\n          0.1987758 ,  0.13213908]],\n\n       [[ 0.43776699,  0.26386485,  0.22134893, ..., -0.18984555,\n          0.32416298,  0.23119425],\n        [ 0.43908954,  0.23884044,  0.19778116, ...,  0.00443882,\n          0.31836275,  0.26836925],\n        [ 0.43934034,  0.20843889,  0.23807365, ...,  0.10622676,\n          0.2327862 ,  0.1697417 ],\n        ...,\n        [ 0.44250468,  0.20593415,  0.15269606, ...,  0.1364032 ,\n          0.19941165,  0.18789469],\n        [ 0.4381275 ,  0.20032721,  0.17673321, ...,  0.24735708,\n          0.1987758 ,  0.13213908],\n        [ 0.43521004,  0.18571241,  0.28310934, ...,  0.17382031,\n          0.18633789,  0.16218255]],\n\n       ...,\n\n       [[ 0.45717231,  0.13311289,  0.14380256, ...,  0.03520121,\n          0.20780182,  0.24384341],\n        [ 0.45515223,  0.09489839,  0.18928293, ..., -0.05131016,\n          0.2533041 ,  0.21624149],\n        [ 0.44933494,  0.14476337,  0.22258414, ..., -0.05252392,\n          0.20187752,  0.19148114],\n        ...,\n        [ 0.42381842,  0.1847243 ,  0.10526406, ...,  0.20529422,\n          0.2644548 ,  0.19252949],\n        [ 0.42709445,  0.22307667,  0.19251917, ...,  0.31557843,\n          0.18478703,  0.22791126],\n        [ 0.43121102,  0.19566242,  0.08584659, ...,  0.13698915,\n          0.18520576,  0.23563594]],\n\n       [[ 0.45515223,  0.09489839,  0.18928293, ..., -0.05131016,\n          0.2533041 ,  0.21624149],\n        [ 0.44933494,  0.14476337,  0.22258414, ..., -0.05252392,\n          0.20187752,  0.19148114],\n        [ 0.45216619,  0.14278716,  0.15610523, ..., -0.10224598,\n          0.21768081,  0.19272261],\n        ...,\n        [ 0.42709445,  0.22307667,  0.19251917, ...,  0.31557843,\n          0.18478703,  0.22791126],\n        [ 0.43121102,  0.19566242,  0.08584659, ...,  0.13698915,\n          0.18520576,  0.23563594],\n        [ 0.43447333,  0.20207363,  0.20175853, ...,  0.17327622,\n          0.18554695,  0.29928175]],\n\n       [[ 0.44933494,  0.14476337,  0.22258414, ..., -0.05252392,\n          0.20187752,  0.19148114],\n        [ 0.45216619,  0.14278716,  0.15610523, ..., -0.10224598,\n          0.21768081,  0.19272261],\n        [ 0.44954459,  0.15195588,  0.24061818, ...,  0.00732672,\n          0.20625095,  0.20979966],\n        ...,\n        [ 0.43121102,  0.19566242,  0.08584659, ...,  0.13698915,\n          0.18520576,  0.23563594],\n        [ 0.43447333,  0.20207363,  0.20175853, ...,  0.17327622,\n          0.18554695,  0.29928175],\n        [ 0.43517477,  0.20225747,  0.08246212, ...,  0.07755705,\n          0.24500699,  0.26613461]]])>, <tf.Tensor: shape=(128,), dtype=int64, numpy=\narray([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])>, [0, 1, 2, 3, 5, 8]) and kwargs: {'training': False} for signature: (features, labels, training=<object object at 0x7fb29c865ae0>)."
     ]
    }
   ],
   "source": [
    "small_batch_size = 128  # Adjust batch size as needed\n",
    "ds_val_fold = tf.data.Dataset.from_tensor_slices((X_val_fold, y_val_fold)).batch(small_batch_size)\n",
    "for batch_features, batch_labels in ds_val_fold:\n",
    "    print(batch_features.shape)\n",
    "    print(batch_labels.shape)\n",
    "    #print(batch_features, batch_labels)\n",
    "    # Predict with the model\n",
    "    \n",
    "    # Assuming the axioms function and predicate are prepared for batch processing\n",
    "    batch_satisfaction_level = axioms(batch_features, batch_labels, expected_classes, training=False)\n",
    "    \n",
    "    # Print or aggregate the batch satisfaction levels as needed\n",
    "    # .numpy() is used to convert TensorFlow tensors to numpy arrays for printing or further processing\n",
    "    print(f\"Batch Satisfaction Level: {batch_satisfaction_level.numpy():.4f}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/tmp/ipykernel_618261/1050530493.py\", line 512, in axioms  *\n        sat_level = formula_aggregator([Forall(sequence_var, p(sequence_var, training=training))]).tensor\n    File \"/home/ubuntu/miniconda3/envs/ltn/lib/python3.10/site-packages/ltn/core.py\", line 189, in __call__  *\n        expr = super().__call__(inputs, *args, **kwargs)\n    File \"/home/ubuntu/miniconda3/envs/ltn/lib/python3.10/site-packages/ltn/core.py\", line 155, in __call__  *\n        t_outputs = self.model(flat_inputs[0].tensor, *args, **kwargs)\n    File \"/home/ubuntu/miniconda3/envs/ltn/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_files2c83d3r.py\", line 18, in tf__call\n        retval_ = ag__.converted_call(ag__.ld(tf).gather_nd, (ag__.ld(truth_degrees), ag__.ld(indices)), dict(batch_dims=1), fscope)\n\n    ValueError: Exception encountered when calling layer 'private__softmax_tf_model' (type _SoftmaxTfModel).\n    \n    in user code:\n    \n        File \"/home/ubuntu/miniconda3/envs/ltn/lib/python3.10/site-packages/ltn/core.py\", line 520, in call  *\n            return tf.gather_nd(truth_degrees, indices, batch_dims=1)\n    \n        ValueError: Cannot reshape a tensor with 240 elements to shape [0,8] (0 elements) for '{{node private__softmax_tf_model/BatchGatherND/Reshape_2}} = Reshape[T=DT_INT32, Tshape=DT_INT32](private__softmax_tf_model/Cast, private__softmax_tf_model/BatchGatherND/concat_2)' with input shapes: [30,8], [2] and with input tensors computed as partial shapes: input[1] = [0,8].\n    \n    \n    Call arguments received by layer 'private__softmax_tf_model' (type _SoftmaxTfModel):\n      • inputs=tf.Tensor(shape=(1, 30, 8), dtype=float32)\n      • args=<class 'inspect._empty'>\n      • kwargs={'training': 'False'}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m label \u001b[38;5;241m=\u001b[39m y_train_fold[i]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Convert the current sequence and label to tensors and call the axioms function\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m individual_sat_level \u001b[38;5;241m=\u001b[39m \u001b[43maxioms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Satisfaction Level: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindividual_sat_level\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Example: break after the first iteration for demonstration\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ltn/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filedwqtb49g.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__axioms\u001b[0;34m(features, label, training)\u001b[0m\n\u001b[1;32m      9\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m     10\u001b[0m sequence_var \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(ltn)\u001b[38;5;241m.\u001b[39mVariable, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msequence_var\u001b[39m\u001b[38;5;124m'\u001b[39m, ag__\u001b[38;5;241m.\u001b[39mld(features)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 11\u001b[0m sat_level \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(formula_aggregator), ([ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(Forall), (ag__\u001b[38;5;241m.\u001b[39mld(sequence_var), \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence_var\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)],), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\u001b[38;5;241m.\u001b[39mtensor\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filefcuvnzpt.py:61\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf____call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m x \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     60\u001b[0m ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mnot_(ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28misinstance\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(inputs), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mlist\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mtuple\u001b[39m))), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)), if_body_2, else_body_2, get_state_3, set_state_3, (), \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 61\u001b[0m expr \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m wff \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(Formula), (ag__\u001b[38;5;241m.\u001b[39mld(expr)\u001b[38;5;241m.\u001b[39mtensor, ag__\u001b[38;5;241m.\u001b[39mld(expr)\u001b[38;5;241m.\u001b[39mfree_vars), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filei7a2yo1c.py:32\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf____call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflat_inputs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     31\u001b[0m t_outputs \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt_outputs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mif_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melse_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mt_outputs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m free_dims \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mcast, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mshape, (ag__\u001b[38;5;241m.\u001b[39mld(inputs)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtensor,), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)[:ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mlen\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(inputs)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mfree_vars,), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)], ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mint32), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state_1\u001b[39m():\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filei7a2yo1c.py:23\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf____call__.<locals>.if_body\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m inputs \u001b[38;5;241m=\u001b[39m [ag__\u001b[38;5;241m.\u001b[39mld(inputs)]\n\u001b[1;32m     22\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(_flatten_free_dims), (ag__\u001b[38;5;241m.\u001b[39mld(inputs),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 23\u001b[0m t_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ltn/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_files2c83d3r.py:18\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mgather_nd, (ag__\u001b[38;5;241m.\u001b[39mld(truth_degrees), ag__\u001b[38;5;241m.\u001b[39mld(indices)), \u001b[38;5;28mdict\u001b[39m(batch_dims\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), fscope)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     20\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_618261/1050530493.py\", line 512, in axioms  *\n        sat_level = formula_aggregator([Forall(sequence_var, p(sequence_var, training=training))]).tensor\n    File \"/home/ubuntu/miniconda3/envs/ltn/lib/python3.10/site-packages/ltn/core.py\", line 189, in __call__  *\n        expr = super().__call__(inputs, *args, **kwargs)\n    File \"/home/ubuntu/miniconda3/envs/ltn/lib/python3.10/site-packages/ltn/core.py\", line 155, in __call__  *\n        t_outputs = self.model(flat_inputs[0].tensor, *args, **kwargs)\n    File \"/home/ubuntu/miniconda3/envs/ltn/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_files2c83d3r.py\", line 18, in tf__call\n        retval_ = ag__.converted_call(ag__.ld(tf).gather_nd, (ag__.ld(truth_degrees), ag__.ld(indices)), dict(batch_dims=1), fscope)\n\n    ValueError: Exception encountered when calling layer 'private__softmax_tf_model' (type _SoftmaxTfModel).\n    \n    in user code:\n    \n        File \"/home/ubuntu/miniconda3/envs/ltn/lib/python3.10/site-packages/ltn/core.py\", line 520, in call  *\n            return tf.gather_nd(truth_degrees, indices, batch_dims=1)\n    \n        ValueError: Cannot reshape a tensor with 240 elements to shape [0,8] (0 elements) for '{{node private__softmax_tf_model/BatchGatherND/Reshape_2}} = Reshape[T=DT_INT32, Tshape=DT_INT32](private__softmax_tf_model/Cast, private__softmax_tf_model/BatchGatherND/concat_2)' with input shapes: [30,8], [2] and with input tensors computed as partial shapes: input[1] = [0,8].\n    \n    \n    Call arguments received by layer 'private__softmax_tf_model' (type _SoftmaxTfModel):\n      • inputs=tf.Tensor(shape=(1, 30, 8), dtype=float32)\n      • args=<class 'inspect._empty'>\n      • kwargs={'training': 'False'}\n"
     ]
    }
   ],
   "source": [
    "# Iterate over sequences and their labels\n",
    "for i in range(len(X_train_fold)):\n",
    "    feature_sequence = X_train_fold[i:i+1]  # Keeping the batch dimension by slicing\n",
    "    label = y_train_fold[i]\n",
    "    \n",
    "    # Convert the current sequence and label to tensors and call the axioms function\n",
    "    individual_sat_level = axioms(tf.convert_to_tensor(feature_sequence, dtype=tf.float32), \n",
    "                                tf.convert_to_tensor(label, dtype=tf.int32),\n",
    "                                training=False)\n",
    "    print(f\"Sample {i} Satisfaction Level: {individual_sat_level.numpy()}\")\n",
    "    if i == 0: break  # Example: break after the first iteration for demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Model for PGB_20_0 saved.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mModel for PGB_20_0 saved.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save the model at the end\n",
    "model.save(model_path)\n",
    "console.print(f\"[bold blue]Model for {base_name} saved.[/]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import ltn\n",
    "\n",
    "# Assuming necessary imports and LTN setup are done\n",
    "\n",
    "def generate_and_evaluate_axioms(features, feature_importances, model):\n",
    "    \"\"\"\n",
    "    Generate axioms based on feature importances and evaluate their satisfaction level.\n",
    "\n",
    "    Parameters:\n",
    "    - features: The input features for the LTN, a tf.Tensor.\n",
    "    - feature_importances: A dictionary with class labels as keys and lists of feature importances as values.\n",
    "    - model: A trained model that can predict class membership probabilities for given features.\n",
    "\n",
    "    Returns:\n",
    "    - sat_level: The aggregated satisfaction level of all generated axioms.\n",
    "    \"\"\"\n",
    "    normalized_importances = normalize_importances(feature_importances)\n",
    "    \n",
    "    Not = ltn.Wrapper_Connective(ltn.fuzzy_ops.Not_Std())\n",
    "    And = ltn.Wrapper_Connective(ltn.fuzzy_ops.And_Prod())\n",
    "    Or = ltn.Wrapper_Connective(ltn.fuzzy_ops.Or_ProbSum())\n",
    "    Implies = ltn.Wrapper_Connective(ltn.fuzzy_ops.Implies_Reichenbach())\n",
    "    Forall = ltn.Wrapper_Quantifier(ltn.fuzzy_ops.Aggreg_pMeanError(p=2), semantics=\"forall\")\n",
    "    \n",
    "    axioms = []\n",
    "    x = ltn.Variable(\"x\", features)\n",
    "    \n",
    "    for class_label, norm_importances in normalized_importances.items():\n",
    "        # Define a fuzzy logical expression for each class based on feature importances\n",
    "        class_expression = None\n",
    "        for i, importance in enumerate(norm_importances):\n",
    "            feature_pred = ltn.Predicate.Lambda(lambda x, i=i: tf.gather(x, [i], axis=1) * importance)\n",
    "            if class_expression is None:\n",
    "                class_expression = feature_pred(x)\n",
    "            else:\n",
    "                class_expression = And(class_expression, feature_pred(x))\n",
    "        \n",
    "        # Define a predicate indicating class membership based on model prediction\n",
    "        # Assuming model outputs class membership probabilities\n",
    "        class_pred = ltn.Predicate.Lambda(lambda x: model(x)[:, class_label])\n",
    "\n",
    "        # Formulating an implication axiom for each class\n",
    "        class_axiom = Forall(x, Implies(class_expression, class_pred(x)))\n",
    "        axioms.append(class_axiom)\n",
    "    \n",
    "    formula_aggregator = ltn.Wrapper_Formula_Aggregator(ltn.fuzzy_ops.Aggreg_pMeanError(p=2))\n",
    "    sat_level = formula_aggregator(axioms).tensor\n",
    "    \n",
    "    return sat_level\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ltn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
