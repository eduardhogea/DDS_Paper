{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import csv\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "\n",
    "# Third-party library imports\n",
    "import config\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from keras import layers, models, optimizers, callbacks, regularizers, utils\n",
    "from keras.callbacks import EarlyStopping, LearningRateScheduler, ModelCheckpoint, TensorBoard\n",
    "from keras.layers import Dense, Dropout, LSTM, Input\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import to_categorical\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from tqdm import tqdm\n",
    "from numpy import mean\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "# Configurations\n",
    "model_path = config.model_path\n",
    "dataset_path = config.dataset_path\n",
    "PGB_path = config.PGB_path\n",
    "RGB_path = config.RGB_path\n",
    "csv_file = config.csv_file\n",
    "preprocessor_file = config.preprocessor_file\n",
    "train_path = config.train_path\n",
    "val_path = config.val_path\n",
    "chunk_size = config.chunk_size\n",
    "csv_directory = config.csv_directory\n",
    "data_root_folder = config.data_root_folder\n",
    "sequence_length = config.sequence_length\n",
    "sequences_directory = config.sequences_directory\n",
    "num_features = config.num_features\n",
    "processed_bases = config.processed_bases\n",
    "batch_size = config.batch_size\n",
    "epochs = config.epochs\n",
    "patience = config.patience\n",
    "learning_rate = config.learning_rate\n",
    "n_splits = config.n_splits\n",
    "model_save_directory = config.model_save_directory\n",
    "reg_value = config.reg_value\n",
    "num_train_samples = config.num_train_samples\n",
    "num_test_samples = config.num_test_samples\n",
    "reg_type = config.reg_type\n",
    "\n",
    "def reset_random_seeds(seed_value=42):\n",
    "    tf.random.set_seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "\n",
    "# Call this function at the beginning of your script\n",
    "reset_random_seeds()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fault(file_name):\n",
    "    fault_mapping = {\n",
    "        '0Health': 'HEA', '1Chipped': 'CTF', '2Miss': 'MTF', \n",
    "        '3Root': 'RCF', '4Surface': 'SWF', '5Ball': 'BWF', \n",
    "        '6Combination': 'CWF', '7Inner': 'IRF', '8Outer': 'ORF'\n",
    "    }\n",
    "    for key, value in fault_mapping.items():\n",
    "        if key in file_name:\n",
    "            return value\n",
    "    return None\n",
    "\n",
    "def make_csv_writer(csv_file):\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow(['Channel1', 'Channel2', 'Channel3', 'Channel4', 'Channel5', 'Channel6', 'Channel7', 'Channel8', 'Fault'])\n",
    "    return csv_writer\n",
    "\n",
    "def generate_csv(output_directory, root_path, speed, experiment, files, num_train_samples, num_test_samples):\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    train_filename_suffix = f\"{speed}_{experiment}_train\" if experiment else f\"{speed}_train\"\n",
    "    test_filename_suffix = f\"{speed}_{experiment}_test\" if experiment else f\"{speed}_test\"\n",
    "    \n",
    "    train_output_file_path = os.path.join(output_directory, f\"PGB_{train_filename_suffix}.csv\")\n",
    "    test_output_file_path = os.path.join(output_directory, f\"PGB_{test_filename_suffix}.csv\")\n",
    "    \n",
    "    with open(train_output_file_path, 'w', newline='', encoding='utf-8') as train_csvfile, \\\n",
    "        open(test_output_file_path, 'w', newline='', encoding='utf-8') as test_csvfile:\n",
    "        train_csv_writer = make_csv_writer(train_csvfile)\n",
    "        test_csv_writer = make_csv_writer(test_csvfile)\n",
    "        \n",
    "        for file in tqdm(files, desc=f\"Processing {speed} {experiment}\", unit=\"file\"):\n",
    "            fault_type = extract_fault(file)\n",
    "            file_path = os.path.join(root_path, file)\n",
    "            \n",
    "            total_rows = num_train_samples + num_test_samples\n",
    "            data = pd.read_csv(file_path, sep='\\t', header=None, encoding='ISO-8859-1', skiprows=1, nrows=total_rows)\n",
    "            train_samples, test_samples = data.iloc[:num_train_samples, :], data.iloc[num_train_samples:total_rows, :]\n",
    "            \n",
    "            for index, row in train_samples.iterrows():\n",
    "                train_csv_writer.writerow(row[:8].tolist() + [fault_type])\n",
    "            \n",
    "            for index, row in test_samples.iterrows():\n",
    "                test_csv_writer.writerow(row[:8].tolist() + [fault_type])\n",
    "\n",
    "def process_pgb_data(data_root_folder, csv_directory, num_train_samples, num_test_samples):\n",
    "    for root, dirs, files in os.walk(data_root_folder):\n",
    "        parts = root.split(os.sep)\n",
    "        if 'Variable_speed' in parts:\n",
    "            speed = \"Variable_speed\"\n",
    "            experiment_dir = parts[-1]  # Get the last part as the experiment name\n",
    "            exp_files = [f for f in os.listdir(root) if f.endswith('.txt')]\n",
    "            generate_csv(csv_directory, root, speed, experiment_dir, exp_files, num_train_samples, num_test_samples)\n",
    "        elif 'PGB' in parts and files:\n",
    "            speed = parts[-1]  # Last part of 'root' is the speed directory\n",
    "            generate_csv(csv_directory, root, speed, '', files, num_train_samples, num_test_samples)\n",
    "            \n",
    "            \n",
    "def overview_csv_files(directory):\n",
    "    data = []\n",
    "    all_faults = set()\n",
    "\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Check if the CSV is empty (aside from the header)\n",
    "            if df.shape[0] == 0:\n",
    "                # Delete the empty CSV file\n",
    "                os.remove(file_path)\n",
    "                print(f\"Deleted empty file: {file_path}\")\n",
    "                continue  # Skip further processing for this file\n",
    "\n",
    "            num_samples = len(df)\n",
    "            fault_distribution = Counter(df['Fault'])\n",
    "            all_faults.update(fault_distribution.keys())\n",
    "            data.append({'File Name': file, 'Number of Samples': num_samples, **fault_distribution})\n",
    "\n",
    "    if not data:  # If no data has been gathered, exit the function\n",
    "        print(\"No data found.\")\n",
    "        return\n",
    "\n",
    "    overview_df = pd.DataFrame(data)\n",
    "    for fault in all_faults:\n",
    "        if fault not in overview_df.columns:\n",
    "            overview_df[fault] = 0\n",
    "\n",
    "    cols = ['File Name', 'Number of Samples'] + sorted(all_faults)\n",
    "    overview_df = overview_df[cols]\n",
    "    overview_df.fillna(0, inplace=True)\n",
    "    overview_df.loc[:, 'Number of Samples':] = overview_df.loc[:, 'Number of Samples':].astype(int)\n",
    "\n",
    "    overview_df = overview_df.sort_values(by='File Name')\n",
    "    print(overview_df.to_string(index=False))\n",
    "    \n",
    "def load_and_scale_data(csv_path, scaler=None, save_scaler_path=None):\n",
    "    \"\"\"\n",
    "    Loads data from a CSV file, scales the features (excluding the 'Fault' column), \n",
    "    and returns the scaled DataFrame. Optionally saves the scaler model.\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    data = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Separate features and target\n",
    "    features = data.columns[:-1]  # Assuming the last column is the target\n",
    "    X = data[features]\n",
    "    y = data['Fault']\n",
    "\n",
    "    # Apply scaling\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        if save_scaler_path:\n",
    "            joblib.dump(scaler, save_scaler_path)\n",
    "    else:\n",
    "        X_scaled = scaler.transform(X)\n",
    "    \n",
    "    # Combine scaled features with target\n",
    "    scaled_df = pd.DataFrame(X_scaled, columns=features)\n",
    "    scaled_df['Fault'] = y\n",
    "    \n",
    "    return scaled_df\n",
    "\n",
    "def create_sequences(df, sequence_length):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    fault_types = df['Fault'].unique()\n",
    "\n",
    "    for fault in fault_types:\n",
    "        df_fault = df[df['Fault'] == fault]\n",
    "        X = df_fault.drop('Fault', axis=1).values\n",
    "        y = df_fault['Fault'].iloc[0]  # Updated to use iloc for consistency\n",
    "        \n",
    "        for i in range(len(df_fault) - sequence_length + 1):\n",
    "            sequences.append(X[i:i+sequence_length])\n",
    "            labels.append(fault)  # Keep the fault type as is\n",
    "    \n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "\n",
    "def save_sequences(input_directory, output_directory, sequence_length):\n",
    "    \"\"\"\n",
    "    Generates sequences and saves them as NumPy files, one for sequences and one for labels.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_directory: The directory with the original, scaled data files.\n",
    "    - output_directory: The directory where the NumPy sequence files will be saved.\n",
    "    - sequence_length: The number of consecutive samples in each sequence.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    for file_name in tqdm(os.listdir(input_directory), desc=\"Generating sequences\"):\n",
    "        if file_name.endswith('.csv'):\n",
    "            df = pd.read_csv(os.path.join(input_directory, file_name))\n",
    "            sequences, labels = create_sequences(df, sequence_length)\n",
    "            \n",
    "            # File names for sequences and labels\n",
    "            base_name = os.path.splitext(file_name)[0]\n",
    "            sequences_file_path = os.path.join(output_directory, f\"{base_name}_sequences.npy\")\n",
    "            labels_file_path = os.path.join(output_directory, f\"{base_name}_labels.npy\")\n",
    "            \n",
    "            # Save sequences and labels\n",
    "            np.save(sequences_file_path, sequences)\n",
    "            np.save(labels_file_path, labels)\n",
    "            \n",
    "def extract_speed_from_filename(file_name):\n",
    "    \"\"\"\n",
    "    Extracts the speed from the filename.\n",
    "    Returns the numeric speed for fixed speeds, or -1 for variable speeds.\n",
    "    \"\"\"\n",
    "    fixed_speed_match = re.search(r\"PGB_(\\d+)_\", file_name)\n",
    "    if fixed_speed_match:\n",
    "        return int(fixed_speed_match.group(1))\n",
    "    variable_speed_match = re.search(r\"Variable_speed\", file_name)\n",
    "    if variable_speed_match:\n",
    "        return -1  # Special value for variable speeds\n",
    "    return None\n",
    "\n",
    "def add_speed_feature_and_save(input_directory, output_directory, sequence_length):\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    for file_name in tqdm(os.listdir(input_directory), desc=\"Processing files\"):\n",
    "        if file_name.endswith('.csv'):\n",
    "            speed = extract_speed_from_filename(file_name)\n",
    "            df = pd.read_csv(os.path.join(input_directory, file_name))\n",
    "            df['Speed'] = speed  # Add speed as a new column\n",
    "            \n",
    "            sequences, labels = create_sequences(df, sequence_length)\n",
    "            \n",
    "            base_name = os.path.splitext(file_name)[0]\n",
    "            sequences_file_path = os.path.join(output_directory, f\"{base_name}_sequences.npy\")\n",
    "            labels_file_path = os.path.join(output_directory, f\"{base_name}_labels.npy\")\n",
    "            \n",
    "            np.save(sequences_file_path, sequences)\n",
    "            np.save(labels_file_path, labels)\n",
    "            \n",
    "def display_samples(sequences_file_path, labels_file_path, num_samples=1):\n",
    "    \"\"\"\n",
    "    Displays a specified number of samples from the sequences and labels .npy files.\n",
    "    \n",
    "    Parameters:\n",
    "    - sequences_file_path: Path to the .npy file containing sequences.\n",
    "    - labels_file_path: Path to the .npy file containing labels.\n",
    "    - num_samples: Number of samples to display. Default is 5.\n",
    "    \"\"\"\n",
    "    # Load the sequences and labels\n",
    "    sequences = np.load(sequences_file_path)\n",
    "    labels = np.load(labels_file_path)\n",
    "    \n",
    "    # Determine the number of samples to display (cannot exceed the length of the data)\n",
    "    num_samples = min(num_samples, len(sequences))\n",
    "    \n",
    "    # Display the specified number of samples\n",
    "    for i in range(num_samples):\n",
    "        print(f\"Sample {i+1}:\")\n",
    "        print(\"Sequence:\")\n",
    "        print(sequences[i])\n",
    "        print(\"Label:\")\n",
    "        print(labels[i])\n",
    "        print(\"-\" * 50)  # Separator for readability\n",
    "        \n",
    "def memmap_append_and_save(input_directory, output_directory, dataset_type, file_type):\n",
    "    # Update the output file path to indicate pickle format\n",
    "    output_file_path = os.path.join(output_directory, f\"{dataset_type}_merged_{file_type}.pkl\")\n",
    "\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    merged_data = None\n",
    "    current_size = 0\n",
    "\n",
    "    file_names = [fn for fn in os.listdir(input_directory) if fn.endswith(f'_{file_type}.npy') and dataset_type in fn]\n",
    "    for file_name in tqdm(file_names, desc=f\"Merging {dataset_type} {file_type}\"):\n",
    "        path = os.path.join(input_directory, file_name)\n",
    "        data = np.load(path)\n",
    "\n",
    "        # Adjust for both 1D and 2D+ data\n",
    "        new_shape = (current_size + data.shape[0],) + data.shape[1:] if len(data.shape) > 1 else (current_size + data.shape[0],)\n",
    "        if merged_data is None:\n",
    "            # Initially, directly use the loaded data\n",
    "            merged_data = data.copy()\n",
    "        else:\n",
    "            # Concatenate new data\n",
    "            merged_data = np.concatenate((merged_data, data), axis=0)\n",
    "        \n",
    "        current_size += data.shape[0]\n",
    "\n",
    "    # After processing all files, save the merged data using pickle\n",
    "    with open(output_file_path, 'wb') as f:\n",
    "        pickle.dump(merged_data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    print(f\"{dataset_type.capitalize()} {file_type} data merged and saved to {output_file_path} in pickle format\")\n",
    "\n",
    "def merge_npy_files_with_memmap_separated(input_directory, output_directory):\n",
    "    for dataset_type in ['train', 'test']:\n",
    "        for file_type in ['sequences', 'labels']:\n",
    "            memmap_append_and_save(input_directory, output_directory, dataset_type, file_type)\n",
    "            \n",
    "def kfold_cross_validation(X, y, num_folds=5):\n",
    "    input_shape = X.shape[1:]  # Assuming X is (num_samples, time_steps, features)\n",
    "    num_classes = y.shape[1]\n",
    "    kfold = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    fold_no = 1\n",
    "    for train, test in kfold.split(X, y):\n",
    "        print(f\"Training on fold {fold_no}...\")\n",
    "        \n",
    "        model = create_model(input_shape, num_classes, l2_reg=reg_value)\n",
    "        lr_scheduler = LearningRateScheduler(lr_schedule, verbose=0)\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
    "        \n",
    "        model.fit(X[train], y[train], validation_data=(X[test], y[test]),\n",
    "                epochs=epochs, batch_size=batch_size, callbacks=[early_stopping, lr_scheduler], verbose=1)\n",
    "        \n",
    "        fold_no += 1\n",
    "        \n",
    "# version that takes speed by speed\n",
    "\n",
    "def lr_schedule(epoch, lr):\n",
    "    if epoch % 10 == 0 and epoch > 0:\n",
    "        return lr * 0.1\n",
    "    return lr\n",
    "\n",
    "def load_sequences(sequence_file_path, label_file_path):\n",
    "    sequences = np.load(sequence_file_path)\n",
    "    labels = np.load(label_file_path)\n",
    "    encoder = LabelEncoder()\n",
    "    labels_encoded = encoder.fit_transform(labels)\n",
    "    labels_onehot = to_categorical(labels_encoded)\n",
    "    return sequences, labels_onehot\n",
    "\n",
    "# def create_model(input_shape, num_classes):\n",
    "#     model = Sequential([\n",
    "#         LSTM(100, return_sequences=True, input_shape=input_shape),\n",
    "#         Dropout(0.2),\n",
    "#         LSTM(100),\n",
    "#         Dropout(0.2),\n",
    "#         Dense(100, activation='relu'),\n",
    "#         Dense(num_classes, activation='softmax')\n",
    "#     ])\n",
    "#     model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, Dense, Input\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "\n",
    "def create_model(input_shape, num_classes, reg_type='l2', reg_value=0.001, return_logits=False):\n",
    "    if reg_type == 'l2':\n",
    "        regularizer = regularizers.l2(reg_value)\n",
    "    elif reg_type == 'l1':\n",
    "        regularizer = regularizers.l1(reg_value)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid regularizer type. Choose 'l1' or 'l2'.\")\n",
    "\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        LSTM(256, return_sequences=True, kernel_regularizer=regularizer),\n",
    "        Dropout(0.3),\n",
    "        LSTM(128, return_sequences=True, kernel_regularizer=regularizer),\n",
    "        Dropout(0.3),\n",
    "        LSTM(64, kernel_regularizer=regularizer),\n",
    "        Dropout(0.3),\n",
    "        Dense(128, activation='relu', kernel_regularizer=regularizer),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu', kernel_regularizer=regularizer),\n",
    "        Dropout(0.3)\n",
    "    ])\n",
    "    \n",
    "    if return_logits:\n",
    "        # If we want the model to return logits, add a Dense layer without activation\n",
    "        model.add(Dense(num_classes))\n",
    "    else:\n",
    "        # Otherwise, add a Dense layer with softmax activation to return probabilities\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Variable_speed Variable_speed: 0file [00:00, ?file/s]\n",
      "Processing Variable_speed Experiment8: 100%|██████████| 9/9 [00:10<00:00,  1.12s/file]\n",
      "Processing Variable_speed Experiment4: 100%|██████████| 9/9 [00:10<00:00,  1.14s/file]\n",
      "Processing Variable_speed Experiment6: 100%|██████████| 9/9 [00:11<00:00,  1.27s/file]\n",
      "Processing Variable_speed Experiment2: 100%|██████████| 9/9 [00:08<00:00,  1.06file/s]\n",
      "Processing Variable_speed Experiment5: 100%|██████████| 9/9 [00:08<00:00,  1.06file/s]\n",
      "Processing Variable_speed Experiment1: 100%|██████████| 9/9 [00:09<00:00,  1.04s/file]\n",
      "Processing Variable_speed Experiment9: 100%|██████████| 9/9 [00:12<00:00,  1.34s/file]\n",
      "Processing Variable_speed Experiment7: 100%|██████████| 9/9 [00:10<00:00,  1.18s/file]\n",
      "Processing Variable_speed Experiment3: 100%|██████████| 9/9 [00:09<00:00,  1.03s/file]\n",
      "Processing Variable_speed Experiment10: 100%|██████████| 9/9 [00:07<00:00,  1.23file/s]\n",
      "Processing 40_0 : 100%|██████████| 9/9 [00:09<00:00,  1.07s/file]\n",
      "Processing 30_4 : 100%|██████████| 9/9 [00:08<00:00,  1.09file/s]\n",
      "Processing 30_3 : 100%|██████████| 9/9 [00:09<00:00,  1.01s/file]\n",
      "Processing 20_0 : 100%|██████████| 9/9 [00:09<00:00,  1.00s/file]\n",
      "Processing 30_5 : 100%|██████████| 9/9 [00:10<00:00,  1.14s/file]\n",
      "Processing 30_2 : 100%|██████████| 9/9 [00:08<00:00,  1.02file/s]\n",
      "Processing 50_0 : 100%|██████████| 9/9 [00:08<00:00,  1.08file/s]\n",
      "Processing 30_0 : 100%|██████████| 9/9 [00:10<00:00,  1.13s/file]\n",
      "Processing 30_1 : 100%|██████████| 9/9 [00:07<00:00,  1.23file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted empty file: /home/ubuntu/dds_paper/DDS_Paper/data/DDS_Data_SEU/data/csvs/PGB_Variable_speed_Variable_speed_train.csv\n",
      "Deleted empty file: /home/ubuntu/dds_paper/DDS_Paper/data/DDS_Data_SEU/data/csvs/PGB_Variable_speed_Variable_speed_test.csv\n",
      "                                File Name  Number of Samples   BWF   CTF   CWF   HEA   IRF   MTF   ORF   RCF   SWF\n",
      "                        PGB_20_0_test.csv              18000  2000  2000  2000  2000  2000  2000  2000  2000  2000\n",
      "                       PGB_20_0_train.csv              90000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
      "                        PGB_30_0_test.csv              18000  2000  2000  2000  2000  2000  2000  2000  2000  2000\n",
      "                       PGB_30_0_train.csv              90000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
      "                        PGB_30_1_test.csv              18000  2000  2000  2000  2000  2000  2000  2000  2000  2000\n",
      "                       PGB_30_1_train.csv              90000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
      "                        PGB_30_2_test.csv              18000  2000  2000  2000  2000  2000  2000  2000  2000  2000\n",
      "                       PGB_30_2_train.csv              90000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
      "                        PGB_30_3_test.csv              18000  2000  2000  2000  2000  2000  2000  2000  2000  2000\n",
      "                       PGB_30_3_train.csv              90000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
      "                        PGB_30_4_test.csv              18000  2000  2000  2000  2000  2000  2000  2000  2000  2000\n",
      "                       PGB_30_4_train.csv              90000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
      "                        PGB_30_5_test.csv              18000  2000  2000  2000  2000  2000  2000  2000  2000  2000\n",
      "                       PGB_30_5_train.csv              90000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
      "                        PGB_40_0_test.csv              18000  2000  2000  2000  2000  2000  2000  2000  2000  2000\n",
      "                       PGB_40_0_train.csv              90000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
      "                        PGB_50_0_test.csv              18000  2000  2000  2000  2000  2000  2000  2000  2000  2000\n",
      "                       PGB_50_0_train.csv              90000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
      " PGB_Variable_speed_Experiment10_test.csv              18000  2000  2000  2000  2000  2000  2000  2000  2000  2000\n",
      "PGB_Variable_speed_Experiment10_train.csv              90000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
      "  PGB_Variable_speed_Experiment1_test.csv              18000  2000  2000  2000  2000  2000  2000  2000  2000  2000\n",
      " PGB_Variable_speed_Experiment1_train.csv              90000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
      "  PGB_Variable_speed_Experiment2_test.csv              18000  2000  2000  2000  2000  2000  2000  2000  2000  2000\n",
      " PGB_Variable_speed_Experiment2_train.csv              90000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
      "  PGB_Variable_speed_Experiment3_test.csv              18000  2000  2000  2000  2000  2000  2000  2000  2000  2000\n",
      " PGB_Variable_speed_Experiment3_train.csv              90000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
      "  PGB_Variable_speed_Experiment4_test.csv              18000  2000  2000  2000  2000  2000  2000  2000  2000  2000\n",
      " PGB_Variable_speed_Experiment4_train.csv              90000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
      "  PGB_Variable_speed_Experiment5_test.csv              18000  2000  2000  2000  2000  2000  2000  2000  2000  2000\n",
      " PGB_Variable_speed_Experiment5_train.csv              90000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
      "  PGB_Variable_speed_Experiment6_test.csv              18000  2000  2000  2000  2000  2000  2000  2000  2000  2000\n",
      " PGB_Variable_speed_Experiment6_train.csv              90000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
      "  PGB_Variable_speed_Experiment7_test.csv              18000  2000  2000  2000  2000  2000  2000  2000  2000  2000\n",
      " PGB_Variable_speed_Experiment7_train.csv              90000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
      "  PGB_Variable_speed_Experiment8_test.csv              18000  2000  2000  2000  2000  2000  2000  2000  2000  2000\n",
      " PGB_Variable_speed_Experiment8_train.csv              90000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n",
      "  PGB_Variable_speed_Experiment9_test.csv              18000  2000  2000  2000  2000  2000  2000  2000  2000  2000\n",
      " PGB_Variable_speed_Experiment9_train.csv              90000 10000 10000 10000 10000 10000 10000 10000 10000 10000\n"
     ]
    }
   ],
   "source": [
    "process_pgb_data(data_root_folder, csv_directory, num_train_samples, num_test_samples)\n",
    "overview_csv_files(csv_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating sequences: 100%|██████████| 58/58 [00:16<00:00,  3.59it/s]\n"
     ]
    }
   ],
   "source": [
    "# scale data\n",
    "\n",
    "# Iterate over your dataset files\n",
    "for root, dirs, files in os.walk(csv_directory):\n",
    "    for file in sorted(files):\n",
    "        if file.endswith('.csv') and not file.endswith('_scaled.csv'):  # Process only unscaled .csv files\n",
    "            csv_path = os.path.join(root, file)\n",
    "            if 'train' in file:\n",
    "                # Handle training data\n",
    "                scaler_path = os.path.join(root, 'scaler_' + file.replace('.csv', '.joblib'))\n",
    "                scaled_train_df = load_and_scale_data(csv_path, save_scaler_path=scaler_path)\n",
    "                # Save the scaled training data\n",
    "                scaled_csv_path = csv_path.replace('.csv', '_scaled.csv')\n",
    "                scaled_train_df.to_csv(scaled_csv_path, index=False)\n",
    "            elif 'test' in file:\n",
    "                # Handle testing data\n",
    "                scaler_path = os.path.join(root, 'scaler_' + file.replace('_test.csv', '_train.joblib'))\n",
    "                scaler = joblib.load(scaler_path) if os.path.exists(scaler_path) else None\n",
    "                scaled_test_df = load_and_scale_data(csv_path, scaler=scaler)\n",
    "                # Save the scaled testing data\n",
    "                scaled_csv_path = csv_path.replace('.csv', '_scaled.csv')\n",
    "                scaled_test_df.to_csv(scaled_csv_path, index=False)\n",
    "\n",
    "            # Delete the original unscaled .csv file\n",
    "            os.remove(csv_path)\n",
    "            \n",
    "#create sequences\n",
    "save_sequences(csv_directory, sequences_directory, sequence_length)\n",
    "\n",
    "#add_speed_feature_and_save(csv_directory, sequences_directory, sequence_length)\n",
    "\n",
    "#merge_npy_files_with_memmap_separated(sequences_directory, sequences_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Training fold </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">/</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">4</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> for PGB_20_0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mTraining fold \u001b[0m\u001b[1;32m1\u001b[0m\u001b[1;32m/\u001b[0m\u001b[1;32m4\u001b[0m\u001b[1;32m for PGB_20_0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "158/158 [==============================] - 69s 399ms/step - loss: 1.1527 - accuracy: 0.7005 - val_loss: 6.6784 - val_accuracy: 0.2578 - lr: 0.0010\n",
      "Epoch 2/5\n",
      "158/158 [==============================] - 61s 389ms/step - loss: 0.5089 - accuracy: 0.8902 - val_loss: 7.7021 - val_accuracy: 0.2578 - lr: 0.0010\n",
      "Epoch 3/5\n",
      "158/158 [==============================] - 60s 379ms/step - loss: 0.3262 - accuracy: 0.9437 - val_loss: 5.9901 - val_accuracy: 0.2530 - lr: 0.0010\n",
      "Epoch 4/5\n",
      "158/158 [==============================] - 60s 378ms/step - loss: 0.2320 - accuracy: 0.9692 - val_loss: 5.2024 - val_accuracy: 0.2489 - lr: 0.0010\n",
      "Epoch 5/5\n",
      "158/158 [==============================] - 63s 400ms/step - loss: 0.2034 - accuracy: 0.9721 - val_loss: 5.8759 - val_accuracy: 0.2488 - lr: 0.0010\n",
      "840/840 [==============================] - 40s 46ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> Accuracy: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2488</span>, Precision: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1250</span>, Recall: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1206</span>, F1: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1228</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold \u001b[1;36m1\u001b[0m Accuracy: \u001b[1;36m0.2488\u001b[0m, Precision: \u001b[1;36m0.1250\u001b[0m, Recall: \u001b[1;36m0.1206\u001b[0m, F1: \u001b[1;36m0.1228\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Training fold </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">/</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">4</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> for PGB_20_0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mTraining fold \u001b[0m\u001b[1;32m2\u001b[0m\u001b[1;32m/\u001b[0m\u001b[1;32m4\u001b[0m\u001b[1;32m for PGB_20_0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "158/158 [==============================] - 69s 404ms/step - loss: 1.6209 - accuracy: 0.4694 - val_loss: 3.2855 - val_accuracy: 0.2235 - lr: 0.0010\n",
      "Epoch 2/5\n",
      "158/158 [==============================] - 62s 394ms/step - loss: 0.7867 - accuracy: 0.7389 - val_loss: 2.9304 - val_accuracy: 0.3696 - lr: 0.0010\n",
      "Epoch 3/5\n",
      "158/158 [==============================] - 65s 411ms/step - loss: 0.4925 - accuracy: 0.8359 - val_loss: 4.3556 - val_accuracy: 0.2921 - lr: 0.0010\n",
      "Epoch 4/5\n",
      "158/158 [==============================] - 61s 387ms/step - loss: 0.4026 - accuracy: 0.8576 - val_loss: 2.4463 - val_accuracy: 0.5692 - lr: 0.0010\n",
      "Epoch 5/5\n",
      "158/158 [==============================] - 62s 395ms/step - loss: 0.3296 - accuracy: 0.8914 - val_loss: 1.8718 - val_accuracy: 0.5623 - lr: 0.0010\n",
      "840/840 [==============================] - 41s 47ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> Accuracy: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5623</span>, Precision: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4296</span>, Recall: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2674</span>, F1: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3282</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold \u001b[1;36m2\u001b[0m Accuracy: \u001b[1;36m0.5623\u001b[0m, Precision: \u001b[1;36m0.4296\u001b[0m, Recall: \u001b[1;36m0.2674\u001b[0m, F1: \u001b[1;36m0.3282\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Training fold </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">3</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">/</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">4</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> for PGB_20_0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mTraining fold \u001b[0m\u001b[1;32m3\u001b[0m\u001b[1;32m/\u001b[0m\u001b[1;32m4\u001b[0m\u001b[1;32m for PGB_20_0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "158/158 [==============================] - 69s 400ms/step - loss: 1.6678 - accuracy: 0.4358 - val_loss: 2.8625 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 2/5\n",
      "158/158 [==============================] - 64s 403ms/step - loss: 1.4000 - accuracy: 0.4452 - val_loss: 2.8573 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 3/5\n",
      "158/158 [==============================] - 62s 393ms/step - loss: 1.3234 - accuracy: 0.4441 - val_loss: 2.4960 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 4/5\n",
      "158/158 [==============================] - 60s 379ms/step - loss: 1.2792 - accuracy: 0.4440 - val_loss: 2.2721 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 5/5\n",
      "158/158 [==============================] - 60s 381ms/step - loss: 1.2351 - accuracy: 0.4578 - val_loss: 2.9261 - val_accuracy: 0.0119 - lr: 0.0010\n",
      "840/840 [==============================] - 40s 46ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> Accuracy: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0119</span>, Precision: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1667</span>, Recall: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0053</span>, F1: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0104</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold \u001b[1;36m3\u001b[0m Accuracy: \u001b[1;36m0.0119\u001b[0m, Precision: \u001b[1;36m0.1667\u001b[0m, Recall: \u001b[1;36m0.0053\u001b[0m, F1: \u001b[1;36m0.0104\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Training fold </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">4</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">/</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">4</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> for PGB_20_0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mTraining fold \u001b[0m\u001b[1;32m4\u001b[0m\u001b[1;32m/\u001b[0m\u001b[1;32m4\u001b[0m\u001b[1;32m for PGB_20_0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "158/158 [==============================] - 67s 391ms/step - loss: 1.3451 - accuracy: 0.5941 - val_loss: 1.5197 - val_accuracy: 0.4160 - lr: 0.0010\n",
      "Epoch 2/5\n",
      "158/158 [==============================] - 59s 376ms/step - loss: 0.7163 - accuracy: 0.7845 - val_loss: 1.5355 - val_accuracy: 0.5214 - lr: 0.0010\n",
      "Epoch 3/5\n",
      "158/158 [==============================] - 61s 388ms/step - loss: 0.5352 - accuracy: 0.8412 - val_loss: 1.6588 - val_accuracy: 0.7625 - lr: 0.0010\n",
      "Epoch 4/5\n",
      "158/158 [==============================] - 65s 409ms/step - loss: 0.3765 - accuracy: 0.9136 - val_loss: 2.8388 - val_accuracy: 0.6944 - lr: 0.0010\n",
      "Epoch 5/5\n",
      "158/158 [==============================] - 58s 365ms/step - loss: 0.2727 - accuracy: 0.9519 - val_loss: 3.4850 - val_accuracy: 0.5797 - lr: 0.0010\n",
      "840/840 [==============================] - 41s 47ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> Accuracy: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5797</span>, Precision: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5333</span>, Recall: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5884</span>, F1: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5296</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold \u001b[1;36m4\u001b[0m Accuracy: \u001b[1;36m0.5797\u001b[0m, Precision: \u001b[1;36m0.5333\u001b[0m, Recall: \u001b[1;36m0.5884\u001b[0m, F1: \u001b[1;36m0.5296\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Average metrics for PGB_20_0 across </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">4</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> folds:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mAverage metrics for PGB_20_0 across \u001b[0m\u001b[1;35m4\u001b[0m\u001b[1;35m folds:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Average Accuracy: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3507</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Average Accuracy: \u001b[1;36m0.3507\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Average Precision: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3136</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Average Precision: \u001b[1;36m0.3136\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Average Recall: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2454</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Average Recall: \u001b[1;36m0.2454\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Average F1: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2477</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Average F1: \u001b[1;36m0.2477\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Model for PGB_20_0 saved.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mModel for PGB_20_0 saved.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Overall Averages Across All File Pairs:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mOverall Averages Across All File Pairs:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Overall Average Accuracy: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3507</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Overall Average Accuracy: \u001b[1;36m0.3507\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Overall Average Precision: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3136</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Overall Average Precision: \u001b[1;36m0.3136\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Overall Average Recall: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2454</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Overall Average Recall: \u001b[1;36m0.2454\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Overall Average F1: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2477</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Overall Average F1: \u001b[1;36m0.2477\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "kf = KFold(n_splits=n_splits, shuffle=False)\n",
    "counter = 0\n",
    "console = Console()\n",
    "\n",
    "# Placeholder for processed base names and metrics\n",
    "processed_bases = set()\n",
    "metrics_summary = []\n",
    "\n",
    "for file in sorted(os.listdir(sequences_directory)):\n",
    "    if \"_train_scaled_sequences.npy\" in file:\n",
    "        base_name = file.replace(\"_train_scaled_sequences.npy\", \"\")\n",
    "        if base_name in processed_bases:\n",
    "            continue\n",
    "        counter+=1\n",
    "        \n",
    "        model_filepath = os.path.join(model_save_directory, f\"saved.keras\")\n",
    "        \n",
    "            \n",
    "            \n",
    "        # Load sequences and labels\n",
    "        train_sequence_file_path = os.path.join(sequences_directory, f\"{base_name}_train_scaled_sequences.npy\")\n",
    "        train_label_file_path = os.path.join(sequences_directory, f\"{base_name}_train_scaled_labels.npy\")\n",
    "        X_train, y_train = load_sequences(train_sequence_file_path, train_label_file_path)\n",
    "\n",
    "        # Assuming the existence of a test set (adjust if necessary)\n",
    "        test_sequence_file_path = os.path.join(sequences_directory, f\"{base_name}_test_scaled_sequences.npy\")\n",
    "        test_label_file_path = os.path.join(sequences_directory, f\"{base_name}_test_scaled_labels.npy\")\n",
    "        X_test, y_test = load_sequences(test_sequence_file_path, test_label_file_path)\n",
    "\n",
    "        # Merge for cross-validation\n",
    "        X = np.concatenate((X_train, X_test), axis=0)\n",
    "        y = np.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "        num_classes = y_train.shape[1]\n",
    "        input_shape = (sequence_length, num_features)\n",
    "        fold_metrics = []\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "            console.print(f\"[bold green]Training fold {fold + 1}/{n_splits} for {base_name}[/]\")\n",
    "            X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "            y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
    "\n",
    "            model = create_model(input_shape, num_classes, reg_type = reg_type, reg_value=reg_value)\n",
    "            lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
    "\n",
    "            # Add ModelCheckpoint and TensorBoard for improved monitoring and model saving\n",
    "            model_filepath = os.path.join(model_save_directory, f\"model_{base_name}_fold_{fold+1}.keras\")\n",
    "            #checkpoint = ModelCheckpoint(model_filepath, save_best_only=True, monitor='val_loss')\n",
    "            tensorboard = TensorBoard(log_dir=f\"./logs/{base_name}_fold_{fold+1}\")\n",
    "\n",
    "\n",
    "\n",
    "            #history = model.fit(X_train_fold, y_train_fold, validation_data=(X_val_fold, y_val_fold),\n",
    "                                #epochs=epochs, batch_size=batch_size, callbacks=[early_stopping, lr_scheduler, checkpoint, tensorboard], verbose=1)\n",
    "\n",
    "            history = model.fit(X_train_fold, y_train_fold, validation_data=(X_val_fold, y_val_fold),\n",
    "                                epochs=epochs, batch_size=batch_size, callbacks=[early_stopping, lr_scheduler, tensorboard], verbose=1)\n",
    "\n",
    "            \n",
    "            # Assuming your model outputs softmax probabilities, adjust as necessary\n",
    "            y_val_pred = model.predict(X_val_fold)\n",
    "            y_val_pred_classes = np.argmax(y_val_pred, axis=1)\n",
    "            y_val_true_classes = np.argmax(y_val_fold, axis=1)\n",
    "\n",
    "            # Calculate and store metrics for this fold\n",
    "            accuracy = accuracy_score(y_val_true_classes, y_val_pred_classes)\n",
    "            precision = precision_score(y_val_true_classes, y_val_pred_classes, average='macro', zero_division=0)\n",
    "            recall = recall_score(y_val_true_classes, y_val_pred_classes, average='macro', zero_division=0)\n",
    "            f1 = f1_score(y_val_true_classes, y_val_pred_classes, average='macro')\n",
    "            fold_metrics.append((accuracy, precision, recall, f1))\n",
    "\n",
    "            console.print(f\"Fold {fold+1} Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "            \n",
    "\n",
    "            \n",
    "                # After processing all folds for the current CSV pair\n",
    "        if fold_metrics:\n",
    "            # Calculate the average of each metric across all folds\n",
    "            avg_accuracy = mean([metric[0] for metric in fold_metrics])\n",
    "            avg_precision = mean([metric[1] for metric in fold_metrics])\n",
    "            avg_recall = mean([metric[2] for metric in fold_metrics])\n",
    "            avg_f1 = mean([metric[3] for metric in fold_metrics])\n",
    "\n",
    "            # Append averaged metrics to the metrics_summary for overall analysis if needed\n",
    "            metrics_summary.append((base_name, avg_accuracy, avg_precision, avg_recall, avg_f1))\n",
    "\n",
    "            # Print the averages\n",
    "            console.print(f\"[bold magenta]Average metrics for {base_name} across {n_splits} folds:[/]\")\n",
    "            console.print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
    "            console.print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "            console.print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "            console.print(f\"Average F1: {avg_f1:.4f}\\n\")\n",
    "        \n",
    "\n",
    "        \n",
    "    if counter!=0:\n",
    "        break\n",
    "\n",
    "\n",
    "console.print(f\"[bold blue]Model for {base_name} saved.[/]\")\n",
    "# Optionally, after all file pairs have been processed, print a summary of averages across all file pairs\n",
    "console.print(\"[bold blue]Overall Averages Across All File Pairs:[/]\")\n",
    "overall_avg_accuracy = mean([metrics[1] for metrics in metrics_summary])\n",
    "overall_avg_precision = mean([metrics[2] for metrics in metrics_summary])\n",
    "overall_avg_recall = mean([metrics[3] for metrics in metrics_summary])\n",
    "overall_avg_f1 = mean([metrics[4] for metrics in metrics_summary])\n",
    "\n",
    "console.print(f\"Overall Average Accuracy: {overall_avg_accuracy:.4f}\")\n",
    "console.print(f\"Overall Average Precision: {overall_avg_precision:.4f}\")\n",
    "console.print(f\"Overall Average Recall: {overall_avg_recall:.4f}\")\n",
    "console.print(f\"Overall Average F1: {overall_avg_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Model for PGB_20_0 saved.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mModel for PGB_20_0 saved.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save the model at the end\n",
    "model.save(model_path)\n",
    "console.print(f\"[bold blue]Model for {base_name} saved.[/]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Training fold </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">/</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> for PGB_20_0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mTraining fold \u001b[0m\u001b[1;32m1\u001b[0m\u001b[1;32m/\u001b[0m\u001b[1;32m1\u001b[0m\u001b[1;32m for PGB_20_0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in y_val (one-hot encoded):\n",
      "[9971. 9971. 6928.    0.    0.    0.    0.    0.    0.]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Training fold </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">/</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> for PGB_20_0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mTraining fold \u001b[0m\u001b[1;32m2\u001b[0m\u001b[1;32m/\u001b[0m\u001b[1;32m1\u001b[0m\u001b[1;32m for PGB_20_0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in y_val (one-hot encoded):\n",
      "[   0.    0. 3043. 9971.    0. 9971.    0.    0. 3885.]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Training fold </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">3</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">/</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> for PGB_20_0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mTraining fold \u001b[0m\u001b[1;32m3\u001b[0m\u001b[1;32m/\u001b[0m\u001b[1;32m1\u001b[0m\u001b[1;32m for PGB_20_0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in y_val (one-hot encoded):\n",
      "[   0.    0.    0.    0. 9971.    0.  841. 9971. 6086.]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Training fold </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">4</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">/</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> for PGB_20_0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mTraining fold \u001b[0m\u001b[1;32m4\u001b[0m\u001b[1;32m/\u001b[0m\u001b[1;32m1\u001b[0m\u001b[1;32m for PGB_20_0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in y_val (one-hot encoded):\n",
      "[ 1971.  1971.  1971.  1971.  1971.  1971. 11101.  1971.  1971.]\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=4, shuffle=False)\n",
    "counter = 0\n",
    "console = Console()\n",
    "\n",
    "# Placeholder for processed base names and metrics\n",
    "processed_bases = set()\n",
    "metrics_summary = []\n",
    "\n",
    "for file in sorted(os.listdir(sequences_directory)):\n",
    "    if \"_train_scaled_sequences.npy\" in file:\n",
    "        base_name = file.replace(\"_train_scaled_sequences.npy\", \"\")\n",
    "        if base_name in processed_bases:\n",
    "            continue\n",
    "        counter+=1\n",
    "        \n",
    "        model_filepath = os.path.join(model_save_directory, f\"saved.keras\")\n",
    "        \n",
    "            \n",
    "            \n",
    "        # Load sequences and labels\n",
    "        train_sequence_file_path = os.path.join(sequences_directory, f\"{base_name}_train_scaled_sequences.npy\")\n",
    "        train_label_file_path = os.path.join(sequences_directory, f\"{base_name}_train_scaled_labels.npy\")\n",
    "        X_train, y_train = load_sequences(train_sequence_file_path, train_label_file_path)\n",
    "\n",
    "        # Assuming the existence of a test set (adjust if necessary)\n",
    "        test_sequence_file_path = os.path.join(sequences_directory, f\"{base_name}_test_scaled_sequences.npy\")\n",
    "        test_label_file_path = os.path.join(sequences_directory, f\"{base_name}_test_scaled_labels.npy\")\n",
    "        X_test, y_test = load_sequences(test_sequence_file_path, test_label_file_path)\n",
    "\n",
    "        # Merge for cross-validation\n",
    "        X = np.concatenate((X_train, X_test), axis=0)\n",
    "        y = np.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "        num_classes = y_train.shape[1]\n",
    "        input_shape = (sequence_length, num_features)\n",
    "        fold_metrics = []\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "            console.print(f\"[bold green]Training fold {fold + 1}/{n_splits} for {base_name}[/]\")\n",
    "            X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "            y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
    "            if len(y_val_fold.shape) > 1 and y_val_fold.shape[1] > 1:\n",
    "                print(\"Class distribution in y_val (one-hot encoded):\")\n",
    "                class_counts = np.sum(y_val_fold, axis=0)\n",
    "                print(class_counts)\n",
    "\n",
    "    if counter !=0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 48ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Feature Importance:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 45ms/step\n",
      "16/16 [==============================] - 1s 47ms/step\n",
      "16/16 [==============================] - 1s 44ms/step\n",
      "16/16 [==============================] - 1s 45ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Feature Importance:  12%|█▎        | 1/8 [00:03<00:22,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 46ms/step\n",
      "16/16 [==============================] - 1s 43ms/step\n",
      "16/16 [==============================] - 1s 43ms/step\n",
      "16/16 [==============================] - 1s 41ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Feature Importance:  25%|██▌       | 2/8 [00:06<00:19,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 43ms/step\n",
      "16/16 [==============================] - 1s 44ms/step\n",
      "16/16 [==============================] - 1s 45ms/step\n",
      "16/16 [==============================] - 1s 43ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Feature Importance:  38%|███▊      | 3/8 [00:10<00:17,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 45ms/step\n",
      "16/16 [==============================] - 1s 46ms/step\n",
      "16/16 [==============================] - 1s 46ms/step\n",
      "16/16 [==============================] - 1s 45ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Feature Importance:  50%|█████     | 4/8 [00:13<00:13,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 46ms/step\n",
      "16/16 [==============================] - 1s 44ms/step\n",
      "16/16 [==============================] - 1s 47ms/step\n",
      "16/16 [==============================] - 1s 46ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Feature Importance:  62%|██████▎   | 5/8 [00:16<00:10,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 45ms/step\n",
      "16/16 [==============================] - 1s 45ms/step\n",
      "16/16 [==============================] - 1s 48ms/step\n",
      "16/16 [==============================] - 1s 45ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Feature Importance:  75%|███████▌  | 6/8 [00:20<00:06,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 43ms/step\n",
      "16/16 [==============================] - 1s 41ms/step\n",
      "16/16 [==============================] - 1s 41ms/step\n",
      "16/16 [==============================] - 1s 45ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Feature Importance:  88%|████████▊ | 7/8 [00:23<00:03,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 42ms/step\n",
      "16/16 [==============================] - 1s 42ms/step\n",
      "16/16 [==============================] - 1s 47ms/step\n",
      "16/16 [==============================] - 1s 44ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Feature Importance: 100%|██████████| 8/8 [00:26<00:00,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 Feature Importances: [-0.04807692307692308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Class 1 Feature Importances: [-0.00625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.018750000000000003]\n",
      "Class 2 Feature Importances: [0.9375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Class 3 Feature Importances: [0.1544117647058824, 0.12500000000000003, 0.01470588235294118, 0.08088235294117649, 0.13970588235294118, 0.23529411764705882, 0.02205882352941177, 0.02205882352941177]\n",
      "Class 4 Feature Importances: [0.125, 0.1323529411764706, -0.07352941176470587, 0.05882352941176471, 0.10294117647058823, 0.10294117647058823, 0.01470588235294118, 0.05147058823529412]\n",
      "Class 5 Feature Importances: [0.9210526315789475, 0.0, 0.006578947368421045, 0.0, 0.0, 0.03947368421052633, 0.08552631578947373, 0.07894736842105265]\n",
      "Class 6 Feature Importances: [0.424468085106383, 0.20212765957446804, 0.08085106382978724, 0.17872340425531913, 0.15638297872340426, 0.3457446808510638, 0.03936170212765955, 0.061702127659574446]\n",
      "Class 7 Feature Importances: [0.7569444444444444, 0.020833333333333315, 0.0, -0.06250000000000003, -0.04861111111111113, 0.07638888888888881, -0.006944444444444475, 0.055555555555555525]\n",
      "Class 8 Feature Importances: [0.4913793103448276, 0.103448275862069, 0.017241379310344862, 0.017241379310344862, -0.008620689655172376, 0.31034482758620696, -0.017241379310344807, -0.02586206896551721]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def calculate_class_accuracies(predictions, true_labels):\n",
    "    pred_labels = np.argmax(predictions, axis=1)\n",
    "    true_labels = np.argmax(true_labels, axis=1)\n",
    "    \n",
    "    # Initialize a dictionary to store accuracy for each class present in true_labels\n",
    "    class_accuracies = {}\n",
    "    \n",
    "    for class_index in np.unique(true_labels):  # Loop only through classes present in true_labels\n",
    "        class_mask = true_labels == class_index\n",
    "        \n",
    "        # Calculate accuracy for the current class\n",
    "        class_accuracies[class_index] = accuracy_score(true_labels[class_mask], pred_labels[class_mask])\n",
    "    \n",
    "    return class_accuracies\n",
    "\n",
    "def permutation_importance_per_class(model, X_val, y_val, n_repeats=10, n_samples=None):\n",
    "    n_samples = n_samples if n_samples is not None else X_val.shape[0]\n",
    "    random_indices = np.random.choice(X_val.shape[0], size=n_samples, replace=False)\n",
    "    X_val_subset = X_val[random_indices]\n",
    "    y_val_subset = y_val[random_indices]\n",
    "    \n",
    "    # Get baseline class-specific accuracies\n",
    "    baseline_predictions = model.predict(X_val_subset)\n",
    "    baseline_class_accuracies = calculate_class_accuracies(baseline_predictions, y_val_subset)\n",
    "    \n",
    "    # Prepare storage for importances, using a dictionary to accommodate variable class presence\n",
    "    feature_importances = {class_index: np.zeros((X_val.shape[2], n_repeats)) for class_index in baseline_class_accuracies.keys()}\n",
    "    \n",
    "    for feature_index in tqdm(range(X_val.shape[2]), desc='Calculating Feature Importance'):\n",
    "        for n in range(n_repeats):\n",
    "            saved_feature = X_val_subset[:, :, feature_index].copy()\n",
    "            np.random.shuffle(X_val_subset[:, :, feature_index])\n",
    "            \n",
    "            permuted_predictions = model.predict(X_val_subset)\n",
    "            permuted_class_accuracies = calculate_class_accuracies(permuted_predictions, y_val_subset)\n",
    "            \n",
    "            for class_index in baseline_class_accuracies.keys():\n",
    "                feature_importances[class_index][feature_index, n] = baseline_class_accuracies[class_index] - permuted_class_accuracies.get(class_index, 0)\n",
    "            \n",
    "            X_val_subset[:, :, feature_index] = saved_feature\n",
    "    \n",
    "    # Average the importance scores across repeats and prepare formatted output\n",
    "    average_importances = {class_index: importances.mean(axis=1) for class_index, importances in feature_importances.items()}\n",
    "    \n",
    "    # Format the output\n",
    "    formatted_importances = {f\"Class {class_index}\": importance.tolist() for class_index, importance in average_importances.items()}\n",
    "    return formatted_importances\n",
    "\n",
    "# Example usage\n",
    "n_samples = 500 if X_val_fold.shape[0] > 1000 else X_val_fold.shape[0]\n",
    "class_importances = permutation_importance_per_class(model, X_val_fold, y_val_fold, n_repeats=4, n_samples=n_samples)\n",
    "\n",
    "for class_id, importances in class_importances.items():\n",
    "    print(f\"{class_id} Feature Importances:\", importances)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ltn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
