{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import tensorflow as tf\n",
    "\n",
    "dataset_path = 'DDS_Data_SEU'\n",
    "\n",
    "PGB_path = '/Users/eduard.hogea/Documents/Facultate/DDS_Paper/DDS_Data_SEU/PGB'\n",
    "RGB_path = '/Users/eduard.hogea/Documents/Facultate/DDS_Paper/DDS_Data_SEU/RGB'\n",
    "\n",
    "# Specify the CSV file path\n",
    "csv_file = '/Volumes/DataBoi/DDS_paper/data_robust.csv'\n",
    "\n",
    "np.random.seed(45)\n",
    "\n",
    "# Set the chunk size for reading the CSV\n",
    "chunk_size = 100000  # Adjust the chunk size according to your memory limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fault(file_name):\n",
    "    match = re.search(r'\\d+', file_name)\n",
    "    if match:\n",
    "        return int(match.group(0)[0])  # Extract the first digit\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def process_files_to_csv(data_folders, output_file):\n",
    "    # Check if the file already exists\n",
    "    if os.path.isfile(output_file):\n",
    "        print(f\"File {output_file} already exists. Skipping processing.\")\n",
    "        return\n",
    "\n",
    "    total_files = sum([len(files) for data_folder in data_folders for r, d, files in os.walk(data_folder)])\n",
    "\n",
    "    with open(output_file, 'w', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow(['Channel1', 'Channel2', 'Channel3', 'Channel4', 'Channel5', 'Channel6', 'Channel7', 'Channel8', 'Speed', 'Type', 'Fault'])\n",
    "\n",
    "        with tqdm(total=total_files, desc=\"Processing files\", unit=\"file\") as pbar:\n",
    "            for data_folder in data_folders:\n",
    "                for root, dirs, files in os.walk(data_folder):\n",
    "                    if '.ipynb_checkpoints' in root:\n",
    "                        continue  # Skip .ipynb_checkpoints folders\n",
    "                    for file in files:\n",
    "                        if file.endswith('.txt'):\n",
    "                            file_path = os.path.join(root, file)\n",
    "                            path_parts = file_path.split('/')\n",
    "                            variable_speed = 'Variable_speed' in file_path\n",
    "                            type_index = -4 if variable_speed else -3\n",
    "                            type = path_parts[type_index] if path_parts[type_index] in ['PGB', 'RGB'] else None\n",
    "                            if type is not None:\n",
    "                                speed_index = -3 if variable_speed else -2\n",
    "                                speed = path_parts[speed_index]\n",
    "                                fault = extract_fault(file)\n",
    "\n",
    "                                data = pd.read_csv(file_path, sep='\\t', encoding='ISO-8859-1')\n",
    "                                reshaped_data = data.values[:, :]\n",
    "\n",
    "                                for row_data in tqdm(reshaped_data, desc=\"Processing rows\", unit=\"row\", leave=False):\n",
    "                                    row = row_data.tolist() + [speed, type, fault]\n",
    "                                    csv_writer.writerow(row)\n",
    "                            pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /Volumes/DataBoi/DDS_paper/data_robust.csv already exists. Skipping processing.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "process_files_to_csv([PGB_path, RGB_path], '/Volumes/DataBoi/DDS_paper/data_robust.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize a dictionary to store the fault counts\n",
    "fault_counts = {}\n",
    "\n",
    "# Iterate through the CSV file using chunksize\n",
    "with tqdm(total=1, unit='chunk', desc='Processing CSV') as pbar:\n",
    "    for chunk in pd.read_csv(csv_file, chunksize=chunk_size):\n",
    "        \n",
    "        #print(chunk)\n",
    "        # Assuming there is a column named 'fault' in the CSV representing the fault type\n",
    "        fault_column = 'Fault'\n",
    "\n",
    "        # Count the occurrences of each fault in the current chunk\n",
    "        fault_chunk_counts = chunk[fault_column].value_counts()\n",
    "\n",
    "        # Aggregate the counts with the overall fault_counts dictionary\n",
    "        for fault, count in fault_chunk_counts.items():\n",
    "            fault_counts[fault] = fault_counts.get(fault, 0) + count\n",
    "\n",
    "        pbar.update()\n",
    "\n",
    "# Print the fault counts\n",
    "for fault, count in fault_counts.items():\n",
    "    print(f\"Fault: {fault}, Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize dictionaries to store the counts\n",
    "speed_counts = {}\n",
    "type_counts = {}\n",
    "\n",
    "# Iterate through the CSV file using chunksize\n",
    "with tqdm(total=1, unit='chunk', desc='Processing CSV') as pbar:\n",
    "    for chunk in pd.read_csv(csv_file, chunksize=chunk_size):\n",
    "        # Assuming there is a column named 'Speed' in the CSV representing the speed values\n",
    "        \n",
    "        speed_column = 'Speed'\n",
    "\n",
    "        # Count the occurrences of each speed value in the current chunk\n",
    "        speed_chunk_counts = chunk[speed_column].value_counts()\n",
    "\n",
    "        # Aggregate the counts with the overall speed_counts dictionary\n",
    "        for speed, count in speed_chunk_counts.items():\n",
    "            speed_counts[speed] = speed_counts.get(speed, 0) + count\n",
    "\n",
    "        # Assuming there is a column named 'Type' in the CSV representing the types\n",
    "        type_column = 'Type'\n",
    "\n",
    "        # Count the occurrences of each type in the current chunk\n",
    "        type_chunk_counts = chunk[type_column].value_counts()\n",
    "\n",
    "        # Aggregate the counts with the overall type_counts dictionary\n",
    "        for typ, count in type_chunk_counts.items():\n",
    "            type_counts[typ] = type_counts.get(typ, 0) + count\n",
    "\n",
    "        pbar.update()\n",
    "\n",
    "# Print the speed counts\n",
    "print(\"Speed Counts:\")\n",
    "for speed, count in speed_counts.items():\n",
    "    print(f\"Speed: {speed}, Count: {count}\")\n",
    "\n",
    "# Print the type counts\n",
    "print(\"Type Counts:\")\n",
    "for typ, count in type_counts.items():\n",
    "    print(f\"Type: {typ}, Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the numeric and categorical features\n",
    "numerical_features = ['Channel1', 'Channel2', 'Channel3', 'Channel4', 'Channel5', 'Channel6', 'Channel7', 'Channel8']\n",
    "categorical_features = ['Speed', 'Type']\n",
    "\n",
    "# initialize the scalers\n",
    "scaler = StandardScaler()\n",
    "encoder = OneHotEncoder(categories=[['20_0','30_0','30_1','30_2', '30_3','30_4','30_5','40_0','50_0', 'Variable_speed'], ['PGB', 'RGB']])  # specify the unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', scaler, numerical_features),\n",
    "        ('cat', encoder, categorical_features)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the total size of the file in bytes\n",
    "total_size = os.path.getsize(csv_file)\n",
    "\n",
    "# define the number of lines to read for the estimate\n",
    "sample_lines = 1000\n",
    "\n",
    "# read the first few lines of the file to get the average bytes per line\n",
    "with open(csv_file, 'r') as file:\n",
    "    lines = [next(file) for _ in range(sample_lines)]\n",
    "avg_bytes_per_line = sum(len(line) for line in lines) / sample_lines\n",
    "\n",
    "# estimate the total number of lines in the file\n",
    "estimated_lines = total_size // avg_bytes_per_line\n",
    "\n",
    "# calculate the estimated total number of chunks\n",
    "total_chunks = estimated_lines // chunk_size\n",
    "if estimated_lines % chunk_size != 0:\n",
    "    total_chunks += 1\n",
    "    \n",
    "# generate a boolean mask for the entire dataset\n",
    "mask = np.random.rand(int(estimated_lines)) < 0.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting Scaler and Encoder:  98%|█████████▊| 72667/73892.0 [12:19<00:12, 98.21chunk/s] \n"
     ]
    }
   ],
   "source": [
    "# initialize a progress bar\n",
    "progress_bar = tqdm(total=total_chunks, desc=\"Fitting Scaler and Encoder\", unit=\"chunk\")\n",
    "\n",
    "# fit the scaler and encoder incrementally\n",
    "for chunk in pd.read_csv(csv_file, chunksize=chunk_size):\n",
    "    chunk_mask = np.random.rand(len(chunk)) < 0.8\n",
    "    train_chunk = chunk[chunk_mask]\n",
    "    scaler.partial_fit(train_chunk[numerical_features])\n",
    "    encoder.fit(train_chunk[categorical_features])\n",
    "    progress_bar.update()\n",
    "\n",
    "# close the progress bar\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting preprocessor:  98%|█████████▊| 72667/73892.0 [14:26<00:14, 83.91chunk/s] \n"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(total=total_chunks, desc=\"Fitting preprocessor\", unit=\"chunk\")\n",
    "\n",
    "# fit the preprocessor on the entire training data\n",
    "for chunk in pd.read_csv(csv_file, chunksize=chunk_size):\n",
    "    chunk_mask = np.random.rand(len(chunk)) < 0.8\n",
    "    train_chunk = chunk[chunk_mask]\n",
    "    preprocessor.fit(train_chunk)\n",
    "    progress_bar.update()\n",
    "# close the progress bar\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a generator function for the training set\n",
    "def train_generator():\n",
    "    for chunk in pd.read_csv(csv_file, chunksize=chunk_size):\n",
    "        chunk_mask = np.random.rand(len(chunk)) < 0.8\n",
    "        train_chunk = chunk[chunk_mask]\n",
    "        if len(train_chunk) == chunk_size:  # only yield full chunks\n",
    "            # perform the transformations\n",
    "            transformed_chunk = preprocessor.transform(train_chunk)\n",
    "            yield transformed_chunk\n",
    "\n",
    "# define a generator function for the validation set\n",
    "def val_generator():\n",
    "    for chunk in pd.read_csv(csv_file, chunksize=chunk_size):\n",
    "        chunk_mask = np.random.rand(len(chunk)) < 0.8\n",
    "        val_chunk = chunk[~chunk_mask]\n",
    "        if len(val_chunk) == chunk_size:  # only yield full chunks\n",
    "            # perform the transformations\n",
    "            transformed_chunk = preprocessor.transform(val_chunk)\n",
    "            yield transformed_chunk \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tf.data.Datasets for training and validation\n",
    "train_dataset = tf.data.Dataset.from_generator(train_generator, output_signature=(\n",
    "    tf.TensorSpec(shape=(None, 20), dtype=tf.float32)))\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_generator(val_generator, output_signature=(\n",
    "    tf.TensorSpec(shape=(None, 20), dtype=tf.float32)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9m/k3vyv4317cz56g6f681k4jnm0000gn/T/ipykernel_41562/1256934740.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train the model on the training dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtrain_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m# perform a training step here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# train the model on the training dataset\n",
    "for train_batch in train_dataset.batch(2):\n",
    "    # perform a training step here\n",
    "    print(train_batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
